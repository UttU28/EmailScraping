{
    "0_only W2, Microsoft Azure Cloud Systems Architect": "priyanka verma,\nTriwave Solutions Inc\npriyanka@triwavesolutions.com\nReply to: priyanka@triwavesolutions.com\nPosition Title*Cloud Systems Architect (7/12)Position ResponsibilitiesJob Title: Cloud and System ArchitectJob Location: Tallahassee, FL (hybrid)Job Duration: 12-month contractJob Overview: Cloud and Systems Architect to play a key role in assisting IT in the oversight, management, and execution of cloud services and platforms to support infrastructure, applications, data, and other business needs.Design, upgrade, administer, test, and modify the virtual desktop infrastructure environment.Assist in ongoing modernization and migration efforts for applications, data, and infrastructure.Oversight and management of Microsoft 365 environments.Oversight and management of other Microsoft platforms and/or environments.Oversight and management of identity platforms.Oversight and management of a multi-cloud environment.Architect, design, facilitate, lead, coordinate, and direct technology initiatives on multiple fronts across a variety of disparate areas within the organization. This could include Infrastructure administration, application development, data management & analytics, cybersecurity, external hosting, identity and access management, infrastructure, network, security hardening, privacy, and compliance.Job Duties:Build and manage relationships in a matrixed environment.Recommend and assist with the building and hosting of complex application solutions.Understand, deploy, and manage Microsoft Azure and/or other cloud services.Understand, deploy, and manage Microsoft Power Platform.Find opportunities to refactor solutions to obtain better performance, cost, and efficiency.Enhance infrastructure, application, and system monitoring solutions to prioritize system uptime.Review, advise, and design based on risks, costs, benefits and impact on the enterprise business process and goals.Automate processes wherever possible.Create and manage Azure DevOps projects, git repositories, and pipelines.Troubleshoot issues brought forth by developers and other architects.Present to your peers on findings of new technologies, enterprise-wide issues, and enhancements you have made to better the mission.Required Experience: 2 \u2013 3 years of strong professional experience as a Microsoft Azure cloud systems architect is required.5 - 8 years of professional experience in information systems engineering or another related field is required.2 - 3 years of experience in Enterprise Architecture is required.Strong analytical skills to determine an organization's needs, develop strategies to meet those needs, and evaluate effectiveness.Ability to categorize work, set priorities, and determine short and/or long-term goals and strategies to achieve them.Ability to troubleshoot and diagnose problems effectively regardless of technology platform.Able to easily convey knowledge to others.Proficiency in working in a fast-paced, complex, and dynamic business environment.In-depth knowledge of Microsoft Azure Services.In-depth knowledge of Microsoft Server products.In-depth knowledge of Linux (Redhat and Ubuntu).Strong grasp of IT infrastructure and application development technology and architectures.Knowledge of assessment and analytical process and practices.Skill in strategic planning.Skill in researching, compiling, and analyzing data to report findings and develop improvement solutions.Skill in system administration and hosting across a wide range of solutionsAbility to research, identify, and implement innovative solutions.Ability to communicate effectively; both verbally and written.Ability to establish and maintain effective working relationships.Ability to work independently and as a team.Preferred Experience:2 \u2013 3 years of experience with other cloud providers, such as AWS or GCP is highly desired.5 years of professional experience supporting middleware such as API Management, IT Service Automation tools, and batch job services is highly desired.2 - 3 years of professional experience in administration of Microsoft software is highly desired.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "1_JD||AWS Engineer||Culver City CA": "vikas kumar,\nsynkriom\nvikas.kumar@synkriom.com\nReply to: vikas.kumar@synkriom.com\nRole name: AWS EngineerRole Description:AWS Cloud Enablement EngineerCompetencies:Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years):8-10Essential Skills:AWS Cloud Enablement EngineerDesirable Skills:AWS Cloud Enablement Engineer Keywords:AWS Cloud Enablement Engineer\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "2_DevOps Architect :: Remote": "Ekta Chaudhary,\nEpeople Technologies\nekta@epeopletech.com\nReply to: ekta@epeopletech.com\nTerraform Template creation Terraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication services\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "3_Job Opportunity for Salesforce Commerce Cloud Technical Lead": "Gunadeep,\nAvance Consulting\ngunadeep.c@avanceservices.com\nReply to: gunadeep.c@avanceservices.com\nHello, Hope you are doing well...!This is GUNADEEP from AVANCE CONSULING, and I have a new job opening for you. Please have a look at the below job description, if interested please share your updated resume or feel free to contact me.Job Description:Title: Salesforce Commerce Cloud Technical Lead roleDuration: ContractLocation: RemotePlease find the JD below:8 to 10 years of hands-on development experienceMust understand eCommerce architecture, product, order, and inventory flows to create detailed designs, architectural documents, and develop solutions.Be able to support the full code review and release management for SFCC.Strong background in SFCC and SF OMS experience with eCommerce/Digital environment, Batch File Scripting, SQL, Java knowledge Should have worked on projects supporting SFCC implementation.Used DevOps such as Copado, Git, Flossum, etc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "4_Very Urgent Job Opportunity of AWS LEAD (TECHNICAL LEAD) at Location- San Antonio, TX (Onsite) is shared with you": "Raghwendra Rao,\nIDC Technologies\nraghwendra.rao@idctechnologies.com\nReply to: raghwendra.rao@idctechnologies.com\nGreetings! This is Raghwendra from IDC Technologies, and I am writing to you regarding an excellent job opportunity that I have with one of IDC\u2019s premier clients in San Antonio, TX (Onsite). I found your resume during my search for qualified candidates on the internet and would like to know if you would be interested in pursuing this opportunity. Please share your updated resume if interested. Job Title: AWS LEAD (TECHNICAL LEAD)Position Type: CTH Location: San Antonio, TX (Oniste)Min. Exp- 12 Years Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA, API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "5_Hiring || Cloud Developer || Atlanta, GA Need a local candidateHybrid": "sakthivel,\nERPMark\nsakthivel@erpmark.com\nReply to: sakthivel@erpmark.com\nRole :Cloud DeveloperLocation : Atlanta, GA Need a local candidate HybridDuration : Long Term Note : Have to F2F interview JD: Strong AWS development with Python exp -Lambda, S3, Athena,Glue, DynamoDB, Event bridge\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "6_AWS Cloud Enablement Engineer": "Pradeep,\nScalable Systems\npradeep.sharma@scalable-systems.com\nReply to: pradeep.sharma@scalable-systems.com\nHi,Greetings of the day! I have an urgent requirement below, please go through JD and let me know if you are comfortable or have any profile. Kindly revert me back with your updated resume as well. Job Title: AWS Cloud Enablement Engineer Location: Culver City, CA (Work from Office)Duration: Long Term Contract Key Responsibilities:AWS Cloud Enablement EngineerAmazon Web Service(AWS)Cloud Computing Thanks & Regards, Pradeep SharmaEmail: pradeep.sharma@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "7_Senior DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nLoopNet - Senior DevOps EngineerJob Overview CoStar Group (NASDAQ: CSGP) is a leading global provider of commercial and residential real estate information, analytics, and online marketplaces. Included in the S&P 500 Index and the NASDAQ 100, CoStar Group is on a mission to digitize the world's real estate, empowering all people to discover properties, insights and connections that improve their businesses and lives. We have been living and breathing the world of real estate information and online marketplaces for over 35 years, giving us the perspective to create truly unique and valuable offerings to our customers. We've continually refined, transformed and perfected our approach to our business, creating a language that has become standard in our industry, for our customers, and even our competitors. We continue that effort today and are always working to improve and drive innovation. This is how we deliver for our customers, our employees, and investors. By equipping the brightest minds with the best resources available, we provide an invaluable edge in real estate. We are currently seeking an accomplished Senior DevOps Engineer to join our team, while supporting our multiple software products and brands across the organization, such as LoopNet , Apartments.com , Homes.com , and Ten-X .This position is located in Irvine and offers 2 days remote per week. ResponsibilitiesGrow the team's experiences and skillset through knowledge sharing and by having proactive and team-centered attitude.Automate and leverage DevOps principles, always striving for operational excellence with infrastructure-as-code mentality.Utilize cutting-edge technology to improve our services and processes.Manage enterprise level web applications.Collaborate with development and operations team to design and build scalable and secure infrastructure.Practice continuous integration/continuous delivery (CI/CD) using latest DevOps tools and innovative methods.Ensure the health and uptime of critical systems and applications with pro-active monitoring and metrics analysis.Strong troubleshooting skills (application, network, systems, infrastructure) with ability to multi-task and context switch.Good communication skills, expresses oneself clearly both verbally and in writing.Participate in a weekly on-call rotation once on-boarded.Basic QualificationsBachelor's Degree required from an accredited, not-for-profit university or college.A track record of commitment to prior employers.5+ years' experience with managing heavy web traffic sitesIaC (Terraform, Cloudformation)CI/CD (Azure DevOps)Kubernetes / EKSAWS (EC2, ALBs, S3, IAM, ASG, Lambda, Dynamo, Step Function, Elasticache, OpenSearch, etc.)Enterprise monitoring tools in areas of APM, system vitals, synthetics, and RUM (DataDog, AppDynamics, Prometheus, Icinga).Scripting language (PowerShell, Bash, YAML)Configuration management (e.g., Chef, Ansible)Experience with configuring CDN for performance caching and global traffic management.Preferred Qualifications And SkillsSome developer background with either Java, C#, Python, NodeJSFamiliar with load balancing technologies such as F5 LTMs, including management of VIPs, pools, nodes, iRule authoring, SSL offloading.SQL or No SQL databasesServer-less architecture methodologiesFamiliarity with PCI compliance and remediationAgile methodologies and working on short sprint cyclesWhat's in it for You When you join CoStar Group, you'll experience a collaborative and innovative culture working alongside the best and brightest to empower our people and customers to succeed.We offer you generous compensation and performance-based incentives. CoStar Group also invests in your professional and academic growth with internal training, tuition reimbursement, and an inter-office exchange program.Our benefits package includes (but is not limited to):Comprehensive healthcare coverage: Medical / Vision / Dental / Prescription DrugLife, legal, and supplementary insuranceVirtual and in person mental health counseling services for individuals and familyCommuter and parking benefits401(K) retirement plan with matching contributionsEmployee stock purchase planPaid time offTuition reimbursementOn-site fitness center and/or reimbursed fitness center membership costs (location dependent), with yoga studio, Pelotons, personal training, group exercise classesAccess to CoStar Group's Diversity, Equity, & Inclusion Employee Resource GroupsComplimentary gourmet coffee, tea, hot chocolate, fresh fruit, and other healthy snacksWe welcome all qualified candidates who are currently eligible to work full-time in the United States to apply. However, please note that CoStar Group is not able to provide visa sponsorship for this position.This position offers a base salary of $124,000 - $211,000, based on relevant skills and experience and includes a generous benefits plan.#LI-IZ1 #LI-HybridCoStar Group is an Equal Employment Opportunity Employer; we maintain a drug-free workplace and perform pre-employment substance abuse testingEmployers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "8_Urgent Need: Azure Architect with EventHub, Snowflake and Cosmos DB, 100% Remote": "SAPNA,\nITECS\nsapna@itecsus.com\nReply to: sapna@itecsus.com\nUrgent need from Infocepts. Only US Citizens. Phone + Video interview.100% Remote from US location. Primary Skills: Technologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBT Job Title: Azure Architect with EventHub, Snowflake and Cosmos DBWork Location: Onshore in USA \u2013 100% Remote Purpose of the Position: Purpose of this role is to design and build data architecture for a migration project. Major responsibilities under this role would be to create high level and low level design for data integration framework, translate the business requirements into technical prototypes, help developers in developing the data integration pipelines, perform peer reviews of the code, own the technical delivery of the DI part from inception to the deployment. Also own the technical consultation for existing or new clients during initial phase of any initiative and help in proposals and pre-sales initiatives. Must Have Technical Experience:At least 12 years of IT experience; Minimum 5 years of experience on AzureMust have minimum 5+ years of experience in implementing and managing Azure Cloud components.Expertise in Python and SQL best practicesExperience in data and analytics domainKnowledge of integrating Azure Active Directory with other servicesIn-depth experience of key Azure services for data integration, BI and processing services including but not limited to Azure HDInsight, Azure Databricks, Azure Portal, Log Analytics, Azure Data Factory, , Azure Search, Azure Functions, Azure Stream Analytics, Managed Airflow and Azure Event Hub.Experience in Azure cosmos DB over PostgreSQL, mongo dB and live streaming with Azure EventHub.Experience on Azure functions with Python language.Experience in migration of on-prem applications and data analytics workload to Azure cloud. Experience in designing Azure cloud architecture and operationalizing solutions around monitoring, alerting, logging etc.Working knowledge on Architecture design, Modelling, prototyping, and benchmarking of DataStore/ EDW solutions.Knowledge of variety of advanced architecture, tools, and concepts across all layers of modern distributed technology stack covering the big data ecosystem.Knowledge of Azure storage services such as Azure storage account, Azure Cosmos DB, Azure Time Series Insights, Redis Cache, SQL Databases, Table StorageExperience with programming languages such as PythonExperience in enabling DevOps automation for Azure with appropriate security and privacy considerations.Knowledge of configuring and provisioning networks and infrastructure in cloudGood to have knowledge of Big Data ecosystems such as Hadoop / Hive, HDFS, Spark, YARN and others.Good to have Snowflake, Scala, and Java experienceGood to have Technical Experience:Pre-sales experience will be an added advantage.Any BI experienceExperience in any other cloud technologies like AWS, ALI, GCP.Snowflake, DBT, Scala and JavaTechnologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBTQualifications:Bachelor\u2019s degree in computer science, engineering, or related field (master\u2019s degree is a plus)Demonstrated continued learning through one or more technical certifications or related methods. ---------------------------------------------------------------------------------------------------------------------------Please follow the submission format table provided and ensure all required information is included with each submission with resume. Incomplete submissions will be disqualified. Candidate Legal Name (As per Documents) Summary Year of experience in each skillset: Email Address Phone Number Work Authorization Current Location For non-local, Relocation (Yes/No) Pay Rate Current working status Reason for change LinkedIn Profile Availability for Interview (Please provide 03 time slots) Availability for Joining the project\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "9_Devops Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nThis company provides a global payments software offering that has grown consistently over the last decade. Great company with vertical growth opportunity!This Jobot Job is hosted by: Alex DickinsonAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.Salary: $120,000 - $165,000 per yearA bit about us:This company is a modern software product firm that services clients all across the US. They have a very mature and modern IT infrastructure posture. Their current team is looking for a Devops engineer functioning in a .NET environment with cloud experience.Why join us?Competitive Base Salary - $120-155kQuarterly bonus plan401k with matchGym reimbursementWFH optionsAccelerated Career Growth!Job DetailsBuilding, configuration, deployment, and management of high volume, highly available .Net applicationsWindows Server/IIS deployment, configuration, and administrationKubernetes (AKS) deployment, configuration, and administrationAzure DevOpsMonolithic architectureTerraformOctopus DeployGitPowershell abilitiesSolid understanding of core concepts: DNS, HTTP/HTTPS, Load-Balancing, TCP/IP routing and switchingInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.Employers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "10_urgent : DevOps engineer | GC EAD,H4 EAD,OPT EAD | Newport Beach, California": "Ramashankar,\nvyzeinc\nramashankar@vyzeinc.com\nReply to: ramashankar@vyzeinc.com\nJob Description -Please try to submit some genuine candidates, as we have previosly submitted some candiates who were fake and couldn't explain the skills mentioned in the resume.Need LinkedIn, DL and Visa with submissionTitle: DevOps engineerLocation: Newport Beach, CA (100% Onsite) Job Description: We need a DevOps engineer to assist developers in utilizing AI for processing unstructured data.This involves establishing a solution for the acquisition, movement, and ingestion of large volumes of unstructured data on a daily basis, and making this data available for AI processing.Skills, in priority order (from must-have to nice-to-have), include:DevOps that can code: 25% coding, 75% operationsPythonKubernetes (K8s), Airflow/Dagster/Airbyte, Argo Workflows, DatabricksInfrastructure as Code (IaC) - Terraform, Cloud Development Kit (CDK)Monitoring (Datadog, Splunk, PagerDuty) - managing a production systemEvent-based systems - KafkaAPIs, Layer 3/7 networking (Envoy)Application authentication (Okta)AI stack - OpenAI, Claude 3, Pinecone, SageMaker, Bedrock\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "11_AWS Cloud Enablement Engineer": "lavanya,\nKK associates\nlavanya@kksoftwareassociates.com\nReply to: lavanya@kksoftwareassociates.com\nWe are hiring hashtag#AWS_Cloud_Enablement_Engineer my Client interesting candidates please send resumes to lavanya@kksoftwareassociates.com or 614-379-0184Visa status: H1B/USC/GCPosition: AWS Cloud Enablement EngineerLocation: - WOODLAND HILLS, Culver City, CARole name:Product EngineerRole Description:AWS Cloud Enablement EngineerCompetencies:Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years):8-10Essential Skills:AWS Cloud Enablement Engineer\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "12_Immediate Req_AWS  Delphix Engineer": "Ben Clark,\nTech Talent Connect LLC\nben@techtalentconnect.com\nReply to: ben@techtalentconnect.com\nJob Title: AWS \u2013 Delphix EngineerLocation: San Antonio, TX (Day1 onsite, Hybrid)Duration: Long Term ContractRate: DOEResponsibilities:\u00b7 Creating, configuring, and maintaining Infrastructure on AWS Cloud services including Virtual Private Cloud VPC, EC2, RDS, S3, Route53, SNS, CloudFront, CloudWatch and IA.\u00b7 Migrating data from on-prem to Amazon Web Services cloud.\u00b7 Creating S3 buckets and folder management within it.\u00b7 Creating role-based policies to access AWS resources like S3 and other AWS services.\u00b7 Creating or Importing Volumes into AWS instances.\u00b7 Create and configure the Elastic Load balancers with Autoscaling groups.\u00b7 Create Alarms, events, logs on CloudWatch for monitoring and Cloud trail for logging events onto S3 to troubleshoot and record event history.\u00b7 Hands-on experience with databases MySQL, Oracle creating users, performing dump/restore, and taking automated snapshots.\u00b7 Exposure to Kubernetes clusters and managing the clusters using KOPS, Rancher.\u00b7 Experience in administration, engineering, and support of the Delphix platform \u2013 Mandatory.\u00b7 Onboard and manage VDB and dSources on Cloud and on prime premises \u2013 Mandatory.\u00b7 Strong scripting skills (e.g., Ansible, Shell scripting, Python, PowerShell) to develop and maintain automation of operational tasks.\u00b7 Good to have migration experience from on-prem to cloud.\u00b7 Formulate and implement Test Data Management plans.\u00b7 Proficiency in Delphix virtualization and Continuous Compliance.\u00b7 Perform in-place data masking with integrity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "13_Urgent Need ||  Lead AWS Data Engineer || NYC NY 3 Days Hybrid from Day 1": "Neeraj Kumar,\nGlobal Applications Solution Pvt. Ltd\nneeraj.k@globalapplications.com\nReply to: neeraj.k@globalapplications.com\nRole: Lead AWS Data EngineerLocation: NYC NY 3 Days Hybrid from Day 1Job Summary:We are looking for an experienced Data Visualization and Migration Specialist with a strong background in AWS QuickSight, AWS Redshift, and Amazon Q. The ideal candidate will have a proven track record of working with data visualization tools, as well as extensive experience in data migration activities, specifically moving data from on-premises data stores to Amazon Redshift.Key Responsibilities:Design, develop, and maintain interactive dashboards and reports using AWS QuickSight.Leverage Amazon Q for natural language query capabilities to enhance data accessibility and usability.Manage and optimize AWS Redshift data warehouses, ensuring data integrity and performance.Conduct data migration from on-premises data stores to AWS Redshift, including planning, execution, and validation.Collaborate with cross-functional teams to understand business requirements and translate them into effective data visualizations and reporting solutions.Implement best practices for data management, data quality, and data governance.Troubleshoot and resolve issues related to data visualization and data migration.Provide training and support to end-users on using AWS QuickSight and Amazon Q.Stay updated with the latest trends and technologies in data visualization and data migration. Required Qualifications:Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or a related field.4-5 years of experience working with data visualization tools, specifically AWS QuickSight.Strong expertise in AWS Redshift, including data warehousing and performance optimization.Experience with Amazon Q for enhancing data accessibility.Proven experience with data migration activities, particularly moving data from on-premises data stores to Amazon Redshift.Proficient in SQL and database management.Strong analytical and problem-solving skills.Excellent communication and collaboration skills.Ability to work independently and manage multiple projects simultaneously. Preferred Qualifications:AWS Certification in any of the following: AWS Certified Solutions Architect, AWS Certified Data Analytics, AWS Certified Big Data.Experience with other data visualization tools like Tableau or Power BI.Knowledge of ETL processes and tools.Experience with scripting languages such as Python or R.Thanks Neeraj Kumar\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "14_AWS Cloud Engineer at remote": "Ujjwal,\nzealhire\nujjwal@zealhire.com\nReply to: ujjwal@zealhire.com\nHi RonHope you are doing well.Greetings from ZealHire Inc. Job Title: AWS Cloud EngineerLocation: remotePosition Type: ContractLooking for someone with Terraform, Python and AWS. This will be new development and support.Job Description:As an Engineer 2, candidate will be responsible for using candidate's technical knowledge of professional concepts to solve business problems.Client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.Candidate will be responsible for evaluating elements of technology's effectiveness through requirements gathering, testing, research and investigation, and making recommendations for improvements resulting in increased quality and effectiveness.Candidate will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.Client Mobility is seeking an Engineer 2 to support client\u2019s rapidly growing cloud enablement engine application and infrastructure portfolio.This role will automate configurations and templates for customer use to consume various AWS based services, manage AWS based services, measure and maintain reliability through instrumentation and monitoring, and interface with multiple development teams to provide services and support.Candidate will create and modify medium to complex Python and Terraform (HCL) code to enable our customers to more easily consume AWS services.Candidate will perform local development builds, being a primary contributor in code reviews, planning, testing, and coordinating of implementation activities.Participate in on-call rotation, which includes 24/7 support every two months of multiple production environments.Candidate will work with cloud vendors and external technical support on upgrades, problem resolution, and design issues.This is an exciting role in a fast-paced, collaborative team that strives to build and foster close working relationships with its application development customers.Required:2 plus years of software development experience.2 plus years of Python (boto3) and Terraform development experience.2 plus years of experience with core Amazon Web Services such as Route 53, EC2 (AMI, EBS, ELB, ASG), S3, CloudWatch, CloudFormation,ElasticBeanstalk, ElastiCache, IAM, VPC, RDS, DynamoDB, SQS, and SNS.Must be committed to incorporating security into all decisions and daily job responsibilities.Must be able to coach and mentor team members and customers.Experience with the design and build of web application cloud infrastructure.Experience in and demonstrable knowledge of Linux command line interface.Experiences with software design methodologies, information systems architecture, object-oriented design, and software design patterns.Excellent verbal and written communication skills.Excellent customer service skills.Familiarity with Agile/Scrum methodology.Ability to quickly triage problems, determine root cause and drive resolution.Familiarity with Software Development Lifecycle (SDLC)Knowledge of version control tools, such as git, BitBucket, and GitHubBachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field.Preferred:Experience in Jira, Confluence, Maven, and Jenkins.Experience with Linux package management (rpm, yum, dnf, etc).Experience with HTTP/Proxy servers (Apache, Nginx).Experience installing, configuring, and troubleshooting application platforms, with a preference for experience in working with Java web applicationplatforms (e.g., Tomcat, Spring Boot, etc.).Experience with monitoring and alerting tools such as Dynatrace, Cloudwatch or similar.Experience with log aggregation and management tools such as Splunk.Experience working with configuration management tools. Examples: Ansible.Systems performance tuning and load testing is a plus.Familiarity with large-scale systems and methodologies. Kind Regards, Ujjwal TiwariTechnical RecruiterZealHire Inc. Email: Ujjwal@zealhire.com14 Wall Street 20th Floor | New York, NY 10005www.zealhire.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "15_Need AWS Cloud Enablement Engineer": "ayush,\nScalable Systems\nayush.yadav@scalable-systems.com\nReply to: ayush.yadav@scalable-systems.com\nAWS Cloud Enablement EngineerCulver City, CAAbout the RoleDo you have a passion for cloud technology and enjoy helping others leverage its potential? We are seeking a highly motivated and experienced AWS Cloud Enablement Engineer to join our growing team in Culver City, CA. In this role, you will play a pivotal role in accelerating our organization's cloud adoption journey by empowering teams with the knowledge and skills they need to thrive in the AWS environment.Highlighted Skills and Keywords:AWS Certified Solutions Architect - Associate (or Professional)AWS Certified Cloud PractitionerStrong understanding of AWS services (EC2, S3, VPC, IAM, Lambda, etc.)Experience with cloud infrastructure design and deploymentExperience with infrastructure as code (IaC) tools (Terraform, CloudFormation)Excellent communication and collaboration skillsAbility to translate complex technical concepts into clear, actionable insights for non-technical audiencesProven ability to develop and deliver engaging technical training programsResponsibilities:Develop and implement a comprehensive cloud enablement strategy that aligns with our business objectives.Design and deliver engaging technical training programs on AWS fundamentals, best practices, and specific AWS services.Collaborate with engineering teams to identify cloud migration and optimization opportunities.Develop and maintain cloud infrastructure as code (IaC) templates for efficient and consistent deployments.Stay up-to-date on the latest AWS advancements and incorporate them into training programs and best practices.Develop and maintain documentation for cloud-related processes and procedures.Assist with troubleshooting and resolving cloud-related issues.Champion a culture of cloud adoption within the organization.Qualifications:Bachelor's degree in Computer Science, Information Technology, or a related field (or equivalent experience)8-10 years of experience in cloud computing or a related fieldProven experience in AWS cloud technologies (3+ years)Experience developing and delivering technical training programs (preferred)Familiarity with Agile methodologies (preferred)Experience with DevOps principles (a plus)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "16_Need Locals || DevOps Engineer || Plano, TX- Day 1 Onsite": "Danish Mujeeb,\nHan Staffing\ndanish@hanstaffing.com\nReply to: danish@hanstaffing.com\nTitle: DevOps EngineerLocation: Plano, TX (Day 1 Onsite Need only local candidates)Long Term Contract Job Description:Excellent working experience in CI/CD to Setup CI/CD pipeline to achieve fully automated CI/CD process.Troubleshoot and rectify build pipeline issues.Schedule automated build and deployment and execute ad-hoc build request by AD teamHands-on knowledge of Bit-bucket, GIT commands and branching strategies.Other Skills:Knowledge of Unix commands required for day to day operations and troubleshooting.Scripting knowledge for automation.Knowledge of webapps (weblogic, Tomcat, Apache , IBM HTTP etc) and their troubleshooting.Working knowledge of schedulers ( control M, Autosys etc).SSL/TLS certificate management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "17_Urgent Need : Devops Engineer :Plano, TX - Need Local": "Jagdeep Singh,\nHAN IT Staffing\njagdeep@hanstaffing.com\nReply to: jagdeep@hanstaffing.com\nTitle: DevOps EngineerLocation: Plano, TX (Day 1 Onsite Need only local candidates)Job Description:Excellent working experience in CI/CD to Setup CI/CD pipeline to achieve fully automated CI/CD process.Troubleshoot and rectify build pipeline issues.Schedule automated build and deployment and execute ad-hoc build request by AD teamHands-on knowledge of Bit-bucket, GIT commands and branching strategies.Other Skills:Knowledge of Unix commands required for day to day operations and troubleshooting.Scripting knowledge for automation.Knowledge of webapps (weblogic, Tomcat, Apache , IBM HTTP etc) and their troubleshooting.Working knowledge of schedulers ( control M, Autosys etc).SSL/TLS certificate management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "18_Urgent hiring for Cloud Architect": "padmavathi,\nYochana IT solutions\npadmavathi@yochana.com\nReply to: padmavathi@yochana.com\nHello,I hope you are doing great.This is Padmavathi from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your timeTitle : Cloud ArchitectLocation : Clayton, Missouri (Hybrid) Note: Only GC and USC visa AcceptableRole/ JD:HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. o Position - Solutions Architect (Azure)o Designation - Consultant/Senior Consultant/Solutions Architecto Experience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions \u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions \u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction. \u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements \u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring \u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. \u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred. \u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Thanks & Regards,Padmavathi Vindeepu,Resource Specialist, Farmington Hills, MI,Email Id: padmavathi@yochana.comhttps://www.linkedin.com/in/padmavathi-vindeepu-25b540261/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "19_Urgent Requirement: Position:Cloud Architect(ONSITE) (Locals to DMV Area)": "ATI Team,\nAspire\nrecruiterpro@aspiretechus.com\nReply to: recruiterpro@aspiretechus.com\nPOSITION: Cloud Architect(ONSITE)Min8yrs Exp (Locals to DMV Area)DURATION:2-18 MonthsADDRESS:Washington DC 200038-10 years of experience. Subject Matter Expert in Azure and AWS cloud infrastructure and network technologieJOB DESCRIPTIONResponsibilities:Experience spanning at least two IT disciplines, including technical architecture, network management, application development, middleware, database management, security, or operations.Experience with multiple public cloud platforms, including AWS, Azure, and GCP.Experience relocating ERP application to a cloud-native environment.Ensures that standards set by Data, Security, Infrastructure, Platform, and other architecture domains are followed when designing and architecting cloud solutions.Demonstrate deep subject matter leadership of cloud architecture and implementation features (OS, multitenancy, virtualization, orchestration, elastic scalability).Manage the development and implementation of cloud strategy, develop the architecture governance framework, and define architecture foundations.Demonstrates expertise in conveying technical and functional concepts for a specific technical specialty.Identifies improvements to project standards to achieve high quality services/products.Identifies best practices and standards for the use of the product.Delivers support and design for industry specific technologies that require integration with systems or networks.Interacts with executive level business users or technical experts.Functions as a niche technical SME.Lead experience with technical expertise across large, complex implementations for systems.Define and enforce Cloud architecture best practices: availability, redundancy schemes, performance, Disaster Recovery, data, and integration patterns.Troubleshoot issues across the entire stack: IaaS, PaaS, services, applications, network, and security.1. Formulates and defines systems scope and objectives based on both user needs and a thorough understanding of business systems and industry requirements.Devises or modifies procedures to solve complex problems considering computer equipment capacity and limitations, operation time, and form of desired results. Includes analysis of business and user needs, documentation of requirements, and translation into proper system requirements specifications.Provides consultation on complex projects and is considered to be the top-level contributor/specialist of most phases of systems analysis, while considering the business implications of the application of technology to the current and future business environment.Minimum Education/Certification Requirements :Bachelor\u2019s degree in IT or related field or equivalent experience; or a current Azure and AWS Architect CertificationsRequired/Desired SkillsCloud DevOps experienceBroad technology experience across several of these areas including, applications development, relational or NoSQL database, DevOps, containers, CI/CDMust have minimum of 8 years of experience in the design and implementation of cloud workloads in Azure, AWS, or GCPExperience with cloud networking and network security, including virtual networks, network security groups, cloud-native firewalls, etc.Bachelor\u2019s degree in IT or related field or equivalent experience8 -10 yrs. experience in one or more architecture domains (e.g., business architecture, solutions architecture, application architecture)8 -10 yrs. preparing complex technical documentation8 - 10 yrs. experience in managing large operational cloud environments spanning multiple tenants through techniques such as Multi-Account managementMinimum 3 years of microservice architectural experienceMinimum of 3 years of experience working exclusively on designing and implementing cloud-native workloads.Experience with application lifecycle tools (Git, GitHub, Jenkins, Bamboo, Azure DevOps, Ansible, Terraform, Cloudformation, etc.).Certified AWS Solutions Architect ProfessionalCertified Microsoft Azure Solutions Architect ProfessionalThanks & RegardsATI TeamRecruiterpro@aspiretechus.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "20_Azure DevOps RDB expert": "Khushi Malhotra,\nTestingXperts\nkhushi.malhotra@testingxperts.com\nReply to: khushi.malhotra@testingxperts.com\nPosition: Azure DevOps RDB expertLocation: Remote\"Skill Score: Full Stack Sr. Software Development Engineer with 6-8 years of experience in modern .NET Web & API development with strong relational DB expertise. \u2022 High level of Expertise in C#, .NET, React, ASP.NET MVC, Javascript, JQuery, AJAX and CSS, HTML, LINQ, Vue.js, Visual Studio, SQL Server, TSQL, Entity Framework Core, SoapUI\u2022 Strong SQL experience with writing complex queries, table design, DB administration, stored procedures, functions, performance optimization, profiler utilization, etc\u2026\u2022 Azure development including Azure Functions, Azure Event Grid, Azure Storage, Resource Groups, Azure SQL, Azure Logic apps and Azure Service bus\u2022 Hands on experience in solving software design issues by applying design patterns such as Factory Pattern, Abstract Factory Pattern\u2022 Ability to contribute to design of complex architecture, UI and services with the use of modern technologies to meet the business requirements\u2022 Experience in design and implementation REST services and microservices\u2022 Clear communicator to be able to translate business requirements into technical solutions\u2022 Proficient with on TFS, Github, CICD, Azure DevOps, GitHub Actions, Jenkins, Powershell\u2022 Experience in developing automation code using Visual Studio and Selenium/Jmeter\u2022 Proficiency with unit test implementation using frameworks such as Microsoft Unit Test and NUnit\u2022 Experience working with Security Tools such as GitHub CodeQL & Dependabot, SonarQube, WhiteHat, etc...\u2022 Experience working within Scrum Agile development environment\u2022 Commitment to support aggressive bi-weekly release cycles - 26 releases per year Nice to Have:\u2022 Experience with application migration to public cloud (Azure)\u2022 Containerization (Kubernetes, Dockers), Infrastructure as Code (Terraform), application observability, messaging, stream architecture\u2022 Experience with non-relational DB patterns, implementation and large data reporting\u2022 Healthcare Industry Knowledge\"\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "21_Locals (AZ) Only -- Devops-GitHub Actions Engineer at Phoenix, AZ (Onsite)": "Manigandan,\nLorven Technologies Inc\nmanigandan.pa@lorventech.com\nReply to: manigandan.pa@lorventech.com\nHi ,Hope you are doing great!! Job Opening for GitHub Actions Engineer at Phoenix, AZ (Onsite) Job Title: GitHub Actions EngineerLocation: Phoenix, AZ (Onsite)Duration: 1+ Year (Contract) Description:GitHub Actions EngineerLooking for an engineer to be part of GitHub Actions support. Someone with good CI/CD & GitHub Actions knowledge is a must. Need to be good at any one Programming language Thanks & Regards ManigandanRecruiterLorven Technologies, Inc.101 Morgan Lane | Suite 209 | Plainsboro | NJ 08536\u260e 609 374 9629 | \ud83d\udcf1 470-440-2646 \u2709 manigandan.pa@lorventech.com | \ud83c\udf10www.lorventech.comhttps://www.linkedin.com/in/manigandan-p-a-22b88693/ Inc 5000 Fastest Growing Companies in America\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "22_Sr Cloud Network Engineer : Bellevue, WA (Onsite)": "Amol,\nChabeztech\namol@chabeztech.com\nReply to: amol@chabeztech.com\nTitle: Sr Cloud Network Engineer Job Location:- Bellevue, WAManadatory requirement: Telecom industry experience Below is the Job Description Reqs Key Responsibilities\u2022 Oversee the network onboarding process for new users and systems in the Cloud environment\u2022 Provision and configure network resources in the Cloud, ensuring compliance with security policies and government regulations.\u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.\u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.\u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.\u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.\u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements.\u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.\u2022 Provide technical support and troubleshooting for Cloud network-related issues\u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "23_Urgent Need: Azure Architect with EventHub, Snowflake and Cosmos DB, 100% Remote": "SAPNA,\nITECS\nsapna@itecsus.com\nReply to: sapna@itecsus.com\nUrgent need from Infocepts. Only US Citizens. Phone + Video interview.100% Remote from US location. Primary Skills: Technologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBT Job Title: Azure Architect with EventHub, Snowflake and Cosmos DBWork Location: Onshore in USA \u2013 100% Remote Purpose of the Position: Purpose of this role is to design and build data architecture for a migration project. Major responsibilities under this role would be to create high level and low level design for data integration framework, translate the business requirements into technical prototypes, help developers in developing the data integration pipelines, perform peer reviews of the code, own the technical delivery of the DI part from inception to the deployment. Also own the technical consultation for existing or new clients during initial phase of any initiative and help in proposals and pre-sales initiatives. Must Have Technical Experience:At least 12 years of IT experience; Minimum 5 years of experience on AzureMust have minimum 5+ years of experience in implementing and managing Azure Cloud components.Expertise in Python and SQL best practicesExperience in data and analytics domainKnowledge of integrating Azure Active Directory with other servicesIn-depth experience of key Azure services for data integration, BI and processing services including but not limited to Azure HDInsight, Azure Databricks, Azure Portal, Log Analytics, Azure Data Factory, , Azure Search, Azure Functions, Azure Stream Analytics, Managed Airflow and Azure Event Hub.Experience in Azure cosmos DB over PostgreSQL, mongo dB and live streaming with Azure EventHub.Experience on Azure functions with Python language.Experience in migration of on-prem applications and data analytics workload to Azure cloud. Experience in designing Azure cloud architecture and operationalizing solutions around monitoring, alerting, logging etc.Working knowledge on Architecture design, Modelling, prototyping, and benchmarking of DataStore/ EDW solutions.Knowledge of variety of advanced architecture, tools, and concepts across all layers of modern distributed technology stack covering the big data ecosystem.Knowledge of Azure storage services such as Azure storage account, Azure Cosmos DB, Azure Time Series Insights, Redis Cache, SQL Databases, Table StorageExperience with programming languages such as PythonExperience in enabling DevOps automation for Azure with appropriate security and privacy considerations.Knowledge of configuring and provisioning networks and infrastructure in cloudGood to have knowledge of Big Data ecosystems such as Hadoop / Hive, HDFS, Spark, YARN and others.Good to have Snowflake, Scala, and Java experienceGood to have Technical Experience:Pre-sales experience will be an added advantage.Any BI experienceExperience in any other cloud technologies like AWS, ALI, GCP.Snowflake, DBT, Scala and JavaTechnologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBTQualifications:Bachelor\u2019s degree in computer science, engineering, or related field (master\u2019s degree is a plus)Demonstrated continued learning through one or more technical certifications or related methods. ---------------------------------------------------------------------------------------------------------------------------Please follow the submission format table provided and ensure all required information is included with each submission with resume. Incomplete submissions will be disqualified. Candidate Legal Name (As per Documents) Summary Year of experience in each skillset: Email Address Phone Number Work Authorization Current Location For non-local, Relocation (Yes/No) Pay Rate Current working status Reason for change LinkedIn Profile Availability for Interview (Please provide 03 time slots) Availability for Joining the project\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "24_DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDiscover an exciting remote DevOps engineering role dealing with management solutions for law firms and academic institutions. Focused on mitigating risk and driving business insights, this contract involves work in AWS or Azure. You'll use Terraform or cloud formation and containerization tools such as Kubernetes or Helm.This company has become profitable and is scaling consistently, fueling upward moment within and surrounding the position. Investments into the company are also contributing to migrating their systems from on-perm to the cloud. The job provides a great quality of life, full benefits such as 401k matching, as well as other exciting perks!Contract Duration: 6 - 12 MonthsRequired Skills & ExperienceAzureTerraformAzure DevOpsAKSDesired Skills & ExperienceAzureTerraformAzure DevOps with a greater concentration of experience with GitlabAKS to be able to maintain, nothing too involvedLots of experience with analyticsComfortability in a complicated APIWhat You Will Be DoingTech Breakdown40% AWS or Azure30% Terraform or Cloud Formation30% Kubernetes or HelmDaily Responsibilities50% Managing API Systems30% Team Collaboration30% Transitional workEmployers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "25_DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDiscover an exciting DevSecOps Engineering role in Moorestown, NJ. The full-time position is for a distinguished technology company specializing in building CI/CD pipelines and working within their on-perm infrastructure. Technical requirements include using Ansible to drive automation, as well as developing overall software security.This onsite position allows for a transition to a hybrid or flex schedule after getting accustomed to their systems, allowing for a more dynamic and adjustable workday. The role will greatly develop the ability to maintain server infrastructure and security practices. After asserting yourself in the position the conversion from hourly to salary also is accompanied with a sign on bonus.Required Skills & ExperienceAnsibleJenkinsLinuxOn-premise InfrastructureExperience building CI/CD pipelinesAnsible to drive automation, and focus on the security of their systemsAPI security, network security, overall software securityDesired Skills & ExperienceAnsibleJenkins (with Gitlab)LinuxOn-premise InfrastructureExperience building CI/CD pipelinesBe experienced with infrastructure as code, and set up disaster recovery.Contract Duration: 6 MonthsWhat You Will Be DoingTech Breakdown60% Building CI/CD Pipelines40% Developing Security of SystemsDaily Responsibilities60% Hands On10% Management Duties30% Team CollaborationThe OfferSign on bonus post flip after first 6 monthsApplicants must be currently authorized to work in the US on a full-time basis now and in the future.Employers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "26_Salesforce Developer with Financial Services Cloud Experiences - Onsite - Plano, TX - Locals Only - H1B Only": "PAVITHRA,\nXFORIA\npavithra.s@xforia.com\nReply to: pavithra.s@xforia.com\nSetup, customize and develop Salesforce.com and related app implementations, drawing on your relevant past experience and understanding of best practices surrounding Salesforce platformDevelop and enhance custom applications & features on the platform, by leveraging Salesforce Service Cloud, Sales Cloud and APIsBuild Salesforce integration with other applications, using relevant APIs and Integration frameworksSupport product owner/s with refinement of user requirement and lead the functional/technical solution architecture & designEnsure the platform is run as intelligently and efficiently as possible through continuous improvement, periodic code reviews, analysis of platform/governor limitsWork in an agile environment with a team of developers, product owners and test engineersAct as a coach/guide to junior engineers, and foster a culture promoting technical growth, respect between team-members, empowerment, continuous innovation and funSupport in maintaining the overall quality and integrity of the platform through appropriate quality assurance activities and automation testingLogging and managing incidents and defects through to resolutionSupport in deployment and release activitiesMinimum of 8 +years experience working on the Salesforce Financial Service Cloud implementations in a multi org structure .Strong Salesforce development experience with hands on knowledge of LWC, Aura Components, Apex and VisualforceStrong understanding of declarative automation features within the platformHands on experience with Version/Source Control and Continuous Integration tools, and DevOps principles Strong experience and understanding of Salesforce APIs, integration patterns, and hands-on knowledge on writing custom web servicesStrong understanding of Salesforce coding best practices, design patterns and enterprise patternsStrong knowledge and experience around Salesforce service cloud and Sales Cloud features\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "27_AWS Engineers - W2": "Deepak Singh,\nVyze Inc\ndeepak@vyzeinc.com\nReply to: deepak@vyzeinc.com\nJob Description -AWS Engineers - W2 onlyRemoteUSC/GC onlyLooking for someone with Terraform, Python, and AWS. This will be new development and support. Job Description:As an Engineer 2, candidate will be responsible for using candidate's technical knowledge of professional concepts to solve business problems.Client is looking for a talented individual who can serve as a subject matter expert in their area of focus and represent their department on complex assignments.Candidate will be responsible for evaluating elements of technology's effectiveness through requirements gathering, testing, research, and investigation, and making recommendations for improvements resulting in increased quality and effectiveness.Candidate will be required to listen to and evaluate customer needs to determine and provide high-quality solutions that align with customer expectations.Client Mobility is seeking an Engineer 2 to support client\u2019s rapidly growing cloud enablement engine application and infrastructure portfolio.This role will automate configurations and templates for customer use to consume various AWS-based services, manage AWS-based services, measure and maintain reliability through instrumentation and monitoring, and interface with multiple development teams to provide services and support.Candidate will create and modify medium to complex Python and Terraform (HCL) code to enable our customers to more easily consume AWS services.Candidate will perform local development builds, being a primary contributor in code reviews, planning, testing, and coordinating of implementation activities.Participate in on-call rotation, which includes 24/7 support every two months of multiple production environments.Candidate will work with cloud vendors and external technical support on upgrades, problem resolution, and design issues.This is an exciting role in a fast-paced, collaborative team that strives to build and foster close working relationships with its application development customers.Required:2 plus years of software development experience.2 plus years of Python (boto3) and Terraform development experience.2 plus years of experience with core Amazon Web Services such as Route 53, EC2 (AMI, EBS, ELB, ASG), S3, CloudWatch, CloudFormation,ElasticBeanstalk, ElastiCache, IAM, VPC, RDS, DynamoDB, SQS, and SNS.Must be committed to incorporating security into all decisions and daily job responsibilities.Must be able to coach and mentor team members and customers.Experience with the design and build of web application cloud infrastructure.Experience in and demonstrable knowledge of Linux command line interface.Experiences with software design methodologies, information systems architecture, object-oriented design, and software design patterns.Excellent verbal and written communication skills.Excellent customer service skills.Familiarity with Agile/Scrum methodology.Ability to quickly triage problems, determine root cause and drive resolution.Familiarity with Software Development Lifecycle (SDLC)Knowledge of version control tools, such as git, BitBucket, and GitHubBachelor's degree in Computer Science, Computer Information Systems, Management Information Systems, or related field.Preferred:Experience in Jira, Confluence, Maven, and Jenkins.Experience with Linux package management (rpm, yum, dnf, etc).Experience with HTTP/Proxy servers (Apache, Nginx).Experience installing, configuring, and troubleshooting application platforms, with a preference for experience in working with Java web applicationplatforms (e.g., Tomcat, Spring Boot, etc.).Experience with monitoring and alerting tools such as Dynatrace, Cloudwatch or similar.Experience with log aggregation and management tools such as Splunk.Experience working with configuration management tools. Examples: Ansible.Systems performance tuning and load testing is a plus.Familiarity with large-scale systems and methodologies. Regards!! Deepak SinghLead Technical Recruiter | IT Healthcare & InformaticsEmail: deepak@vyzeinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "28_Urgent hiring on ::Job Description -DevOps Engineer :: Location: Remote :: Duration: 12 months": "Rasmita Bhuyan,\nAdventa Tech INC\nrasmita@adventatech.com\nReply to: rasmita@adventatech.com\nJob Description -DevOps EngineerLocation: RemoteDuration: 12 monthsVisa : CITIZEN,GREEN CARD,H4 EAD,H1B,GC EADMOi : SkypeJob Description:Excellent working experience in CI/CD to Setup CI/CD pipeline to achieve fully automated CI/CD process.Troubleshoot and rectify build pipeline issues.Schedule automated build and deployment and execute ad-hoc build request by AD teamHands-on knowledge of Bit-bucket, GIT commands and branching strategies.Other Skills:Knowledge of Unix commands required for day to day operations and troubleshooting.Scripting knowledge for automation.Knowledge of webapps (weblogic, Tomcat, Apache , IBM HTTP etc) and their troubleshooting.Working knowledge of schedulers ( control M, Autosys etc).SSL/TLS certificate management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "29_Hiring for Azure  solution architect Engineer || Chicago, IL": "rajan,\nquantumworldit\nrajan@quantumworldit.com\nReply to: rajan@quantumworldit.com\nDear Partner, Trust you are doing well ! My name is Rajan Pandey and I am a Staffing Specialist at Quantum World Technologies Inc. I am reaching out to you on an exciting job opportunity with one of our clients. Job Title \u2013 Terraform EngineerLocation \u2013 Chicago, ILDuration- 6 Months +Hire Job Description We are seeking a highly skilled DevSecOps Engineer to integrate security practices into our DevOps processes. The ideal candidate will be responsible for implementing, managing, and improving our security posture in our software development lifecycle.Key Responsibilities:Incorporate security measures into CI/CD pipelines.Develop and Maintain automation scripts and tools for security processes.Set up and manage security monitoring tools and generate reports on security incidents and vulnerabilities.Work closely with development, operations and security teams to ensure security is intergated into the development lifecycleEnsure that systems and processes comply with security standards and regulationsLead incident response activities, including investigation and mitigation of security breachesConduct regular security assessments, vulnerability scans and penetration testsMaintain up-to-date documentation of security procedures, policies and protocolsProvide training and guidance to teams on security best practices and emerging threatsQualificationsProven experience as a DevSecOps engineer or in a similar roleStrong Knowledge of DevOps pracitces and tools like Jenkins, Git, Docker and KubernetesProficiency in scripting language like Python and BashExperience with security tools like snyk, Fortify and NessusFamiliarity with cloud platforms especially Azure and their security featuresIn-depth understanding of security protocols and security frameworksExcellent problem solving skills and attention to detailStrong communication and collaboration skillsExperience with IaC tools espacially very good and strong in TerraformShould you be interested, please send me a copy of your resume in word format along with the following details ASAP.Full Name:Current Location:Hourly rate on C2C/W2:Work Authorization:Earliest Available date to start:Date and times available to interview:Two Professional References:(Preferably Supervisory references): Regards, RajanMail ID : rajan@quantumworldit.comCell : +1 805-667-0372Linkedin: www.linkedin.com/in/rajanpandeyQuantum World Technologies Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "30_Looking for DEVOPS ENGINEER, Remote": "Mudassir,\nAvance consulting\nmudassir.uddin@avanceservices.com\nReply to: mudassir.uddin@avanceservices.com\nJob Title: DEVOPS ENGINEERLocation: RemoteMode : Contract (6+ Months) ROLES & RESPONSIBILITIES\u2022 Understanding customer requirements and project KPIs\u2022 Implementing various development, testing, automation tools, and IT infrastructure\u2022 Planning the team structure, activities, and involvement in project management activities.\u2022 Managing stakeholders and external interfaces\u2022 Setting up tools and required infrastructure\u2022 Defining and setting development, test, release, update, and support processes for DevOps operation\u2022 Have the technical skill to review, verify, and validate the software code developed in the project.\u2022 Troubleshooting techniques and fixing the code bugs\u2022 Monitoring the processes during the entire lifecycle for its adherence and updating or creating new processes for improvement and minimizing the wastage\u2022 Encouraging and building automated processes wherever possible\u2022 Identifying and deploying cybersecurity measures by continuously performing vulnerability assessment and risk management\u2022 Incidence management and root cause analysis\u2022 Coordination and communication within the team and with customers\u2022 Selecting and deploying appropriate CI/CD tools\u2022 Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline)\u2022 Mentoring and guiding the team members\u2022 Monitoring and measuring customer experience and KPIs\u2022 Managing periodic reporting on the progress to the management and the customer TOOLS & TECHNOLOGIES \u2022 Jenkins\u2022 GitHub Actions\u2022 Continuous integration / Continuous deployment (CI/CD Pipelines)\u2022 Artifactory\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "31_Looking for DEVOPS ENGINEER, Remote": "Mudassir,\nAvance consulting\nmudassir.uddin@avanceservices.us\nReply to: mudassir.uddin@avanceservices.us\nJob Title: DEVOPS ENGINEERLocation: RemoteMode : Contract (6+ Months) ROLES & RESPONSIBILITIES\u2022 Understanding customer requirements and project KPIs\u2022 Implementing various development, testing, automation tools, and IT infrastructure\u2022 Planning the team structure, activities, and involvement in project management activities.\u2022 Managing stakeholders and external interfaces\u2022 Setting up tools and required infrastructure\u2022 Defining and setting development, test, release, update, and support processes for DevOps operation\u2022 Have the technical skill to review, verify, and validate the software code developed in the project.\u2022 Troubleshooting techniques and fixing the code bugs\u2022 Monitoring the processes during the entire lifecycle for its adherence and updating or creating new processes for improvement and minimizing the wastage\u2022 Encouraging and building automated processes wherever possible\u2022 Identifying and deploying cybersecurity measures by continuously performing vulnerability assessment and risk management\u2022 Incidence management and root cause analysis\u2022 Coordination and communication within the team and with customers\u2022 Selecting and deploying appropriate CI/CD tools\u2022 Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline)\u2022 Mentoring and guiding the team members\u2022 Monitoring and measuring customer experience and KPIs\u2022 Managing periodic reporting on the progress to the management and the customer TOOLS & TECHNOLOGIES \u2022 Jenkins\u2022 GitHub Actions\u2022 Continuous integration / Continuous deployment (CI/CD Pipelines)\u2022 Artifactory\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "32_AWS EKS Engineer": "Ben Clark,\nTech Talent Connect LLC\nben@techtalentconnect.com\nReply to: ben@techtalentconnect.com\nClient: AMEXJob Title: AWS EKS EngineerLocation: Phoenix, AZ (Onsite) (Locals preferred)Duration: 6+ Months ContractRate: $60/hr on C2CRequired Skills:\u00b7 Experience with CI/ CD, Git (GitHub, GitLab, BitBucket, SVN), Kubernetes, Linux, AWS\u00b7 Bachelor's degree in information technology, Computer Science or the equivalent combination of training, education, and experience\u00b7 8 years of experience in managing Kubernetes clusters, with a focus on AWS EKS.\u00b7 Experience with CI/CD tools (Jenkins, GitLab CI, CircleCI).\u00b7 Strong experience with AWS services.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "33_NEED - Cloud Architect MORE THAN 10+ YEARS OF CANDIDATES LOOKING FOR USC ONLY - Overland Park, Kansas (onsite)": "Ayushi,\nCygnuspro\nayushi@cygnuspro.com\nReply to: ayushi@cygnuspro.com\nHi,This is Ayushi Kaushik from Cygnus Professionals. I have an urgent job requirement matching your profile. If You are Interested, Please reply with updated resume at ayushi@cygnuspro.com or directly reach me Role: Cloud ArchitectLocation: Overland Park, Kansas (onsite)Duration: Long Term Contract JD :Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutionsCollaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutionsKeeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction.Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.RequirementsSubject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, MonitoringExtensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred.Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)Thanks & RegardsAyushi KaushikCygnus Professionals3490 US Highway 1Princeton, NJ 08540 E: ayushi@cygnuspro.comLinkedIn: https://www.linkedin.com/in/ayushi-kaushik-032706217\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "34_Sr. DevOps Architect": "Zara,\nTechRakers\nzararaza@techrakers.com\nReply to: zararaza@techrakers.com\nJob Title: Sr. Devops ArchitectLocation : Chicago, IL \u2013 Onsite Duration: Long Term List of Mandatory Tools/ Technology: GitHub Platform \u2013 Git Action mandatoryBit Bucket \u2013 Migration to GitHub mandatoryAzure mandatorySonarQubeApp DynamicsKubernetes SUMMARYThis position is responsible for provisioning, managing, monitoring, and decommission services in an in-house or on-premises hosted environment. This role provides support and techniques for automating provision and management of infrastructure in a distributed environment through scripting or infrastructure as code. All tasks are planned with time estimates and performed in a timely manner. The DevOps Engineer will partner with the development team to facilitate better coordination among operations, development, and testing functions by automating and streamlining the integration and deployment processes. WORK PERFORMEDPerforms system administration (Linux/Unix or Windows) at the command-line level.Maintains Docker and Kubernetes infrastructure.Build pipes to migrate on prem work loads to cloudProvides guidance and expertise on system options including identifying risk, impact and costs vs. benefits. Create requirements and development forecasts to allow for timely and accurate planning of projects.Installs and configures solutions, implements reusable components, translates technical requirements, assists with all stages of test data, develop interface stubs and simulators and performs script maintenance and updates.Deploys new modules and upgrades and complete fixes including CI/CD, continuous testing, app performance monitoring, infrastructure settings and configurations.Performs routine application maintenance tasksCreates requirements and procedures for implementing routine maintenance.Troubleshoots existing information systems for errors and resolving those errorsProvisions multi-tier architectures: load balancers, caching, web servers, application servers, databases, and networking. Basic monitoring techniques in a dynamic environment.Infrastructure as Code: design with security, configuration management, integration, deployment, performance monitoring and tuning, automation of infrastructure.Creates automated build and release pipelines for code deploymentsPerforms other duties, as assigned. DOMAIN, KNOWLEDGE, SKILLS AND ABILITIESBanking domain experienceKnowledge of Windows and Linux is requiredExcellent research and self learning skills is requiredKnowledge of a scripting language (PowerShell, Bash etc.) is requiredExcellent time management skills are requiredKnowledge of Azure and Azure portal is requiredKnowledge of Azure Kubernetes, Azure API Management, Azure Application Gateway is an assetStrong documentation skills are requiredKnowledge of Automation Tools (ex. Azure Dev Ops, GitHub (Git Action) is requiredCapability to work independently is required\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "35_AWS Devops Engineer Remote Role": "Shubham Singh,\nStellentit\nshubham@stellentit.com\nReply to: shubham@stellentit.com\nDevOps/Cloud EngineerLocation: RemoteInterview: Phone + SkypeMax Rate: $40-$43/hr on c2c USC/GC Only Job Description:Need a strong DevOps/Cloud Engineer with below hands on skills5+ years\u2019 experienceAWS (ECS/EKS, Lambda, CloudTrail etc)NGINX- Must HaveSplunk Monitoring, queriesNew Relic TerraformJenkinsIBM WebSphere Shubham Singh- Sr. Technical RecruiterEmail- shubham@stellentit.com STELLENT IT \u2013 A Nationally Recognized Minority Certified Enterprise\"Happiness can be found, even in the darkest of times, if one only remembers to turn on the light.\" - JK Rowling\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "36_Azure Terraform Developer": "john,\nVaaridatech\nchand@vaaridatech.com\nReply to: chand@vaaridatech.com\nJob Summary: We are seeking an experienced Azure Terraform Developer to join our team. As an Azure Terraform Developer, you will be responsible for designing, implementing, and maintaining infrastructure as code using Terraform on the Azure cloud platform. You will work closely with our development teams to ensure seamless integration of infrastructure and applications. Responsibilities: - Design and implement scalable and secure infrastructure on Azure using Terraform- Develop and maintain Terraform configurations, modules, and scripts- Collaborate with development teams to integrate infrastructure with applications- Implement infrastructure as code (IaC) best practices and standards- Manage and optimize Azure resources using Terraform- Troubleshoot and resolve infrastructure-related issues- Stay up-to-date with the latest Azure and Terraform features and releases Requirements: - 2+ years of experience in Azure cloud computing and Terraform- Strong understanding of Azure services (e.g., Virtual Machines, Storage, Networking)- Experience with Terraform configuration language (HCL)- Familiarity with Azure DevOps and CI/CD pipelines- Strong programming skills in languages like Python, PowerShell, or Bash- Excellent problem-solving and analytical skills- Bachelor's degree in Computer Science or related fieldNice to Have: - Experience with other cloud platforms (AWS, GCP)- Knowledge of Azure security and compliance best practices- Certification in Azure or Terraform (e.g., Azure Solutions Architect, Terraform Certified Associate)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "37_Need Terraform Engineer || Phoenix, AZ || Onsite": "Prashanth B,\nTekgence\nprashanth.b@tekgence.com\nReply to: prashanth.b@tekgence.com\nTitle: Terraform EngineerLocation: Phoenix, AZDuration: Long term Requried: Need Terraform Engineer with AWS / AZURECoding test is mandatory --Regards,Prashanth B | Tekgence Inc.Talent Acquisition Specialist Email: prashanth.b@tekgence.com | Website: www.tekgence.com6655 Deseo Dr \u2022 Suite 104 \u2022 Irving, TX \u2022 75039\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "38_12+ Years Cloud Architect with AWS &amp; Azure - Washington DC (onsite)": "Susan,\nCapitaltechsolutions\nsusan@capitaltechsolutions.com\nReply to: susan@capitaltechsolutions.com\nHello,We are looking for the candidates for the below position. Please find the below Job Description and update me with the required details.Role : Cloud ArchitectClient: DC Government (Onsite)Location : Washingto, D.C. (Onsite)Responsibilities: Experience spanning at least two IT disciplines, including technical architecture, network management, application development, middleware, database management, security, or operations.Experience with multiple public cloud platforms, including AWS, Azure, and GCP.Experience relocating ERP application to a cloud-native environment.Ensures that standards set by Data, Security, Infrastructure, Platform, and other architecture domains are followed when designing and architecting cloud solutions.Demonstrate deep subject matter leadership of cloud architecture and implementation features (OS, multitenancy, virtualization, orchestration, elastic scalability).Manage the development and implementation of cloud strategy, develop the architecture governance framework, and define architecture foundations.Demonstrates expertise in conveying technical and functional concepts for a specific technical specialty.Identifies improvements to project standards to achieve high quality services/products.Identifies best practices and standards for the use of the product.Delivers support and design for industry specific technologies that require integration with systems or networks.Interacts with executive level business users or technical experts.Functions as a niche technical SME.Lead experience with technical expertise across large, complex implementations for systems.Define and enforce Cloud architecture best practices: availability, redundancy schemes, performance, Disaster Recovery, data, and integration patterns.Troubleshoot issues across the entire stack: IaaS, PaaS, services, applications, network, and security.1. Formulates and defines systems scope and objectives based on both user needs and a thorough understanding of business systems and industry requirements.Devises or modifies procedures to solve complex problems considering computer equipment capacity and limitations, operation time, and form of desired results. Includes analysis of business and user needs, documentation of requirements, and translation into proper system requirements specifications.Provides consultation on complex projects and is considered to be the top-level contributor/specialist of most phases of systems analysis, while considering the business implications of the application of technology to the current and future business environment. Minimum Education/Certification Requirements :Bachelor\u2019s degree in IT or related field or equivalent experience; or a current Azure and AWS Architect Certifications Thanks,Susan,susan@capitaltechsolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "39_Immediate Interview  || AWS LEAD (TECHNICAL LEAD) || San Antonio, TX": "Prashant,\nIntellectt INC\nprashant@intellectt.com\nReply to: prashant@intellectt.com\nHi,Job Title: AWS LEAD (TECHNICAL LEAD)Position Type: CTHLocation: Onsite San Antonio, TX Experience needed: 12\u201320 years We can schedule an immediate interview. Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "40_DevOps Engineer - Contract Role": "Raghu Prasad,\nBlue Ocean Ventures\nraghu.prasad@blue-oceanventures.com\nReply to: raghu.prasad@blue-oceanventures.com\nHi Hope everyone doing well. Role: DevOps EngineerLocation: Alpharetta,GA/Berkeley Heights,NJ/Omaha,Nebraska (5 days a week onsite(100% Onsite)Local profiles with DL As a Kubernetes Container Platform Engineer you will play a pivotal role in designing, implementing, and operating container services platforms across on-premises and cloud environments. Your focus will be on seamless integration with other tools through robust Continuous Integration (CI) and Continuous Delivery (CD) pipelines. Here are the key responsibilities:Key Responsibilities:Platform Design and Implementation:Architect, implement, and operate container services platforms, emphasizing integration with CI/CD pipelines.Leverage automation to enhance efficiency and minimize manual tasks.Meticulously assess risks while ensuring task efficiency.Technical Issue Resolution:Analyze, diagnose, replicate, and resolve technical issues reported by customers using Fiserv PaaS and CaaS platforms.Develop maintainable, reusable software from a consumer-centric perspective.Take ownership of unresolved issues, seeking in-depth knowledge for timely resolution.Support and Documentation:Handle and maintain support requests from business unit customers.Collaborate with other engineering groups to triage customer and business unit issues.Contribute to building and peer-reviewing knowledge base articles and product documentation.Qualifications:Experience and Expertise:Minimum of 5 years of hands-on experience building and maintaining Container Platforms (e.g., PAS/TAS, PKS/TKGI, GKE, IBM Redhat OpenShift, ARO, ROSA).Proficiency in deploying and supporting Kubernetes on at least one public IAAS provider (Azure, Amazon, GCP, Alibaba).Strong networking fundamentals, including TCP/IP, subnetting, socket vs. connect timeouts, routing, and DNS.Experience designing and implementing Infrastructure as Code APIs using tools like Swagger.Solid understanding of virtual computing environments, including Hypervisors and load balancing concepts.Familiarity with Identity Management, Access Management, and Certificate Management.Demonstrated competency in programming/scripting languages (with a preference for Go, Ansible, Terraform, and Python).Hands-on experience with distributed systems (Windows and Linux) and virtualization software (VMware).DevOps Practices:Understanding of DevOps practices, with hands-on experience in Automation/Continuous Integration using tools like Jenkins, Harness, RunDeck, GitLab, and GitHub.Additional Skills (Preferred):Experience with NSX, NSX-T, and Panorama.Familiarity with Vsphere7 and TKG.Background in DevOps methodologies.Excellent communication skills.On-call experience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "41_Sr. DevOps Architect": "Zara,\nTechRakers\nzararaza@techrakers.com\nReply to: zararaza@techrakers.com\nJob Title: Sr. Devops ArchitectLocation : Chicago, IL \u2013 Onsite Duration: Long Term List of Mandatory Tools/ Technology: GitHub Platform \u2013 Git Action mandatoryBit Bucket \u2013 Migration to GitHub mandatoryAzure mandatorySonarQubeApp DynamicsKubernetes SUMMARYThis position is responsible for provisioning, managing, monitoring, and decommission services in an in-house or on-premises hosted environment. This role provides support and techniques for automating provision and management of infrastructure in a distributed environment through scripting or infrastructure as code. All tasks are planned with time estimates and performed in a timely manner. The DevOps Engineer will partner with the development team to facilitate better coordination among operations, development, and testing functions by automating and streamlining the integration and deployment processes. WORK PERFORMEDPerforms system administration (Linux/Unix or Windows) at the command-line level.Maintains Docker and Kubernetes infrastructure.Build pipes to migrate on prem work loads to cloudProvides guidance and expertise on system options including identifying risk, impact and costs vs. benefits. Create requirements and development forecasts to allow for timely and accurate planning of projects.Installs and configures solutions, implements reusable components, translates technical requirements, assists with all stages of test data, develop interface stubs and simulators and performs script maintenance and updates.Deploys new modules and upgrades and complete fixes including CI/CD, continuous testing, app performance monitoring, infrastructure settings and configurations.Performs routine application maintenance tasksCreates requirements and procedures for implementing routine maintenance.Troubleshoots existing information systems for errors and resolving those errorsProvisions multi-tier architectures: load balancers, caching, web servers, application servers, databases, and networking. Basic monitoring techniques in a dynamic environment.Infrastructure as Code: design with security, configuration management, integration, deployment, performance monitoring and tuning, automation of infrastructure.Creates automated build and release pipelines for code deploymentsPerforms other duties, as assigned. DOMAIN, KNOWLEDGE, SKILLS AND ABILITIESBanking domain experienceKnowledge of Windows and Linux is requiredExcellent research and self learning skills is requiredKnowledge of a scripting language (PowerShell, Bash etc.) is requiredExcellent time management skills are requiredKnowledge of Azure and Azure portal is requiredKnowledge of Azure Kubernetes, Azure API Management, Azure Application Gateway is an assetStrong documentation skills are requiredKnowledge of Automation Tools (ex. Azure Dev Ops, GitHub (Git Action) is requiredCapability to work independently is required\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "42_Cloud Program Manager": "Sai Sandesh,\nTetrahed\nsai@tetrahed.com\nReply to: sai@tetrahed.com\nHello Hope you're doing wellRole: Cloud Program ManagerLocation: New York, NY \u2013 3 days in the officeVisa: USC,GC,EADLocal candidates onlyMust be open to onsite interview if requested Responsibilities10-15 total years of experience5-7 plus years of cloud project management experience with the ability to manage and drive iterative technology and architecture deliveries working with cross functional teamsDefine and manage key measurements in support of architecture visionEstablish decision-making model to calculate business benefits, identify priority and phasing of architecture strategy implementationsOwn and drive iterative cadences and report progress, risks and issues via demos and metrics. Ability to use agile toolkit (e.g. Jira) Required SkillsStrong knowledge of Cloud (Azure, AWS) and experience leading cloud migrations.Experience within a large complex corporate organization or major consulting organizationHighly versed in Scrum and Kanban methodologies.Technical background in integration, development, testing, and/or infrastructure.Quick learner capable of understanding and implementing front-to-back solutions and performing in-depth analysis as required to resolve issues.Excellent relationship management, communication, teamwork, and influence skills; ability to operate at senior levels in both written and verbal communications.Ability to develop full-scale project plans including identifying and managing project dependencies and critical path.Ability to proactively manage change in project scope, identify potential issues, and devise contingencies.Ability to effectively prioritize and execute tasks in a high-pressure environment.Detail oriented, organized, self-motivated, and able to work independently as well as in a team environment.Strong familiarity with Agile project management software/tools (e.g., JIRA, etc.). Proficient with MS Office applications: Excel, MS Project, Visio, Word, and PowerPoint.Thanks and Regards,Sai SandeshTetrahed Inc.sai@tetrahed.comlinkedin.com/in/sai-sandesh-reddy-0372062a9\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "43_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "44_Azure Cloud Architect-Clayton, Missouri (Hybrid)": "Sai Supriya,\nyochana\nsupriya@yochana.com\nReply to: supriya@yochana.com\nHello ,Hope you are doing great!!!This is Sai Supriya from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your time.Title : Cloud ArchitectLocation : Clayton, Missouri (Hybrid) Remote : No Role/ JD: HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutions Keeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction. Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred. Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "45_Cloud Service Manager || Dallas, TX(onsite)": "khushboo,\nDigital Dhara\nkhushboo@digitaldhara.com\nReply to: khushboo@digitaldhara.com\nRole : Cloud Service ManagerLocation : Dallas, TX(onsite)We are seeking a highly skilled Cloud Architect to join our team and take charge of managing and maintaining our customer organization\u2019s cloud architecture.In this role, you will be responsible for build and operate the cloud environment of customer spanned across public clouds (AWS, Azure and GCP). Also, responsible to play the service manager role in the day-to-day cloud operations. Responsibilities: Work on day-to-day cloud operations requirement to make sure 100% availability of cloud environment.Work closely with customer cloud team to understand the new requirements and provide the solutions.Respond to technical issues in a professional and within the agreed SLA.Identify the top cloud architecture solutions to successfully meet the needs of the companyLead customer through cloud adoption and establish best practicesManage cloud ops team. Skills:Proven work experience as a Cloud Architect on public clouds (AWS, Azure and GCP)Strong communication skill to handle the P1/P2 incidentsWork experience on all foundation services on public cloud (AWS, Azure and GCP) like, compute, Storage, Networking, IAM, cloud DNS, Cloud Firewall, Disaster recovery, Cloud Databases.Work experience in ITSM tools like Service Now and well versed with ITIL processes.Multiple technical teams' coordination skills to deliver end to end service in a hybrid IT environment. Thanks & Regards,KhushbooKhushboo@digitaldhara.comPh: 609-791-0074 Ext 115\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "46_Urgent need for  Cloud Architect": "swathi,\nYochana IT solutions\nswathin@yochana.com\nReply to: swathin@yochana.com\nHi,This is Swathi from Yochana IT Solutions. i tried reaching you unfortunately went to the VM; this call is regarding the job opportunity. if you are in the job market please get back to this number (949) 577-7136 or Reach me at swathin@yochana.com Title: Cloud ArchitectLocation: Clayton, Missouri (Hybrid)Type: contractImplementation: HCL TechVisa: GC and CItizensJob description:Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutions Keeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction. Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred. Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Thanks & RegardsSwathi NerallaResource SpecialistDirect: (949) 577-7136Email: swathin@yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "47_Node.js Backend Developer + AWS :: Bay Area, CA (Hybrid at Mountain View office)": "Ekta Chaudhary,\nEpeople Technologies\nekta@epeopletech.com\nReply to: ekta@epeopletech.com\n5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "48_AWS Cloud Data bricks Architect - Remote": "Rohit,\nmsys inc\nresume@msysinc.com\nReply to: resume@msysinc.com\nTitle: AWS Cloud Data bricks Architect - RemoteLocation: RemoteLength: Long termRestriction: W2 or C2CSend resume to: rohit@msysinc.com Description:**** Webcam interview; *** Long term project *** Remote ****Responsibilities:Data Architecture Design: Design, develop, and maintain scalable data architectures on AWS to support data warehousing, analytics, and business intelligence applications.Data Warehousing: Build and manage data warehouses using AWS services such as Redshift, S3, and Glue. Ensure optimal performance, scalability, and reliability.Data bricks Integration: Utilize Data bricks for data processing, transformation, and analysis. Develop and optimize ETL/ELT pipelines using Data bricks and other AWS tools.Data Modeling: Create and maintain complex data models, including conceptual, logical, and physical models. Implement dimensional models to support analytical and reporting needs.Solution Design: Convert business requirements into technical solutions. Work with stakeholders to gather requirements, define project scope, and develop detailed technical specifications.Best Practices: Implement industry best practices for data management, data governance, and data security. Ensure compliance with relevant regulations and standards.Collaboration: Work closely with data scientists, analysts, and business stakeholders to understand data requirements and deliver solutions that meet their needs.Primary Skills:Data ModelingDimension ModelingData bricksAWS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "49_Azure Data Architect  On C2C": "Vijay,\nYochana IT\nvijay@yochana.com\nReply to: vijay@yochana.com\nHi There, My Name is Vijay Kumar from Yochana IT. I hope you are doing great. We have an urgent requirement with one of our client for \u201cAzure Data Architect | Contract\u201d Kindly go through the requirement below and let know you interest and share me your Updated Word Document Resume at Vijay@yochana.com or Call Me at 248-306-8171. My apologies if this position is not an ideal fit, we'll keep you in mind for other suitable positions and referrals would be appreciated Role : Azure Data Architect | ContractLocation : (Durham/Raleigh, NC)-Onsite JD: o Relevant experience in data engineering tools like Azure Data pipelines , Azure Data factory, Azure Data Lake Storage, Databricks,o Delta Lake, Python, SQL Server, and experience with Azure Cloudo Strong knowledge and experience designing and implementing solutions on Databases like Hadoop, SQL Server, Oracle, Azure Synapseo Demonstrated experience in building, tuning data pipelines on Spark, Cloud native tools like Azure Data Factory, Databricks and ETL toolso Hands on Azure Data architecture experienceo Optimize Azure Synapse databaseo Performance tuning of workload and recommend adjustmento Understand the need for both batch and stream ingestion of the data from source databases like SQL Server, Oracle, MySQL and other RDBMS based databases.o Experience demonstrating and ability to talk about wide variety of data engineering tools, architectures across cloud providers and open-source tools and packages.o Experience working with business teams, dev teams and coming up with designs to deliver data solutions with best practices and standards. Thanks & Regards Vijay kumarYochana IT Solutions inc23000 Commerce MI-48335Email: Vijay@yochana.comDirect: 248-306-8171 (If I missed your Call Please E-mail me)https://www.yochana.com/Linked in- ID : https://www.linkedin.com/in/vijay-kumar-aousharala-bb13a11bb/GitHub ID : https://github.com/VijayKumaraousharala Fishbowl : https://www.fishbowlapp.com/fb/vijay-kumar-gnwe LinkedIn Group : https://www.linkedin.com/groups/12904724/ \u201cJoin the Referral Revolution of Yochana by Sharing, Earning, and Empowering!\u201d. Ask us about our rewarding referral program. Note: If you have received this mail in error or prefer not to receive such emails in the future, please reply with \u201cREMOVE\u201d in the subject line and the email id(s)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "50_Cloud Migration and Infrastructure Lead": "Amarendra,\nsmartitframe\namarendra@smartitframe.com\nReply to: amarendra@smartitframe.com\nHello, I hope you\u2019re doing well Please check below job description and let me know if interested to apply. Job Position: Cloud Migration and Infrastructure LeadLocation: Atlanta, GA (Onsite) Job Description:Candidates should have 10-12 years in IT Infra management space along with strong experience in datacentre migration to public cloud specifically GCP.Leads the migration team on technical aspects under the guidance of the architect.Delivers an outcome of infrastructure readiness for the data centre migrationResponsible for migration/creation of x86 workloads and relative infrastructure at GCP as per move group planning by US MarketResponsible for delivering technical tasks on time, assigned during the migration phase.Responsible for creating the component sheet for each move group.Technical discussion related to infrastructure (under TCS Scope of work) with application teams along with architectResponsible to ensure installation and function of GIS Infrastructure and Security tools on all the migrated workloads in GCPResponsible for ensuring patching compliance on all the migrated workloads in GCPResponsible to ensure mandatory tags applied on all the migrated workloads in GCPResponsible to handle and resolve technical escalation under TCS scope of workSkills:NO remote , need to be in Customer office in Atlanta Strong experience in leading a technical team and public cloud migration projects.Strong understanding and hands on experience of GCP & AWS Cloud Infrastructure, preferably GCP certifiedStrong experience Windows / Active Directory AdministrationExperience in Datacentre migration (On-prem to GCP Cloud)Experience in Cloud backup and network technologiesExperience with virtualization technologies Excellent communication skillsThanks and RegardsAmarendra SrivastavaUS IT Recruiter amarendra@smartitframe.comSmart IT Frame LLC.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "51_Cloud Reliability Engineer !! Remote !! USC-GC": "Tanupriya,\nPransu tech solutions\ntanupriya@pransutechsolutions.com\nReply to: tanupriya@pransutechsolutions.com\nRemote Banking experience requiredCloud Reliability EngineerRole and ResponsibilitiesReporting to the Head of Cloud/API Engineering, the Cloud Reliability Engineer will play a critical role in driving innovation and growth for the Banking Solutions business. In this role, the candidate will have the opportunity to make a lasting impact on the company's digital transformation journey, drive customer-centric innovation and automation, and position the organization as a leader in the competitive digital banking landscape. Specifically, the Cloud Reliability Engineer will be responsible for the following:Strategize and drive the building blocks of reliability engineering as we make the transition from private to public cloud.Ensure the reliability, availability, and performance of applications and services, focusing on minimizing downtime, optimizing response times, and maintaining high availability for users.Lead incident response efforts for incidents, including identification, triage, resolution, and post-incident analysis to prevent recurrence and improve system resilience.Develop and maintain monitoring solutions and alerting mechanisms for infrastructure, application performance, and user experience metrics, enabling proactive issue detection and mitigation.Implement automation tools and processes to automate routine tasks, scale infrastructure, and ensure seamless deployments, updates, and rollbacks with minimal user impact.Conduct capacity planning, performance tuning, and resource optimization for environments, collaborating with development and operations teams to meet scalability and performance goals.Collaborate with security teams to implement security best practices, perform vulnerability assessments, and ensure compliance with security standards and regulatory requirements for applications.Manage deployment pipelines, release processes, and configuration management for app deployments, ensuring consistency, reliability, and version control across environments.Identify areas for improvement in reliability, performance, and efficiency through data analysis, root cause analysis, and trend analysis, and drive initiatives to enhance system reliability and operational efficiency.Create and maintain documentation, runbooks, and knowledge base articles for operational procedures, troubleshooting guides, and best practices, and promote knowledge sharing within the team.Develop and test disaster recovery plans, backup strategies, and failover mechanisms for app services, ensuring business continuity and data integrity in case of failures or disasters.Collaborate with development, QA, DevOps, and product teams to ensure alignment on reliability goals, performance metrics, release schedules, and incident response processes.Participate in on-call rotations and provide 24/7 support for critical incidents, troubleshoot issues, and coordinate with teams for resolution, escalation, and follow-up actions as per defined SLAs.Professional QualificationsSpecific experience in reliability engineering for a large-scale transition from private to public cloud and strategies for such.Proficient in development technologies, architectures, and platforms (web, api) to understand system complexities and performance considerations.Experience in cloud platforms (e.g., AWS, Azure, Google Cloud) and infrastructure as code (IaC) tools for managing app infrastructure and deployments.Knowledge of monitoring tools (e.g., Dynatrace, Logrocket, DataDog) and logging frameworks (e.g., ELK Stack) for real-time visibility into system health, performance metrics, and user experience.Experience in incident management, including incident response, triage, root cause analysis (RCA), and post-mortem reviews to prevent recurring issues.Strong troubleshooting skills to diagnose complex technical issues in app environments, infrastructure, networking, and performance bottlenecks.Proficiency in scripting languages (e.g., Python, Bash) and automation tools (e.g., Ansible, Terraform) for automating routine tasks, deployments, and infrastructure management.Experience in implementing continuous integration/continuous deployment (CI/CD) pipelines for apps using tools like Jenkins, GitLab CI/CD, or Azure DevOps.Expertise in setting up monitoring solutions, configuring alerts, and creating dashboards to monitor system performance, application metrics, and user experience.Familiarity with APM (Application Performance Monitoring) tools to analyze app performance, identify bottlenecks, and optimize resource utilization.Commitment to continuous learning, staying updated with industry trends, new technologies, and best practices in app reliability, performance, and operations.Adaptability to evolving requirements, technologies, and business needs, with a focus on driving continuous improvement and operational excellence.Personal CharacteristicsDemonstrates judgment and flexibility; thinks about issues and develops solutions that thoughtfully take the broader context into account - positively deals with a shifting demand for time, priorities, and the rapid change of environments.Takes an ownership approach to engineering and product outcomes.Action-oriented self-starter who can set strategy and drive execution with a \u201croll up the sleeves\u201d approach.Excellent interpersonal communication, negotiation and influencing skills to work effectively with all stakeholders (internal & external), making information-based decisions.Penchant for excellence, both personally and professionally, demonstrated by intellectual curiosity, record of accomplishment, and reputation; shows strong attention to detail and implementation of best practices with an inclination for continuous improvement.Ability to quickly establish strong credibility with employees, business partners and external resources.Embodies and delivers the firm's values and culture towards colleagues, clients, and communities:Win as one teamLead with integrity Thanks & Regards, Tanupriya Singh |Technical RecruiterPransuTech Solutions|www.pransutechsolution.comEmail:Tanupriya@pransutechsolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "52_Urgent Need: AWS Data Architect || New York || Hybrid|| Skype": "Ashish Kumar,\nTek inspirations LLC\nashish.kumar@tekinspirations.com\nReply to: ashish.kumar@tekinspirations.com\nHi All Hope you are doing wellJob Description -Role: Data Architect- AWS (Data Visualization and Migration Specialist) Visa: CITIZEN,GC EAD,GREEN CARD,H4 EAD,L2 EAD,TN PermitLocation: NYC, NY Local with DLClient-Mizuho BankOnsite Hybrid LinkedIn id is must (Should not been created recently, at least 6 Years old in case of senior candidates)Job DetailsWe are looking for an experienced Data Visualization and Migration Specialist with a strong background in AWS QuickSight, AWS Redshift, and Amazon Q. The ideal candidate will have a proven track record of working with data visualization tools, as well as extensive experience in data migration activities, specifically moving data from on-premises data stores to Amazon Redshift.Key Responsibilities:Design, develop, and maintain interactive dashboards and reports using AWS QuickSight.Leverage Amazon Q for natural language query capabilities to enhance data accessibility and usability.Manage and optimize AWS Redshift data warehouses, ensuring data integrity and performance.Conduct data migration from on-premises data stores to AWS Redshift, including planning, execution, and validation.Collaborate with cross-functional teams to understand business requirements and translate them into effective data visualizations and reporting solutions.Implement best practices for data management, data quality, and data governance.Troubleshoot and resolve issues related to data visualization and data migration.Provide training and support to end-users on using AWS QuickSight and Amazon Q.Stay updated with the latest trends and technologies in data visualization and data migration.Required Qualifications:Bachelor\u2019s degree in Computer Science, Information Systems, Data Science, or a related field.4-5 years of experience working with data visualization tools, specifically AWS QuickSight.Strong expertise in AWS Redshift, including data warehousing and performance optimization.Experience with Amazon Q for enhancing data accessibility.Proven experience with data migration activities, particularly moving data from on-premises data stores to Amazon Redshift.Proficient in SQL and database management.Strong analytical and problem-solving skills.Excellent communication and collaboration skills.Ability to work independently and manage multiple projects simultaneously. Preferred Qualifications:AWS Certification in any of the following: AWS Certified Solutions Architect, AWS Certified Data Analytics, AWS Certified Big Data.Experience with other data visualization tools like Tableau or Power BI.Knowledge of ETL processes and tools.Experience with scripting languages such as Python or R.Regards, Ashish Kumar Senior Technical RecruiterTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035.Direct: +1 469-898-0378 | Email: - ashish.kumar@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "53_Looking for Azure DevOps Engineer need 10+yrs need locals to NY and Princeton NJ": "Austin varma,\nEminencets\naustin@eminencets.com\nReply to: austin@eminencets.com\nRole: Azure DevOps EngineerDuration: Long TermLocation: Princeton NJ or NYC office (1166 Avenue ofAmericas, New York)Hybrid: 3 days WFORoles and Responsibilities:(4-5 Years Azure experience,Expertise in deploying .NET based apps, Docker/Kubernetes/GHActions/Azure DevOps) Thanks, and regards.Austin VarmaRecruiter Eminence Technology Solutions LLCEmail: Austin@eminencets.comWebsite: www.eminencets.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "54_Hiring Azure DevOps Solution Architect aat  333 S Wabhash Ave Chicago, IL 60604 (100% Day One Onsite)": "srinath,\nCalabitek INC\nsrinath@calabitek.com\nReply to: srinath@calabitek.com\nJOB DETAILS: Job Title: Azure DevOps Solution ArchitectJob Location: 333 S Wabhash Ave Chicago, IL 60604 (100% Day One Onsite)Job Type: Long TermOpenings: 1Hiring Type: Video Call any visa ( no Opt ,CPT ) JD: Software Solution Architect This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications: - 14+ Yrs of experience in IT/DevOps/Cloud Environment.- Bachelor's degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Preferred Qualifications: - Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure Administrator).- Experience with other cloud platforms (AWS, Google Cloud).- Familiarity with containerization and orchestration tools like Docker and Kubernetes Preferred tool experience: - Microsoft Azure DevOps, JIRA, Bitbucket, Bamboo, ALM, Load Runner/Performance Center, JMeter, GitHub, Jenkins, Microfocus UFT/Lean FT, Selenium, Eclipse, ServiceNow, Perfecto, SeaLights, Gremlin, ToxiProxy, Chaos Monkey, Selenium Grid.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "55_Cloud Architect": "Aditya Raghuvanshi,\nVyze\naraghuvanshi@vyzeinc.com\nReply to: araghuvanshi@vyzeinc.com\nJob Description -Job Title : Cloud Architect Job Location : Richmond, VAClient is VA DSSIt\u2019s remote but VA folks are always good to send over. *This position is remote; however, the selected candidate must reside in Eastern or Central time zone only. Responsibilities:Lead the development and implementation of a governance strategy and framework for all Dynamics 365 and Power Platform solutions, within the single Commonwealth of Virginia (COV) tenant, shared with over 60 independent executive branches.Will be the Power Platform technical liaison between the stakeholders, IT, vendors and the Information Technology Agency.Understanding the Power Platform environments strategy designed by, and ensure the vendors and citizen developers comply with the governance strategy.Provide technology guidance, road maps, principles, standards, and best practices.Work with business stakeholders, vendors, and engineering teams to implement a technical architecture, governance framework, and best practices to develop efficient solutions, product capabilities and repeatable processes.Act as primary platform architect and technical lead for Dynamics 365 and M365 Power Platform suite (Power Pages, Power Apps, Power Automate, Power BI, Power Virtual Agents)Work on highly complex projects and turnkey solutions that require in-depth knowledge across multiple specialized architecture domains, including Azure IaaS, Microsoft Entra ID, Azure DevOps, Okta Identity management, azure storage accounts and Azure app registrations.Analyze 3rd-party products to document overall solution and system architectures, and identity/permissions security models.Contribute to the Product features roadmaps and to the Power Platform Community of Practice (CoP models)Ensure high-quality platform deliverables in accordance with the platform roadmap and capabilitiesDefine delivery documentation including best practices, design, build and operations guides, as well as the management processes and proceduresLead efforts in defining, documenting and estimating the following for enterprise applications.Design/Implement Enterprise Security/technology PatternsAlign architecture strategy with business goals.Develop and communicate architectural policies, standards, guidelines, and procedures.Promote the governance framework, outcomes, and results to the organization and senior leadership.Review and/or analyze and develop architectural requirements as needed for the organization.Ensure the conceptual completeness of the technical solutions.Review existing business processes and establishes metrics to improve business processes, as well as support of all architectural disciplines under their direction.Tackle complex and ambiguous problems that span numerous systems and organizationsCollaborate with project management and IT leaders to ensure progress towards architectural alignment with project goals and requirements.Resolve conflict productively across teamsKnowledge, Skills and Education:In-depth knowledge and experience with Dynamics 365, Microsoft Power Platform, Power BI, Power Pages, Power Apps, and Power Automate.15+ years of experience in technology industry and IT consulting services.Experience managing end-to-end product deployment from concept, development, and successful launch.Experience leading programs with full customer journey impact covering program planning, risk assessment, cost, and delivery.Proven Experience in managing delivery of services / products related to M365 and MS technology in a GCC environmentProven ability to create product roadmap, best practices, development and deployment standards for M365 and its productsDemonstrated communication and influencing skills at all levels of the organization and across technical and non-technical teamsBS/MS in Computer Science, Computer Engineering & Process AutomationRequired Skills:5 Years - Azure Power Apps5 Years - Azure Power Pages5 Years - Dynamics 36510 Years - Technical Program or Technical Product management10 Years - Cloud and hybrid, on-premises infrastructures, architecture designs, industry standards, and/or technology management\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "56_Onsite Contract Position for AWS Data Application Infrastructure Architect (with Big4 Consulting Companies Exp) in Atlanta, GA": "Gaurav Sharma,\nUSG Inc.\ngaurav.ks@usgrpinc.com\nReply to: gaurav.ks@usgrpinc.com\nHi, Hope you are doing great.Please go through the job description given below and if you are interested do share an updated word copy of your resume and best time to reach you over the phone. Position: AWS Data Application Infrastructure LeadLocations: Atlanta, GA 30301Duration: Contract Job Description:Manage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience in Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship management Thanks with regards,Gaurav Sharma | Sr. Technical RecruiterEmail ID: gaurav.ks@usgrpinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "57_Senior Azure Cloud Engineer ( F2F INTERVIEW --MD Location )": "peter,\nvsiiusa\npeter@vsiiusa.com\nReply to: peter@vsiiusa.com\nSenior Azure Cloud Engineer Rockville, MD. (Hybrid 1-2 days a week in office at fisher\u2019s lane)Local or nearby state candidates.F2F is must. Interested candidate can apply.Certification is mandatory. Key Responsibilities:\u2022 Azure Administrator - Provision, configure, manage, and monitor Azure virtual machines (Window VMs), storage accounts, IaaS, PaaS, and networking components; Perform regular OS and security patching, backups/restore, and implement disaster recovery strategies for Azure resources with maintenance, trouble shooting.\u2022 Azure Security \u2013 be able to set up and manage Microsoft Defender for Cloud, Azure Key Vault, network security, data encryption and related security compliance and remediation.\u2022 DevOps \u2013 be able to set up and integrate related DevOps process, managing CI/CD pipelines for Azure applications.\u2022 Azure Monitoring \u2013 be able to set up Azure Monitor, Log Analytics, Network Watcher, and Azure Application Insights with relate automated alerts in emails based on pre-configured rules and thresholds and perform related monitoring and trouble shooting.\u2022 Expert in Azure App Service, Azure SQL Database, Azure Application Gateway (WAF, load balancing), etc. including set up, configuration and support.Job Requirements:\u2022 10 years\u2019 experience as Azure Cloud Administrator or Azure Cloud Engineer Peter VisionSoft International Inc (www.vsiiusa.com)Email : peter@vsiiusa.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "58_Urgent Requirement for \"Cloud Architect\" :: Richmond, VA:: CITIZEN,GREEN CARD,GC EAD": "Alok Panigrahi,\nAdventa Tech INC\nalok@adventatech.com\nReply to: alok@adventatech.com\nJob Description -I NEED SOMEONE WHO IS HAVING STATE CLIENT EXPERIENCECloud Architect for a long-term contract opportunity clients in the Richmond, VA area. *This position is remote; however, the selected candidate must reside in Eastern or Central time zone only. Responsibilities:Lead the development and implementation of a governance strategy and framework for all Dynamics 365 and Power Platform solutions, within the single Commonwealth of Virginia (COV) tenant, shared with over 60 independent executive branches.Will be the Power Platform technical liaison between the stakeholders, IT, vendors and the Information Technology Agency.Understanding the Power Platform environments strategy designed by, and ensure the vendors and citizen developers comply with the governance strategy.Provide technology guidance, road maps, principles, standards, and best practices.Work with business stakeholders, vendors, and engineering teams to implement a technical architecture, governance framework, and best practices to develop efficient solutions, product capabilities and repeatable processes.Act as primary platform architect and technical lead for Dynamics 365 and M365 Power Platform suite (Power Pages, Power Apps, Power Automate, Power BI, Power Virtual Agents)Work on highly complex projects and turnkey solutions that require in-depth knowledge across multiple specialized architecture domains, including Azure IaaS, Microsoft Entra ID, Azure DevOps, Okta Identity management, azure storage accounts and Azure app registrations.Analyze 3rd-party products to document overall solution and system architectures, and identity/permissions security models.Contribute to the Product features roadmaps and to the Power Platform Community of Practice (CoP models)Ensure high-quality platform deliverables in accordance with the platform roadmap and capabilitiesDefine delivery documentation including best practices, design, build and operations guides, as well as the management processes and proceduresLead efforts in defining, documenting and estimating the following for enterprise applications.Design/Implement Enterprise Security/technology PatternsAlign architecture strategy with business goals.Develop and communicate architectural policies, standards, guidelines, and procedures.Promote the governance framework, outcomes, and results to the organization and senior leadership.Review and/or analyze and develop architectural requirements as needed for the organization.Ensure the conceptual completeness of the technical solutions.Review existing business processes and establishes metrics to improve business processes, as well as support of all architectural disciplines under their direction.Tackle complex and ambiguous problems that span numerous systems and organizationsCollaborate with project management and IT leaders to ensure progress towards architectural alignment with project goals and requirements.Resolve conflict productively across teamsKnowledge, Skills and Education:In-depth knowledge and experience with Dynamics 365, Microsoft Power Platform, Power BI, Power Pages, Power Apps, and Power Automate.15+ years of experience in technology industry and IT consulting services.Experience managing end-to-end product deployment from concept, development, and successful launch.Experience leading programs with full customer journey impact covering program planning, risk assessment, cost, and delivery.Proven Experience in managing delivery of services / products related to M365 and MS technology in a GCC environmentProven ability to create product roadmap, best practices, development and deployment standards for M365 and its productsDemonstrated communication and influencing skills at all levels of the organization and across technical and non-technical teamsBS/MS in Computer Science, Computer Engineering & Process AutomationRequired Skills:5 Years - Azure Power Apps5 Years - Azure Power Pages5 Years - Dynamics 36510 Years - Technical Program or Technical Product management10 Years - Cloud and hybrid, on-premises infrastructures, architecture designs, industry standards, and/or technology management\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "59_Cloud Architect ONSITE - Candidate MUST be local to the DC metro area": "sunil kumar,\npransu tech solutions\nsunil@pransutechsolutions.com\nReply to: sunil@pransutechsolutions.com\nJOB TITLE: 740713 - OCTO (ECIS) Cloud ArchitectLocation: ONSITE - Candidate MUST be local to the DC metro areaDuration: 6+ monthsVisa: GC/USC OnlyClient: DC Government SHORT DESCRIPTION: 8-10 years of experience. Subject Matter Expert in Azure and AWS cloud infrastructure and network technologies. COMPLETE JOB DESCRIPTION: Responsibilities: 1. Experience spanning at least two IT disciplines, including technical architecture, network management, application development, middleware, database management, security, or operations. 2. Experience with multiple public cloud platforms, including AWS, Azure, and GCP. 3. Experience relocating ERP applications to a cloud-native environment. 4. Ensures that standards set by Data, Security, Infrastructure, Platform, and other architecture domains are followed when designing and architecting cloud solutions. 5. Demonstrate deep subject matter leadership of cloud architecture and implementation features (OS, multitenancy, virtualization, orchestration, elastic scalability). 6. Manage the development and implementation of cloud strategy, develop the architecture governance framework, and define architecture foundations. 7. Demonstrates expertise in conveying technical and functional concepts for a specific technical specialty. 8. Identifies improvements to project standards to achieve high-quality services/products. 9. Identifies best practices and standards for the use of the product. 10. Delivers support and design for industry-specific technologies that require integration with systems or networks. 11. Interact with executive-level business users or technical experts. 12. Functions as a niche technical SME. 13. Lead experience with technical expertise across large, complex implementations for systems. 14. Define and enforce Cloud architecture best practices: availability, redundancy schemes, performance, Disaster Recovery, data, and integration patterns. 15. Troubleshoot issues across the entire stack: IaaS, PaaS, services, applications, network, and security.1. Formulates and defines systems scope and objectives based on both user needs and a thorough understanding of business systems and industry requirements. 16. Devises or modifies procedures to solve complex problems considering computer equipment capacity and limitations, operation time, and form of desired results. Includes analysis of business and user needs, documentation of requirements, and translation into proper system requirements specifications. 17. Provides consultation on complex projects and is considered to be the top-level contributor/specialist of most phases of systems analysis, while considering the business implications of the application of technology to the current and future business environment. MINIMUM EDUCATION/CERTIFICATION REQUIREMENTS: \u2713 Bachelor\u2019s degree in Information Technology, or related field or equivalent experience,\u2713 or a current Azure and AWS Architect Certifications Candidate must complete/return the skill matrix below\u2026740713 - OCTO (ECIS) Cloud ArchitectCLIENT REQUIREMENTSItemSkillsRequired orYears ofCandidate's Years of Exp.DesiredExperience1Cloud DevOps experienceRequired5 2Broad technology experience across several of these areas including, applications development, relational or NoSQL database, DevOps, containers, CI/CDRequired5 3Must have minimum of 8 years of experience in the design and implementation of cloud workloads in Azure, AWS, or GCPRequired8 4Experience with cloud networking and network security, including virtual networks, network security groups, cloud-native firewalls, etc.Required8 5Bachelor\u2019s degree in IT or related field or equivalent experienceRequired 68 -10 yrs. experience in one or more architecture domains (e.g., business architecture, solutions architecture, application architecture)Required8 78 -10 yrs. preparing complex technical documentationRequired8 88 - 10 yrs. experience in managing large operational cloud environments spanning multiple tenants through techniques such as Multi-Account managementRequired8 9Minimum 3 years of microservice architectural experienceRequired3 10Minimum of 3 years of experience working exclusively on designing and implementing cloud-native workloads.Required3 11Experience with application lifecycle tools (Git, GitHub, Jenkins, Bamboo, Azure DevOps, Ansible, Terraform, CloudFormation, etc.).Required8 12Certified AWS Solutions Architect ProfessionalRequired 13Certified Microsoft Azure Solutions Architect ProfessionalRequired\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "60_Cloud Engineer :: Remote :: USC::H4 only": "Suraj,\nInfotech Spectrum\nsuraj.mishra@infotechspectrum.com\nReply to: suraj.mishra@infotechspectrum.com\nHello My name is Suraj Mishra, and I am a Staffing Specialist at Infotech Spectrum. I am reaching out to you on an exciting job opportunity with one of our clients. POSITIONCloud EngineerLOCATIONRemoteDURATION12 Month CTHREQUIRED SKILLSReact, Angular, Spring, Node and experience in migrating application using these platforms to public cloudAWS, Azure, or Private CloudDocker, Kubernetes OWASP Top 10, GDPR, HIPAABash, Python Thank YouBest RegardsSuraj MishraInfoTech Spectrum Inc2060, Walsh Ave, #120, Santa Clara, CA 95050Email : suraj.mishra@infotechspectrum.com Web: www.infotechspectrum.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "61_Only H1B with Architect experience - DevOps Architect - Remote": "Asha,\nERP MARK\nasha@erpmark.com\nReply to: asha@erpmark.com\nHI, Share only H1B profiles with Arcihtect experience.Linkedin is must Job Title: DevOps ArchitectLocation: Remote Terraform Template creationTerraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication services\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "62_Senior Azure Cloud Engineer Onsite": "Rajmysa,\nVSII\nmysar@vsiiusa.com\nReply to: mysar@vsiiusa.com\nHi All, Senior Azure Cloud EngineerRockville, MD. (Hybrid 1-2 days a week in office at fisher\u2019s lane)Local or nearby state candidates.F2F is must. Interested candidate can apply.Certification is mandatory. Key Responsibilities:\u2022 Azure Administrator - Provision, configure, manage, and monitor Azure virtual machines (Window VMs), storage accounts, IaaS, PaaS, and networking components; Perform regular OS and security patching, backups/restore, and implement disaster recovery strategies for Azure resources with maintenance, trouble shooting.\u2022 Azure Security \u2013 be able to set up and manage Microsoft Defender for Cloud, Azure Key Vault, network security, data encryption and related security compliance and remediation.\u2022 DevOps \u2013 be able to set up and integrate related DevOps process, managing CI/CD pipelines for Azure applications.\u2022 Azure Monitoring \u2013 be able to set up Azure Monitor, Log Analytics, Network Watcher, and Azure Application Insights with relate automated alerts in emails based on pre-configured rules and thresholds and perform related monitoring and trouble shooting.\u2022 Expert in Azure App Service, Azure SQL Database, Azure Application Gateway (WAF, load balancing), etc. including set up, configuration and support.Job Requirements:\u2022 10 years\u2019 experience as Azure Cloud Administrator or Azure Cloud Engineer\u2022 Certification - Azure Administrator Associate or higher\u2022 BS/MS degree in MIS, CS, or related discipline is required.Preferred Skills:\u2022 Some experience with Object-Oriented Programming and working with N-Tier Architecture.\u2022 Some experience with design patterns.\u2022 Great customer service and interfacing with customers required.\u2022 Excellent oral and written communication skills.\u2022 Experience with Agile/SCRUM experience, preferred.\u2022 DevOps skills and experience with Azure Dev Ops Server is a plus.\u2022 Ability to change and have the ability to work in a fast-paced environment. Kindly send me the ID Proof & details to proceed forward. Req details for Submission:Full Name as per your Driver License:Email:Contact #:Current location (City, State):DOB:Availability for in-person/Video interview:Availability to Start from acceptance of offer:Relocate:Rate:Skype:Visa Status (US Citizen/Green Card/H1B/etc.):Clearance level, if any:How many years you are in US:Last 4 digits SSN -Educational Qualification in details with University name and pass out years \u2013 Thanks & RegardsMysaRajVisionsoft International IncPhone: 770-872-0898 x 18\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "63_Local Profiles for Azure Cloud Architect": "Pradeep,\nScalable Systems\npradeep.sharma@scalable-systems.com\nReply to: pradeep.sharma@scalable-systems.com\nHi,Greetings of the day! I have an urgent requirement below, please go through JD and let me know if you are comfortable or have any profile. Kindly revert me back with your updated resume as well. Job Title: Azure Cloud Architect Location: Chicago, IL (Work from Office)Duration: Long Term Contract Overview:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration:- Lead the planning, design, and execution of cloud migration projects to Azure.- Assess existing on-premises infrastructure, applications, and data for cloud readiness.- Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed.- Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform:- Design and implement infrastructure using Terraform for repeatable and consistent deployment.- Develop and maintain Terraform scripts and modules for provisioning Azure resources.- Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management:- Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab.- Automate build, test, and deployment processes to enhance efficiency and reliability.- Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools:- Set up and manage enterprise testing tools.- Integrate testing tools with CI/CD pipelines to enable automated testing.- Develop and enforce testing best practices and standards to ensure high-quality deliverables.- Provide support activities for the queries, issues, access, installation and configuration for supported tools- In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation:- Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives.- Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows.- Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect Thanks & Regards, Pradeep SharmaEmail: pradeep.sharma@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "64_Sr. Cloud Network Engineer :: Bellevue, WA (Onsite - Need DL from WA)": "Nigesh,\nERP MARK\nnigesh@erpmark.com\nReply to: nigesh@erpmark.com\nHiWe have priority Job requirement from our Direct customer and here I am checking your interest/ Availability for the same. Please review.Role: Sr. Cloud Network EngineerClient: UST Global / T- MobileLocation: Bellevue, WA (Onsite - Need only consultant from WA)Mandatory Skills: NIST(Framework exp), Python, Azure..Responsibilities:Oversee the network onboarding process for new users and systems into Cloud environment Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations. Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms. Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways. Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats. Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents. Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure. Conduct regular network assessments and audits to ensure compliance with internal and external requirements. Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards. Provide technical support and troubleshooting for Cloud network-related issues Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications Experience with network onboarding and provisioning in cloud environments. Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud. Familiarity with security regulations, standards, and compliance requirements for cloud network environments. Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers. \u2022 Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud. Strong communication and documentation skills for collaborating with cross-functional teams. Experience with automation tools (e.g., Python, Terraform) for network configuration and management Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "65_Urgent Need - Sr Cloud Network Engineer": "Jon,\nSmarttechlink\njon@smarttechlink.com\nReply to: jon@smarttechlink.com\nHi Team, We are looking for a role with our client, please send me your consultant details along with the visa copy and photo ID to shortlist and schedule the interview.Immediate Opportunity Passport number Mandatory Sr Cloud Network EngineerFrisco,TXKey Responsibilities\u2022 Oversee the network onboarding process for new users and systems into Cloud environment\u2022 Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.\u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.\u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.\u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.\u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.\u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements.\u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.\u2022 Provide technical support and troubleshooting for Cloud network-related issues\u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platformsQualifications\u2022 Experience with network onboarding and provisioning in cloud environments.\u2022 Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).\u2022 Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie\u2022 In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.\u2022 Familiarity with security regulations, standards, and compliance requirements for cloud network environments.\u2022 Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.\u2022 Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.\u2022 Strong communication and documentation skills for collaborating with cross-functional teams.\u2022 Experience with automation tools (e.g., Python, Terraform) for network configuration and management\u2022 Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred. Thanks & Regards JonTechnical Recruiterjon@smarttechlink.com | www.smarttechlink.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "66_Urgent Role : : Azure Architect with Terraform : : Chicago,IL": "Luvpreet,\nQuantum World IT\nluvpreet.singh@quantumworldit.com\nReply to: luvpreet.singh@quantumworldit.com\nHelloPlease let me know your interest for below role Role name:Product ArchitectRole Description:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Competencies:Digital : Microsoft Azure, Digital: TerraformExperience (Years):10 & AboveEssential Skills:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Desirable Skills:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Country:United StatesBranch | City | Location:TCS - Chicago(Downtown), IL Name: Luvpreet SinghDesignation : Senior Technical RecruiterEmail- Luvpreet.singh@quantumworldit.comQuantum World Technologies Inc.4281 Katella Ave, Suite #102 Los Alamitos CA 90720 USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "67_Azure Cloud Architect-Chicago, IL- Local to IL Preferred": "Navneet,\nSiriinfo\nnavneet.jha@siriinfo.com\nReply to: navneet.jha@siriinfo.com\nHello, My Name is Navneet Jha. I am reaching out to you today because we have your resume in our database and wanted to check if you are currently available in the market actively looking for an opportunity. Please see the below mentioned requirement and I believe you would be a good match for the role. I would appreciate if you could respond with your updated resume along with the good time to speak so we can move forward with the process. Role: Azure Cloud ArchitectLocation: Chicago, IL- Local Preferred Duration: Contract to HireMode: VideoJob Details: Role Description: This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Competencies: Digital : Microsoft Azure, Digital: TerraformExperience (Years): 10 & AboveEssential Skills: This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Desirable Skills: This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Best Regards,Navneet JhaD : 848-999-0314 | Email: navneet.jha@siriinfo.comSiri InfoSolutions Inc, 3 Ethel Rd, Suite # 302, Edison NJ 08817. CPUC CertifiedWe respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "68_Need AWS Data Engineer - Scottsdale, AZ (Day 1 onsite)": "Venkat Ram,\nNeo Tech Solutions, Inc.\nvenkat@neotechusa.com\nReply to: venkat@neotechusa.com\nHi ,Hope you are doing wellPlease find the below Job Description and let me know if you are interested in applying for this position. Position: AWS Data EngineerDuration: 12 MonthsLocation: Scottsdale, AZ (Day 1 onsite) Job Description:Must have : IAM , AWS , Glue , S3 Redshift , Kinesis , Python/Java , Scala RDS SQL ServerAWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Needs someone who can understand and work on AWS echo system - Glue , Kinensis, DynamoDB etc. ) and the access permissions and especially terraform infra as code.It is combination of skills AWS DE + some hands-on knowledge in AWS infra. Position Summary:We are seeking a highly skilled AWS Data Engineer with extensive experience in AWS technologies to join our team as a contractor. The ideal candidate will have a strong background in designing, building, and maintaining data pipelines, and must be capable of contributing immediately to our ongoing projects.Key Responsibilities:Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage.Implement data ingestion processes to handle streaming and batch data.Ensure data quality and integrity through robust ETL processes.Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS.Optimize and tune data processing workflows for performance and cost efficiency.Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability.Document data architecture, processes, and workflows.Qualifications:Bachelor's or master\u2019s degree in computer science, Engineering, or a related field.Minimum of 5 years of experience in data engineering, with a focus on AWS technologies.Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server.Strong proficiency in SQL and experience with database design and optimization.Expertise in ETL/ELT processes and tools.Familiarity with data warehousing concepts and best practices.Experience with data modeling and schema design.Proficiency in programming languages such as Python, Java, or Scala.Knowledge of data governance and security best practices in a cloud environment.Excellent problem-solving skills and the ability to work independently with minimal supervision.Strong communication and collaboration skills.Qualifications:AWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL ServerFamiliarity with big data technologies like Apache Spark or Hadoop.Experience with reporting and visualization tools like Tableau Knowledge of DevOps practices and tools for CI/CD such as Jira and Harness. Required details for submissionFull Name: Email id:Contact Number:Current Location:Relocation:Interview Availability:Notice Period:Visa Status:Any Interviews in Pipeline:Any offers in Pipeline:Linked in: Pay rate: Thanks & RegardsVenkat RamTalent Acquisition Manager+1 (908) 864-9002venkat@neotechusa.comNeo Tech Solutions, Inc.Suite 204, 515 Plainfield Ave, Edison, NJ 08817\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "69_Immediate hiring || Cloud Developer || Atlanta, GA": "Joseph,\nDiamond Pick\njoseph.antonyraj@diamondpick.com\nReply to: joseph.antonyraj@diamondpick.com\nRole: Cloud DeveloperLocation: Atlanta, GA Job description:Mandatory SkillsAWS Lambda, DynamoDB, CloudWatch, S3 Bucket, APIGateway, CloudFront (required)Job Description 10 years of overall Experience in software developmentExperience using various design patterns preferably MVC Singleton Facade and Factory Proficient in OOD and Implementation Design Patterns2 3 years of experience with Cloud Technologies such as OpenShift Google Cloud Azure AWS requiredExperience in other Backend programming languages like Python Node js Golang is a plus2 years experience with Angular JSExpertise in other Frontend Frameworks like React Vue js is a plusMust have experience working in GIT Hibernate JBOSS Apache Tomcat Oracle UML JUnit Mockito Wire Mock and REST APIsSolid understanding of web mobile application architecture and security OAuth JWT Spring SecurityExtensive experience implementing APIs REST via microservicesHands on UI development experience utilizing Responsive DesignProficient in Bootstrap or Foundation frameworks2 years\u2019 experience with unit testing frameworks e g Jasmine ProtractorProficient in OOD and Implementation Design Patterns MVVM etcMust have diverse experience utilizing Java tools in business Web and client server environments including Java Platform J2EE JDBC technologies and Apigee gateway platformExperience or exposure to Database Design and ImplementationKnowledge of Multitier Architecture Rational Visio modeling. RESPONSIBILITIES Leading the technical scrum team and implementing hands on the front ends associated with the technical designs for product project teamsLeading technical efforts in the scrum team including in the creation of quality in our deliveryEnsuring the team develops with quality measuring via clean pipelines and 90 coverageMentoring and elevating your teammates to be stronger engineers by staying ahead with technology evolution and educating the teamTransforming business requirements into application architecturesDetermining feasibility scalability of front-end solutions interacting with business and product owners in order to define technical solutions for customer problemsProduction issue triage management and prevention as neededUI Design reviews for feasibility tech design and impact analysisDevelop sustainable accessibility compliant solutionsLong term technical debt resolutions debt prevention code reviewsAnalysis and implementation of Performance Stability Reliability Architecture initiativesResearch Development of POCs innovative new ideas for customer interactions with DeltaAssisting in defining alternate solutions for the business problems and providing estimates for potential new workstreamsCreating a learning culture by establishing sharing and enforcing best practices and lessons learned Thanks & regards,Joseph Antony Raj AEmail: joseph.antonyraj@diamondpick.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "70_CLOUD RELIABILITY ENGINEER || 12 MONTHS  || VIDEO || REMOTE": "Jyoti Mittal,\nKpg99 Inc\njyotim@kpgtech.com\nReply to: jyotim@kpgtech.com\nHi,My name is Jyoti and I represent KPG99 INC. We work with clients across the USA. I have a very urgent role. If interested then please share with me your updated resume along with below information. POSITIONCloud Reliability EngineerLOCATIONRemoteDURATION12 Months CTHINTERVIEW TYPEVideoVISA RESTRICTIONSUSC with LI REQUIRED SKILLSProficient in development technologies, architectures, and platforms (Web, API)Experience in cloud platforms (e.g., AWS, Azure, Google Cloud) Infrastructure as code (IaC) tools for managing app infrastructure and deploymentsKnowledge of monitoring tools (e.g., Dynatrace, Logrocket, DataDog)Logging frameworks (e.g., ELK Stack) for real-time visibility into system health, performance metrics, and user experience Thanks & Regards,Jyoti Mittal| Technical Recruiter | KPG99,INC jyotim@kpgtech.com | http://www.kpg99.com/3240 E State St EXT, Hamilton, NJ 08619An E- Verified Company Certified Minority Owned BusinessNote: KPG99 does not endorse undesired email. If you do not want to receive our mails, please reply with Remove in the subject; I would ensure that you are not troubled further. I apologize for any inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "71_Kubernetes Infrastructure Engineer - Remote": "Prakash,\nBOV\nprakash.gs@blue-oceanventures.com\nReply to: prakash.gs@blue-oceanventures.com\nHi ,Hope all is well,My name is Prakash, and I am a representative at Blue Ocean Ventures, a Search firm based out of Atlanta. I work closely with some of the leading Global IT Management Consulting companies for their hiring needs in the United States. I came across your profile during an online search and felt that you could potentially be a strong match for some of their current needs.Job Title: Kubernetes Infrastructure Engineer Location: Remote / Home BasedDuration: Long Term Description: Note \u2013 \u201cWe need the Kubernetes infrastructure engineer who can build the platform and manage it\" You will be responsible for implementing/integrating our Cloud and Commercial kubernetes distributions into Verizon's ecosystem.Translate application requirements into system requirements that can then be used to design and implement the operating environment for applications to run in a Kubernetes CaaS platform.Evaluate legacy application design and recommend fundamental application redesign changes required to function in a Kubernetes CaaS platform.Lead problem resolution for business impacting failures and drive the resolution to meet platform service level objectives. This includes assembling resources from existing team members and engaging other engineering resources from peer teams in order to restore service disruptions.Understand architecting and engineering large enterprise solutions to achieve cost efficiencies, scaling, and availability for a large enterprise environment.Share support and oncall responsibilities with core team membersCritical SkillsFive or more years of experience with deployment and orchestration technologies (such as Docker, Kubernetes, EKS, Openshift,)Three years or more of continuous integration and continuous delivery experienceWorking knowledge of cloud computing including virtualization, hosted services, multi-tenantcloud infrastructuresExperience architecting and developing software or infrastructure for scalable, distributed systemsExperience creating and maintaining Gitlab CI pipelinesExperience with the Linux kernelKnowledge of APIs design and implementation.Knowledge of Monitoring tools: New Relic and or Prometheus/Grafana.Required certificationsAWS Professional CertificationCKA - Certified Kubernetes Administrator--Warm Regards,Prakash G SBlue Ocean Ventures LLCTalent Acquisition AssociateEmail: Prakash.gs@blue-oceanventures.comPrakash G S | LinkedInWill To Serve Will to Win Will To Lead\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "72_JOB | Azure Architect (E4) | Dallas, Texas - Hybrid Onsite": "Manoj Rathee,\nSunray Enterprise, Inc.\nmanoj@sunraycorp.com\nReply to: manoj@sunraycorp.com\nHi Dear,I hope my mail finds you in good health and doing well!We currently have the JOB POSITION listed below available.Kindly go through the job description and share your latest updated RESUME, visa copy, and photo ID so that I can submit the profile to the client.Job Position :- Azure Architect (E4)Locations :- Dallas, Texas - Hybrid OnsiteDuration :- LONG TERMJob Description:-Technical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutions Keeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction. Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred. Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Hope to hear from you soon !!!\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "73_Tomorrow Interview (Strong Azure exp) || C# Microservices Developer || Issaquah, WA (Onsite from Day1)": "Rahul Kumar,\nSPAR Information Systems\nrahul.k@sparinfosys.com\nReply to: rahul.k@sparinfosys.com\nHello Folks, (Plz must be expert in C#, Azure, Microservices & SQL) - Prefer Payment exp note it & Prefer PST/MST/CST timezone profilesHope you all are doing good.Please go through the Job description and let me know your interest. Title: C# Microservices Developer - Prefer Payment expWork Location: Issaquah, WA (Onsite from Day1) Duration: Long Term ContractJob Description:Requirements:\u2022 Minimum 7 years\u2019 experience in performing API microservices development using C#.\u2022 Experience with developing and running applications in a public cloud environment (Azure preferred). \u2022 Hands-on working knowledge of a major relational database (DB2, SQL Server etc.) and/or NoSql.\u2022 Experience working in CI/CD, and designing and delivering DevSecOps automation for app deployment and testing.\u2022 Strong knowledge and experience working with Web Technologies using JavaScript, React.\u2022 Experience describing solutions and generating the architecture (Logical, Physical) artifacts.\u2022 Must be extremely responsive, able to work under pressure in crisis with a strong sense of urgency.\u2022 Responsible, conscientious, organized, self-motivated and able to work with limited supervision.\u2022 Application development in C#.\u2022 Experience programming in major databases such as Microsoft SQL Server.\u2022 Must possess a strong understanding of the software development process.\u2022 Strong knowledge of object-oriented concepts.\u2022 Strong knowledge and experience working with databases.\u2022 Strong verbal and written communication skills and be able to communicate to both technical and Business audiences.\u2022 Demonstrated ability to execute against iteration plans and deliver assignments within scope, schedule, and budget.\u2022 Ability to work with all management levels.\u2022 Strong organizational and time management skills with ability to multitask and prioritize work effectively in a fast-paced working environment.\u2022 Must be detail-oriented and possess strong problem-solving skills and ability to analyze potential future issues.Responsibilities:\u2022 Supports Systems Architects with the design of the overall architecture of a specific product/application, applying principles that promote availability, reusability, interoperability and security into the design framework.\u2022 Performs development, optimization, and automation activities to support the implementation of a product/application.\u2022 Increases proficiency and understanding of relational and non-relational databases, distributed application architectures, user interfaces and user experience, quality assurance, security concerns, and business value creation.\u2022 Adopts engineering best practices to deliver higher quality and scalable solutions.\u2022 Provides the team with the development strategy, solution recommendations, and estimates for a given product/application.\u2022 Works with team members to move user stories from the development backlog into testing and provides clarification when needed.\u2022 Estimates, plans, and manages all implementation tasks and reports on development progress.\u2022 Demonstrates a strong understanding of emerging technologies to support the development of new solutions.\u2022 Conducts peer code reviews for the software changes made by other engineers within a team.\u2022 Designs, builds, debugs, optimizes, and implements solutions in C#, NoSql, and RDBMS. Developments high quality, efficient, and fault tolerant solutions.\u2022 Delivers automation scripts and unit test automation under DevSecOps.\u2022 Works with and supports Systems Architects with the design of the overall architecture of a specific product/application, applying principles that promote availability, reusability, interoperability, and security into the design framework.\u2022 Performs development, debugging, optimization, and automation activities to support the implementation of a product/application.\u2022 Uses APM/monitoring tools such as Dynatrace/Splunk and browser tools to perform request purepath/waterfall analysis to identify bottlenecks and suggest improvements.\u2022 Understands the full technology stack and underlying applications, services, and databases in order to ensure optimal performance.Thanks & Regards,Rahul KumarSr. Technical RecruiterSPAR Information Systems(a E-verify Company)Email : rahul.k@sparinfosys.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "74_Devops Engineer-US Citizen-Tampa FL": "Shambhu,\nSUS Infotech\nshambhu@susinfotek.com\nReply to: shambhu@susinfotek.com\nTitle: Devops Engineer-US CitizenLocation: Tampa FL (Hybrid-3 days Onsite a week)Client : Trademark Metals Recycling (We are through Prime Vendor)Rate : $60/hr on c2cDuration: 6 months/contractProcess:Technical assessment verbal first-(top engineers in cloud & devops)Interview (soft skills & more technical)NOTES:Understands it can be tough to find someone solid in DevOps & AWS Focus more on Cloud Ops side if need beThis could grow into a lead positionResource will be shared amongst a few different projectsDEVOPS:Creating CI/CD pipelines (IaC ), TerraformGitLab pipeline, Sonar cube strengthHelmTerraform, (pref) or cloud formAWS NEEDS:Must haves- AIM, KubernetesEC2 instances , Docker containersSecurity Run53, vcc?\u201ccore AWS critical\u201dNot specific to a service (can vary) Someone who can learn & adapt Interview Questions: How do you impact? How did you improve your environment? Need someone that can do start to finish, trouble shot on their own. Ex: application that is heavy on queuing, AWS from scaling perspective. Doesn\u2019t have scaling options that are based on number of messages in ques. Other services that can- asked old person to investigate, get option, and find something to allow them to scale of que depth. Did a DOC and then supported implementation. YOUR CREDENTIALS: \u2022 Bachelor's Degree in Computer Science or similar engineering discipline from an accredited university\u2022 At least 5 years of professional experience\u2022 Knowledge of deployment and configuration technologies (e.g., Gitlab, Jenkins, Ansible, etc.)\u2022 Knowledge of Containerization and orchestration technologies (e.g., Docker, K8s, etc.)\u2022 Working Experience with Cloud technologies (e.g. AWS, Azure)\u2022 Knowledge of IaC tools (e.g., Terraform, CloudFormation, Azure ARM)\u2022 Knowledge of tools like Helm, Artifactory, SonarQube, etc.\u2022 Knowledge of source control technologies (e.g., Git, subversion)\u2022 Experience operating in virtual environments required\u2022 Experience with both Windows and Unix based operating systems required\u2022 Knowledge of one or more scripting languages (e.g., Python, Ruby, Bash)\u2022 Critical thinker and problem-solving skills\u2022 Team player\u2022 Excellent time-management skills\u2022 Interpersonal and communication skills\u2022 Ability to effectively analyze data and present solid recommendations TOP SKILLS REQUIRED 1 Tech strong in devops & azure/aws world (cloud most important) & deployments in cloud ( more aws focused)2 Aptitude to shift with tech3 Self-invested in career4 Terraform, cloudFormation YOUR CONTRIBUTION: \u2022 Drive automation standards across our testing and production environments and be a key resource as we migrate our monolith and microservice application to the cloud\u2022 Create high-performance, scalable, and fault-tolerant computing systems while researching and providing solutions to challenging technical issues related to DevOps \u2022 Automate the infrastructure creation on cloud like AWS, Azure, etc.\u2022 Design and implement security governance practices for the applications and infrastructure\u2022 Automate pipelines and environments, build resilient systems and create repeatable processes, as well as update monitoring systems to detect service and security issues\u2022 Improve automation degree in integration, deployment, monitoring, and configuration management aspects of developed solutions and development infrastructure \u2022 Design and deploy, in an Agile (Scrum) environment, assets and code that will help, deploy, test, monitor, as well as maintain components and services for next-generation service platforms\u2022 Participate in the design and development process from definition to deployment while leading day-to-day operational support activities for services developed using the DevOps practice \u2022 Verify and monitor on-premises and cloud environments and respond to issues as they arise, adding new monitoring as needed\u2022 Recommend alternate choices and trade-offs for various design decisions while assisting with troubleshooting issues \u2022 Design and implement Fault tolerance architecture for all the applications YOUR EXPERTISE:\u2022 Customer-Centric \u2013 Focus efforts to ensure internal and external customers thrive \u2022 Communication and Persuasion \u2013 Possess solid communication and persuasion skills essential to success in this position. You will represent the Digital IT group effectively and employ negotiation skills that build organizational consensus around a specific course of action while identifying communication barriers and taking action to facilitate mutual understanding. \u2022 Business Acumen \u2013 Acknowledge the impact of your actions in local and global contexts while working to generate sustainable value (economic, social, and environmental) for the organization. In addition, you should understand basic business processes and possess the ability to identify and develop workflows and process maps for continual improvement and waste elimination within the business.\u2022 Technical Expertise \u2013 Understand all IT functions, cloud technologies, and services in use, and become familiar with the current direction in the infrastructure market \u2022 Strategic Thinking and Vision \u2013 Possess a clear sense of the direction of the organization and understand what will create a competitive advantage for the business in the future. Distinguish tactical issues from strategic ones and take action while effectively translating strategic imperatives at the macro level into initiatives and priorities. \u2022 Simplistic Agility \u2013 Foster simplicity and agility to look for ways to streamline work and processes to become more effective and efficient while developing new ideas quickly\u2022 Global Perspective \u2013 Ability to successfully operate in a global IT organization bringing perspective to daily work while leveraging relationships, expertise, and opportunities across borders taking the company\u2019s strategy and priorities into consideration. \u2022 Accountability \u2013 Take initiative and assume personal accountability and ownership for goals, outcomes, and deadlines holding others accountable for achieving individual and organizational objectives. \u2022 Analysis \u2013 Possess critical thinking ability and deal systematically with input from a variety of sources to suggest alternative approaches to unfamiliar situations or concepts, as well as strong analytical skills, including an understanding of business economics and financial resources.\u2022 Supplier Management \u2013 Possess the ability to engage and manage suppliers as needed for discovery, modeling, implementation, and support. Thanks & RegardsShambhu Team Lead Recruiter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "75_Contract role::: Azure Architect with IaaS, network and security exp (L4)::: Dallas,Texas (Hybrid role)": "Jasmine Kaur,\nResource Logistics\njasmine.kaur@resource-logistics.com\nReply to: jasmine.kaur@resource-logistics.com\nHi , Greetings of the day!!! We have a wonderful job opportunity of Azure Architect with aaS, network and security exp (L4), located in Dallas, Texas (Hybrid role), matching your skill sets. Please have a look on below mentioned detailed job description and let me know if interested: Job Title: Azure Architect with IaaS, network and security exp(L4)Location: Dallas, Texas (Hybrid role)Mode: Contract (6+ Months)Rate $70/hr. on C2CTechnical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of client\u2019s's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects.Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company.Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutionsCollaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutionsKeeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction.Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.RequirementsSubject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, MonitoringExtensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred.Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "76_Azure Architect Dallas, TX Hybrid": "Aashish N,\nYochana IT Solutions\naashish@yochana.com\nReply to: aashish@yochana.com\nHi, This is Aashish from Yochana IT Solutions. We are looking Azure Architect (E4) Dallas, TX Hybrid For the below mentioned job description. Kindly forward your resume, rate and contact details for further process Title : Azure Architect (E4)Location : Dallas, Texas - Hybrid # of positions : 2 Technical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & ResponsibilitiesUnderstand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutions Keeping up to date with market trends and competitive insights and maintain technical skills and knowledgeAdvocate new features and solutions to bring operational efficiency and cost reduction. Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. Rich experience in Migration projects and hand-on experience with ASR/Azure MigrateGood Knowledge of Azure DevOps, ARM Templates & PowerShell scriptingExperience in PaaS services and Containers is preferred. Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.Demonstrated ability to adapt to new technologies and learn quickly.Proven track record of driving decisions collaboratively.Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Thanks & Regards....Aashish NPResource specialistYochana IT Solutions23000 Commerce DriveFarmington Hills MI 48335Email : aashish@yochana.comLinkedIn :- https://www.linkedin.com/in/aashish-aash-96345b238/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "77_AWS Infrastructure Lead for Atlanta, GA (Onsite)": "Kiran Nagani,\nGAC Solutions\nkiran@gacsol.com\nReply to: kiran@gacsol.com\nRole: AWS Infrastructure LeadLocation: Atlanta, GA (Onsite) Job Description:Manage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "78_Sr Cloud Network Engineer : (Bellevue, WA : (Onsite)": "Amol,\nChabeztech\namol@chabeztech.com\nReply to: amol@chabeztech.com\nTitle: Sr Cloud Network Engineer Job Location:- Bellevue, WAMandatory exp- Telecom Sector Below is the Job Description Reqs Key Responsibilities\u2022 Oversee the network onboarding process for new users and systems in the Cloud environment\u2022 Provision and configure network resources in the Cloud, ensuring compliance with security policies and government regulations.\u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.\u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.\u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.\u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.\u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements.\u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.\u2022 Provide technical support and troubleshooting for Cloud network-related issues\u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "79_OracleFusion Cloud HCM Developer": "Parimala,\nTetrahed\nparimala@tetrahed.com\nReply to: parimala@tetrahed.com\nHiHope you are doing good Role: Oracle Fusion Cloud HCM DeveloperLocation: Vienna, VA(Hybrid) Need only USC, GC and EADsOracle Fusion Cloud HCM Cloud Developer is a technical role that provides project coordination technical expertise and contributes to the support, maintenance, and enhancement of Oracle HCM Cloud modules (Recruiting, Benefits, Time & Labor, Payroll, Profile Management, and Performance Management.) Experience in product personalization, data integration, data conversion, and reports development.This resource will serve as a technical expert to the business to manage the development of new system functionality system upgrades and must have experience working with at least two modules in Oracle HCM Cloud, with demonstrated experience in identifying, creating, documenting, and implementing solutions that enhance business and system operations.This resource should have experience in Oracle HCM Cloud advanced tools such as OIC, HCM Extracts, Fast Formula, HDL, BI Publisher reports, OTBI reports, Redwood Page Composer, Page Configurator, REST APIs, SOAP, Webservices, Fast Formula, Oracle HCM Application Security.Experience/Qualifications\u2022 Bachelor's degree in information technology, Computer Science, or another related field\u2022 Minimal 5 years experience with Oracle Cloud HCM\u2022 Experience supporting Oracle HCM Cloud modules (HR-core, Recruiting, Benefits, Time & Labor, Payroll, Profile Management, and Performance Management).\u2022 Proven experience gathering and analyzing data and creating functional documentation. Including business process flows, requirements documents, test plans, and test scripts.\u2022 Proven experience testing and documenting results.\u2022 Expertise with Oracle Applications, Cloud Infrastructure/Solutions, and experience in SQL (Oracle or MS SQL).\u2022 Expertise in the knowledge of BI Publisher (BIP) and OTBI reports development, Interface development and troubleshooting, Application security roles, and Workflow.\u2022 Hands-on experience configuring and implementing an enterprise integration architecture solution using OIC adapters (SOAP, REST, FTP, and other pre-built adapters) based on NFCU's data mapping requirement.\u2022 Understanding leveraging Oracle SHDL and HDL capabilities to import data into Oracle Fusion Cloud via OIC.\u2022 Willing to learn a range of business or technical specialties based on business needs.\u2022 Experience with Agile software development practices, including SAFe, Scrum, Azure DevOps, Peer Review, GitHub, and CI/CD.\u2022 Excellent interpersonal, communication, and presentation skills.\u2022 Demonstrated ability to convey complex information in a clear and concise manner.\u2022 Proficiency in Microsoft applications, including Word, Excel, and Visio.Desired Qualifications and Education Requirements:\u2022 Bachelor's degree in information technology, Computer Science, or another related field.\u2022 Knowledge of banking/financial industry trends, products, and services (desired).\u2022 Knowledge of Navy Federal products, services, programs, policies, and procedures (desired).\u2022 Technical certifications for Oracle Cloud HCM and Oracle Cloud Infrastructure (desired). Thanks & Regards, Parimala Technical Recruiterparimala@tetrahed.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "80_AWS Data Application Infrastructure Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: AWS Data Application Infrastructure Lead - OnsiteWork location:ATLANTA (US:30301), GA CLient : Capgemini DESCRIPTION.Manage AWS Infrastructure for provisioning\ufffdManage and Streamline Deployments of Pipelines to Production\ufffdManage FinOps for Productionalized applications\ufffd.POC and recommendations for product usage and processes\ufffdMust Have Experience in Big 4 Consulting Companies\ufffd Strong Communication Skills and Client Relationship management\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "81_(One Round Interview)-ETL Test Lead with Azure  Remote": "Selvakumar,\nVbeyond Corporation\nselvak@vbeyond.com\nReply to: selvak@vbeyond.com\nGreetings! I hope you are doing well. I am Selva from V-Beyond Corp. We are a global recruitment company with a specialization in the hiring of IT professionals. One of our clients is looking for a ETL Test Lead with Azure \u2013 Remote Role : ETL Test Lead with Azure(ADF, Databrick, Azure Devops)Mode : ContractLocation : Remote As an ETL Test Lead you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards .Responsibilities: Experience of ETL testing, specifically utilizing ADF ,Databricks and Azure, to test data integration workflows ensuring seamless data flow across diverse systems. Knowledge on Azure based application Ability to review test scenarios and test cases Develop and execute comprehensive test plans, test cases, and test scripts for interoperability projects. Ability to test UI based application also added advantage.Collaborate with development, DevOps, and QA teams to ensure seamless integration and deployment processes.Conduct functional, regression, integration, and performance testing on various systems and applications. Identify, document, and track defects, and work with developers to ensure timely resolution.Ensure testing processes align with industry standards and best practices.Participate in Agile ceremonies, including sprint planning, daily stand-ups, and retrospectives.Utilize automated testing tools and frameworks to enhance test coverage and efficiency.Continuously improve test processes and methodologies to support DevOps practices.Stay updated on the latest industry trends and technologies to ensure the adoption of best practice Ensure data exchange processes comply with industry standards and regulations Thanks & RegardsSelva KumarVBeyond Corporation || PARTNERING FOR GROWTHHillsborough, New Jersey, USAEmail ID: SelvaK@vbeyond.comMobile : 862-270-5844Website : www.vbeyond.com Note: Please allow me to reiterate that I chose to contact you either because your resume had been posted to one of the internet job sites to which we subscribe, or you had previously submitted your resume with Vbeyond. I assumed that you are either looking for a new employment opportunity, or you are interested in investigating the current job market.If you are not currently seeking employment, or if you would prefer I contact you at some later date, please indicate your date of availability so that I may honor your request. In any event, I respectfully recommend you continue to avail yourself to the employment options and job market information we provide with our e-mail notices.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "82_DevOps Engineer:: Onsite:: Los Angeles, CA  OR Dallas, TX": "Subhra Shahi,\nUrBench\nshubhra@urbench.com\nReply to: shubhra@urbench.com\nJob Title: DevOps Engineer - Data Engineering Contract: Long Term Client: IBM Location: Los Angeles, CA / Dallas, TX(onsite) VISA: USC,GC,H4EAD Job Summary: We are seeking a skilled DevOps Engineer to join our technology team, focusing on automating, and optimizing our data engineering pipelines. The ideal candidate will have a strong background in software development, automation of CICD using Jenkins in data platforms like Databricks and Snowflake. You will play a crucial role in developing and enhancing CI/CD pipelines, automating tasks, and ensuring seamless data operations across our platforms. Key Responsibilities: Develop and enhance CI/CD pipelines using Jenkins to automate and streamline deployment processes.Write and maintain scripts in Groovy to support automation and integration tasks.Utilize REST APIs to integrate different systems and services, and process JSON responses effectively using Python.Work closely with data engineering teams to understand their Devops needs and implement solutions that enhance data workflow efficiencies.Deploy dathub jobs using Jenkins pipeline.Collaborate with development and operations teams to ensure system reliability, security, and efficiency.Stay updated with the latest industry trends and technologies and apply insights to improve existing systems and workflows. Qualifications: Proven experience as a DevOps Engineer or similar role with a focus on data engineering platforms.Strong programming skills in Groovy, Python, and experience in Jenkins and CI/CD practices.Experience with REST APIs and data manipulation, particularly JSON.Familiar with Databricks and Snowflake.Strong problem-solving skills, attention to detail, and ability to work in a collaborative team environment.Excellent verbal and written communication skills.This role is perfect for someone passionate about automation, data flow, and continuous improvement in data operations environments. Kind Regards, Shubhra Shahi | Technical Recruiter Direct: +15122542396Email: Shubhra@urbench.com https://www.linkedin.com/in/shubhra-shahi-kaushik\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "83_Lead Azure Cloud Engineer  Tampa, FL-Hybrid-US Citizen": "Shambhu,\nSUS Infotech\nshambhu@susinfotek.com\nReply to: shambhu@susinfotek.com\nTitle : Lead Azure Cloud EngineerLocation: Tampa, FL (Hybrid)Duration: 6+ months/contractVISA: US CitizenSkills:5+ years or related experiencePythonAWSAzure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plus Thanks & RegardsShambhu Team Lead Recruiter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "84_Urgent Requirement ---  Devops Engineer---- San Ramon CA": "Lokesh Varma,\nYochana\nlokesh@yochana.com\nReply to: lokesh@yochana.com\nHI, This is Lokesh Varma from Yochana IT. I hope you are doing great. We have an urgent requirement with one of our client for Devops Engineer Kindly go through the requirement below and let know you interest. Position: Devops EngineerLocation: Palo Alto or San Ramon CA Primary Responsibilities: \u2022 Monitor production systems in multiple data centers. \u2022 Manage critical incident, including escalation, debugging, fix and root cause analysis. \u2022 Help R&D debug production issues. \u2022 End-to-end K8s application modernization and delivery via on K8S G6R \u2022 Decentralized \"enforced state\" GitOps approach. \u2022 Automated certificate management, DNS, and MTLS \u2022 Unified, auto scaling, easier to adopt and faster e2e develop to deploy framework and strategy.\u2022 Reduced go to market timelines (build to prod )\u2022 Managed DR with reduced costs \u2022 Cloud agnostic and support across 4+1 & Sovereign Cloud deployments \u2022 SGS, ISBN SecOps, and SAP HASI approved processes \u2022 Managed multi-stage backups (storage, Velero, DB-native) \u2022 Process automation and implementation. \u2022 Manage public cloud environments including Terraform, GCP, and/or AWS. \u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault. \u2022 Kubernetes containerization. \u2022 Operate system internals, file system structures and machine architectures in a Linux operating environment; and \u2022 Apache, DNS, Send mail, SSH, TCP/IP, and NFS. \u2022 Support services before they go live through activities such as system design consulting, developing software platforms and frameworks, capacity planning and launch reviews. \u2022 Scale systems sustainably through mechanisms like automation and evolve systems by pushing for changes that improve reliability and velocity. \u2022 Work with product team to define SLA, SLO, monitoring/telemetry pattern and stability across from customer standpoint. \u2022 Help product team gain momentum and velocity for hassle free development across BTP, Hyperscaler, K8S. \u2022 Build, Manage, Scale and Right size/TCO initiatives across SAP Hyperscaler(4+1) strategy towards cloud spend management. \u2022 Engage in and improve the whole lifecycle of services\u2014from inception and design, through to deployment, operation, and refinement. Qualifications \u2022 Development experience in Bash/Python/Go/Groovy, Perl and Shell scripting. \u2022 Process automation and implementation. \u2022 Experience with public cloud environments and toolsets including Terraform, GCP, and/or AWS. \u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault. \u2022 Kubernetes containerization experience. \u2022 Experience with operating system internals, file system structures and machine architectures in a Linux operating environment; and Apache, DNS, Send mail, SSH, TCP/IP, and NFS. \u2022 5-7 years of experience working in a public cloud, automation, and Unix internals \u2022 > 4 year devops/SRE experience. \u2022 Familiar with SRE operation and on-call procedure. \u2022 Familiar with log analysis, such as Splunk or ELK. \u2022 Scrum master certification preferred. \u2022 BA/BS degree in MIS/CS or equivalent experience \u2022 Experienced working in a dynamic, fast-paced environment with well-developed practices and procedures.\u2022End-to-end K8s application modernization\u2022Manage public cloud environments including Terraform, GCP, and/or AWS.\u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault\u2022 SGS, ISBN SecOps, and SAP HASI approved processes\u2022 Managed multi-stage backups (storage, Velero, DB-native)Artificial intelligenceScrum Master Certified Thanks & Regards,Lokesh VarmaTalent Acquisition SpecialistYochana It SoluctionsM:, 248-986-1359 O: 248-986-1359: EXT*217lokesh@Yochana.com || www.yochana.com Let\u2019s Challenge The Status Quo\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "85_Immediate Req_AWS  Delphix Engineer": "Ben Clark,\nTech Talent Connect LLC\nben@techtalentconnect.com\nReply to: ben@techtalentconnect.com\nJob Title: AWS \u2013 Delphix EngineerLocation: San Antonio, TX (Day1 onsite, Hybrid)Duration: Long Term ContractRate: DOEResponsibilities:\u00b7 Creating, configuring, and maintaining Infrastructure on AWS Cloud services including Virtual Private Cloud VPC, EC2, RDS, S3, Route53, SNS, CloudFront, CloudWatch and IA.\u00b7 Migrating data from on-prem to Amazon Web Services cloud.\u00b7 Creating S3 buckets and folder management within it.\u00b7 Creating role-based policies to access AWS resources like S3 and other AWS services.\u00b7 Creating or Importing Volumes into AWS instances.\u00b7 Create and configure the Elastic Load balancers with Autoscaling groups.\u00b7 Create Alarms, events, logs on CloudWatch for monitoring and Cloud trail for logging events onto S3 to troubleshoot and record event history.\u00b7 Hands-on experience with databases MySQL, Oracle creating users, performing dump/restore, and taking automated snapshots.\u00b7 Exposure to Kubernetes clusters and managing the clusters using KOPS, Rancher.\u00b7 Experience in administration, engineering, and support of the Delphix platform \u2013 Mandatory.\u00b7 Onboard and manage VDB and dSources on Cloud and on prime premises \u2013 Mandatory.\u00b7 Strong scripting skills (e.g., Ansible, Shell scripting, Python, PowerShell) to develop and maintain automation of operational tasks.\u00b7 Good to have migration experience from on-prem to cloud.\u00b7 Formulate and implement Test Data Management plans.\u00b7 Proficiency in Delphix virtualization and Continuous Compliance.\u00b7 Perform in-place data masking with integrity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "86_Job opening for  Sr Cloud Network Engineer || Hybrid || Frisco, TX || Locals preferred": "bhuvanesh,\nSmart IT Frame\nbhuvanesh@smartitframe.com\nReply to: bhuvanesh@smartitframe.com\nRole: Sr Cloud Network EngineerLocation: Frisco, TXOnsite; Hybrid; 3 days a weekHire Mode: ContractKey Responsibilities \u2022 Oversee the network onboarding process for new users and systems into Cloud environment.\u2022 Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.\u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.\u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.\u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.\u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.\u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements.\u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards. \u2022 Provide technical support and troubleshooting for Cloud network-related issues \u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications\u2022 Experience with network onboarding and provisioning in cloud environments.\u2022 Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). \u2022 Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie \u2022 In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.\u2022 Familiarity with security regulations, standards, and compliance requirements for cloud network environments. \u2022 Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.\u2022 Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.\u2022 Strong communication and documentation skills for collaborating with cross-functional teams.\u2022 Experience with automation tools (e.g., Python, Terraform) for network configuration and management \u2022 Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "87_Senior AWS Connect Engineer or Developer GC USC only": "Sonali,\nkpg99 inc\nspandit@kpgtech.com\nReply to: spandit@kpgtech.com\nJob Title: Fully Remote Senior AWS Connect Engineer/DeveloperVisa: GC, USC onlyDuration: Long Term ContractLocation: RemoteMust be able to obtain and maintain a Public Trust Key Required Skills The successful candidate will have fluency in AWS Connect design and development principles and work with stakeholders to develop enterprise business solutions that leverages industry processes and best practices Position Description Design, develop and implement AWS Connect processes to include new or modifications to applications, forms, workflow, policies, actions, access control, interfaces, and any other configurations required to support client processes. Provide recommendations for enhancing existing operations and provide recommendations for new updates or modifications. Support and enhance existing processes based on new requirements or upgrades. Facilitate requirements gathering, review and document requirements. Design and develop solutions within the ServiceNow environment. Detailed Skills Requirements FOUNDATION FOR SUCCESS (Basic Qualifications) Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field 7 years of experience working with large enterprise framework 7 years of hands-on experience working with AWS Cloud platform 3 years of hands-on experience working with Amazon Connect 3 years of experience using Node.js, Python or Java for development 3 years of experience in designing call routing & messaging Excellent interpersonal and communication skills Must be able to obtain and maintain a Public Trust. Contract requirement. FACTORS TO HELP YOU SHINE (Required Skills)These skills will help you succeed in this position: Knowledge or experience with CCaaS with Artificial Intelligence (AI) strategies and solutions. Knowledge and experience with integrating Pega workflows with CCaaS. Knowledge and experience with building reports and dashboards preferably with Tableau. Solid AWS Cloud and AWS Connect Developer experience. Extensive experience and knowledge with IVR, Automatic Call Distribution (ACD), Skill-Based Routing, Live Chat. Experience with process flow documentation creation. Strong verbal and written communication skills. HOW TO STAND OUT FROM THE CROWD (Desired Skills)Showcase your knowledge of modern development through the following experience or skills: Adept at working independently, but also in a team environment. Demonstrated experience delivering technology solutions in a fast-paced, deadline driven enterprise environment. Excellent understanding of change management, testing requirements, techniques, and tools to ensure high availability of systems. Prior Federal government experience. Self-starter, highly motivated individual who adapts to a dynamic work environment. Strong attention to detail with an ability to operate effectively across multiple priorities. Education Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field with 7+ years of experience Must be able to obtain and maintain a Public Trust. Contract requirement. Thanks & RegardsSonali KumariTechnical RecruiterKPG99, INC\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "88_Cloud Engineer with Appian": "Sonu Uprati,\nValiantIQ Inc\nsuprati@valiantiq.com\nReply to: suprati@valiantiq.com\nJob Title: Cloud Engineer with Appian Position Type: ContractLocation: Malvern, PA(Onsite)Client: Appian Corporation Primary SkillsCloud SQL JOB DESCRIPTION:Work on a small and highly skilled Appian team to deliver solutions from discovery through delivery.Perform critical application development, new application design and third-party integrations.Current portfolio consists of 61 applications across all lines of business.This is an application development role, with production support duties managed by a separate team.ResponsibilitiesProvides senior level system analysis, design, development, and implementation of work flow management system using Appian platformPractices full product ownership from discovery, analysis, design and implementation through testing and long-term maintenance support.Translates technical specifications, and/or logical and physical design into code for new or enhancement projects for internal clients. Develops code and test artifacts that reuse objects, is well structured, backed by automated tests, includes sufficient comments and is easy to maintain. Writes programs, appropriate test artifacts, ad hoc queries, and reports. Employs contemporary software development techniques to ensure tests are implemented in a way that supports automation.Elevates code into the development, test, and production environments on schedule. Provides follow up production support.Thoroughly understands software development methodology and architecture standards. Trains and mentors staff with less experience. Resolves issues elevated from staff with less experience.Participates in design and code reviews throughout life cycle to identify issues.Explains technical considerations at related meetings, including those with internal clients.Performs systems analysis, including system requirements analysis and definition (e.g., prototyping), and logical and physical design.Thoroughly understands client business functions and technology needs. Has a broad understanding of client's technologies, tools, and applications, including those that interface with business area and systems.Is highly functional on large, matrixed project teams. Interfaces with cross functional team members, including architects, IT security, scrum lead, product owner and clients. Communicates systems issues at the appropriate technical level for each audience.Thoroughly understands and complies with IT policies and procedures, especially those for quality and productivity standards that enable the team to meet established milestones. Thoroughly understands and complies with all Information Security policies and procedures, and verifies deliverables meet Information Security requirements.Participates in special projects and performs other duties as assigned.QualificationsTechnical Skills: Strong experience designing and developing applications using Appian PlatformDesign and implementation of complex SAIL interfaces and process modelsAppian certification is a plusAble to translate requirements into well-architected solutionsProblem-solver and a team playerStrong knowledge of SQLFamiliarity with the agile scrum SDLCNice to have: experience with automation including UIPath and Appian RPANice to have: experience with AWS including S3, EC2, Lambda, SQS and SNSNice to have: experience with Atlassian suite including Jira and ConfluenceAppian Design & Development \u2013 ExpertAppian Records & Data Fabric \u2013 ProficientAppian Integrations \u2013 ProficientSQL - ProficientPod Leadership Experience Thanks & Regards,Sonu UpratiTechnical Recruiter- ValiantIQ Inc.\"Searching Best Minds \u25a0 Searching Best Minds\"Email: suprati@valiantiq.com F. (302) 482-3672Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line for automatic removal. And mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "89_Looking for Azure Architect (E4), Dallas, Texas - Hybrid": "Mudassir,\nAvance consulting\nmudassir.uddin@avanceservices.com\nReply to: mudassir.uddin@avanceservices.com\nTitle : Azure Architect (E4)Location : Dallas, Texas - HybridRemote : No Technical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects.Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company.Roles & Responsibilities\u2022Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions\u2022Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022Develop deep relationships with key customer IT decision makers.Define long term cloud strategy and mentor operations teams to implement the solutions\u2022Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022Advocate new features and solutions to bring operational efficiency and cost reduction.\u2022Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements\u2022Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring\u2022Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.\u2022Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022Experience in PaaS services and Containers is preferred.\u2022Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022Demonstrated ability to adapt to new technologies and learn quickly.\u2022Proven track record of driving decisions collaboratively.\u2022Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "90_AWS CONNECT ENGINEER OR DEVELOPER || LONG TERM CONTRACT || VIDEO || HYBRID": "Jyoti Mittal,\nKpg99 Inc\njyotim@kpgtech.com\nReply to: jyotim@kpgtech.com\nHi,My name is Jyoti and I represent KPG99 INC. We work with clients across the USA. I have a very urgent role. If interested then please share with me your updated resume along with below information.Senior AWS Connect Engineer/DeveloperType: Long Term ContractLocation: Remote, but candidate must get badged onsite twice (Mandatory)USC OR GCClearance: Public Trust, open to all work statuses that meet the required criteria Key Required SkillsThe successful candidate will have fluency in AWS Connect design and development principles and work with stakeholders to develop enterprise business solutions that leverages industry processes and best practicesPosition DescriptionDesign, develop and implement AWS Connect processes to include new or modifications to applications, forms, workflow, policies, actions, access control, interfaces, and any other configurations required to support client processes. Provide recommendations for enhancing existing operations and provide recommendations for new updates or modifications. Support and enhance existing processes based on new requirements or upgrades. Facilitate requirements gathering, review and document requirements. Design and develop solutions within the ServiceNow environment.? Detailed Skills RequirementsFOUNDATION FOR SUCCESS (Basic Qualifications) Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field 7 years of experience working with large enterprise framework 7 years of hands-on experience working with AWS Cloud platform 3 years of hands-on experience working with Amazon Connect 3 years of experience using Node.js, Python or Java for development 3 years of experience in designing call routing & messaging Excellent interpersonal and communication skills Must be able to obtain and maintain a Public Trust. Contract requirement. FACTORS TO HELP YOU SHINE (Required Skills)These skills will help you succeed in this position: Knowledge or experience with CCaaS with Artificial Intelligence (AI) strategies and solutions. Knowledge and experience with integrating Pega workflows with CCaaS. Knowledge and experience with building reports and dashboards preferably with Tableau. Solid AWS Cloud and AWS Connect Developer experience. Extensive experience and knowledge with IVR, Automatic Call Distribution (ACD), Skill-Based Routing, Live Chat. Experience with process flow documentation creation. Strong verbal and written communication skills. HOW TO STAND OUT FROM THE CROWD (Desired Skills)Showcase your knowledge of modern development through the following experience or skills: Adept at working independently, but also in a team environment. Demonstrated experience delivering technology solutions in a fast-paced, deadline driven enterprise environment. Excellent understanding of change management, testing requirements, techniques, and tools to ensure high availability of systems. Prior Federal government experience. Self-starter, highly motivated individual who adapts to a dynamic work environment. Strong attention to detail with an ability to operate effectively across multiple priorities. EducationBachelor's Degree in Computer Science, Mathematics, Engineering, or a related field with 7+ years of experience Must be able to obtain and maintain a Public Trust. Contract requirement. Thanks & Regards,Jyoti Mittal| Technical Recruiter | KPG99,INC jyotim@kpgtech.com | http://www.kpg99.com/3240 E State St EXT, Hamilton, NJ 08619An E- Verified Company Certified Minority Owned BusinessNote: KPG99 does not endorse undesired email. If you do not want to receive our mails, please reply with Remove in the subject; I would ensure that you are not troubled further. I apologize for any inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "91_Cloud Architect!!ONSITE - Candidate MUST be local to the DC metro area!!!GC USC": "Priyanshi Goel,\nPransu Tech Solutions\npriyanshi@pransutechsolutions.com\nReply to: priyanshi@pransutechsolutions.com\nJOB TITLE: 740713 - OCTO (ECIS) Cloud ArchitectLocation: ONSITE - Candidate MUST be local to the DC metro areaDuration: 6+ monthsVisa: GC/USC OnlyClient: DC Government SHORT DESCRIPTION: 8-10 years of experience. Subject Matter Expert in Azure and AWS cloud infrastructure and network technologies. COMPLETE JOB DESCRIPTION: Responsibilities: 1. Experience spanning at least two IT disciplines, including technical architecture, network management, application development, middleware, database management, security, or operations. 2. Experience with multiple public cloud platforms, including AWS, Azure, and GCP. 3. Experience relocating ERP applications to a cloud-native environment. 4. Ensures that standards set by Data, Security, Infrastructure, Platform, and other architecture domains are followed when designing and architecting cloud solutions. 5. Demonstrate deep subject matter leadership of cloud architecture and implementation features (OS, multitenancy, virtualization, orchestration, elastic scalability). 6. Manage the development and implementation of cloud strategy, develop the architecture governance framework, and define architecture foundations. 7. Demonstrates expertise in conveying technical and functional concepts for a specific technical specialty. 8. Identifies improvements to project standards to achieve high-quality services/products. 9. Identifies best practices and standards for the use of the product. 10. Delivers support and design for industry-specific technologies that require integration with systems or networks. 11. Interact with executive-level business users or technical experts. 12. Functions as a niche technical SME. 13. Lead experience with technical expertise across large, complex implementations for systems. 14. Define and enforce Cloud architecture best practices: availability, redundancy schemes, performance, Disaster Recovery, data, and integration patterns. 15. Troubleshoot issues across the entire stack: IaaS, PaaS, services, applications, network, and security.1. Formulates and defines systems scope and objectives based on both user needs and a thorough understanding of business systems and industry requirements. 16. Devises or modifies procedures to solve complex problems considering computer equipment capacity and limitations, operation time, and form of desired results. Includes analysis of business and user needs, documentation of requirements, and translation into proper system requirements specifications. 17. Provides consultation on complex projects and is considered to be the top-level contributor/specialist of most phases of systems analysis, while considering the business implications of the application of technology to the current and future business environment. MINIMUM EDUCATION/CERTIFICATION REQUIREMENTS: \u2713 Bachelor\u2019s degree in Information Technology, or related field or equivalent experience,\u2713 or a current Azure and AWS Architect Certifications\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "92_Urgent Require--Devops Engineer--San Ramon, CA(Onsite from day 1)": "Rakesh,\nYochana\nrakeshv@yochana.com\nReply to: rakeshv@yochana.com\nHiHope you are doing great..! This is Rakesh from Yochana IT Solutions. I am currently recruiting for the below opportunity, Kindly let me know if you are comfortable with the requirement. Job Title :- Devops Engineer Job Location :- San Ramon, CA (Onsite from day 1)Duration:- Contract Job Description :-Primary Responsibilities: \u2022 Monitor production systems in multiple data centers. \u2022 Manage critical incident, including escalation, debugging, fix and root cause analysis. \u2022 Help R&D debug production issues. \u2022 End-to-end K8s application modernization and delivery via on K8S G6R \u2022 Decentralized \"enforced state\" GitOps approach. \u2022 Automated certificate management, DNS, and MTLS \u2022 Unified, auto scaling, easier to adopt and faster e2e develop to deploy framework and strategy.\u2022 Reduced go to market timelines (build to prod )\u2022 Managed DR with reduced costs \u2022 Cloud agnostic and support across 4+1 & Sovereign Cloud deployments \u2022 SGS, ISBN SecOps, and SAP HASI approved processes \u2022 Managed multi-stage backups (storage, Velero, DB-native) \u2022 Process automation and implementation. \u2022 Manage public cloud environments including Terraform, GCP, and/or AWS. \u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault. \u2022 Kubernetes containerization. \u2022 Operate system internals, file system structures and machine architectures in a Linux operating environment; and \u2022 Apache, DNS, Send mail, SSH, TCP/IP, and NFS. \u2022 Support services before they go live through activities such as system design consulting, developing software platforms and frameworks, capacity planning and launch reviews. \u2022 Scale systems sustainably through mechanisms like automation and evolve systems by pushing for changes that improve reliability and velocity. \u2022 Work with product team to define SLA, SLO, monitoring/telemetry pattern and stability across from customer standpoint. \u2022 Help product team gain momentum and velocity for hassle free development across BTP, Hyperscaler, K8S. \u2022 Build, Manage, Scale and Right size/TCO initiatives across SAP Hyperscaler(4+1) strategy towards cloud spend management. \u2022 Engage in and improve the whole lifecycle of services\u2014from inception and design, through to deployment, operation, and refinement. Qualifications \u2022 Development experience in Bash/Python/Go/Groovy, Perl and Shell scripting. \u2022 Process automation and implementation. \u2022 Experience with public cloud environments and toolsets including Terraform, GCP, and/or AWS. \u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault. \u2022 Kubernetes containerization experience. \u2022 Experience with operating system internals, file system structures and machine architectures in a Linux operating environment; and Apache, DNS, Send mail, SSH, TCP/IP, and NFS. \u2022 5-7 years of experience working in a public cloud, automation, and Unix internals \u2022 > 4 year devops/SRE experience. \u2022 Familiar with SRE operation and on-call procedure. \u2022 Familiar with log analysis, such as Splunk or ELK. \u2022 Scrum master certification preferred. \u2022 BA/BS degree in MIS/CS or equivalent experience \u2022 Experienced working in a dynamic, fast-paced environment with well-developed practices and procedures.\u2022End-to-end K8s application modernization\u2022Manage public cloud environments including Terraform, GCP, and/or AWS.\u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault\u2022 SGS, ISBN SecOps, and SAP HASI approved processes\u2022 Managed multi-stage backups (storage, Velero, DB-native)Artificial intelligenceScrum Master Certified Thanks & Regards,Rakesh Vellampalli,Email Id:rakeshv@yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "93_Kubernetes EKS AWS Cloud Engineer": "rahul sharma,\n3B Staffing LLC\nrahul.sharma@3bstaffing.com\nReply to: rahul.sharma@3bstaffing.com\nHiRole- Kubernetes EKS/AWS Cloud EngineerLocation \u2013 RemoteMode- Contract (C2C and W2) Must Haves: Kubernetes(EKS), Docker, AWS (S3, ECS, EC2, EFS, EBS, RDS, OpenSearch, ECR, EMR,IAM, ELB, Secrets Manager, KMS, Lambda, Autoscaling, CloudWatch, etc.)At least 5+ years of experience in developing cloud-native applications preferably using AWS (EKS, S3, ECS, EC2, EFS, EBS, RDS, OpenSearch, ECR, EMR, etc.)Hands-on experience with provisioning, maintaining, and deploying Kubernetes clusters in production environments, preferably AWS EKSPLUSES(BONUS)TelecommunicationCertified Kubernetes Administrator (CKA)AWS Certification Preferred Job ResponsibilitiesDesigning and implementing scalable solutions in a cloud environment leveraging cloud enterprise technology and services in AWS such as VPC, IAM, ELB, EC2, EKS, ECR, S3, EFS, WAF, EBS, EMR, Secrets Manager, KMS, RDS, IAM, Autoscaling, CloudWatch, OpenSearch and etc. using IAC tools AWS CLI or Cloud formationContinuously evaluate various parts of the platform for opportunities for security hardening and improvements in observability Evaluate new technologies and industry trends, develop proofs-of-concept, and present findingsIdentify the best cloud architecture options to help Verizon accomplish its strategic goalsDesign, implement, and run security solutions to detect vulnerabilities and misconfigurations at the various layers of the cloud infrastructure (virtual machines; containers; network, cloud environment).Integrate Application code/image scanning tools such as SonarQube, HPE Fortify, Twistlock, and Black Duck through the Jenkins CI/CD pipeline to automatically flag security vulnerabilities or policy violations to developers in their code before it\u2019s deployed.Identify new tools/technologies that will make the release and SCM process more efficient, repeatable, reliable,and cost-effective.Configure Identity and Access Management (IAM) users, roles, and policies.Must have a deep understanding of Kubernetes and Docker architecture and associated toolsAbility to debug issues with Kubernetes clusters and complex applications hosted on k8s/EKS.Experience with writing Helm Charts, Kubernetes manifest files, and docker filesExperience in migrating applications from On-premise bare metal to AWS EKS and supporting Hybrid environmentsCreate and manage Elastic Block Storage (EBS), S3 buckets, and Enable versioning & amp; Life cycle management policies on S3 buckets.Experience with observability, alerting, and tracing of distributed systems using tools like Prometheus, Grafana, etcMust be able to develop and maintain continuous integration and continuous deployment systems using tools like Git, GitHub, Jenkins, and HelmInfrastructure as Code development and deployments\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "94_Azure Architect || Chicago, IL (Onsite) Need Local": "Rishi Kumar,\nSiriinfo Sulution\nrishi.kumar@siriinfo.com\nReply to: rishi.kumar@siriinfo.com\nHi, Hope you are doing Well, We have an opening for Azure Architect. Below are the requirement details, just go through it & if you feel interested, revert back to me with your updated resume Your earliest reply is highly appreciated. Job Role \u2013 Azure ArchitectJob Type \u2013 ContractLocation \u2013 Chicago, IL (Onsite)Client -- TCS Job Description: This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Thanks, and regards,Rishi kumar Siri Info Solutions. Ph. No: 848 847 1089 Ext: 3160Mail Id: rishi.kumar@siriinfo.com Disclaimer: We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce. Note : Any resume submitted by Siriinfo is presented with the understanding that the candidate is being considered for your direct end-client (end-client is the company where the work will be performed). If there is any other company involved between the end-client and your company, please do not submit this resume without our written approval. If you submit the resume to another third party, Siriinfo reserves the right to work with the third party directly.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "95_Job For Azure Architect in Dallas, Texas - Hybrid": "Harish Joshi,\nResource Logistics\njoshi@resource-logistics.com\nReply to: joshi@resource-logistics.com\nTitle : Azure Architect Location : Dallas, Texas - HybridRemote : NoRate : $70 on C2C Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company.Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions\u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions\u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction.\u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements\u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring\u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.\u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred.\u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "96_Azure Data Engineer : Yorkville, TN": "Vishnu,\nsiri\nvishnu.m@siriinfo.com\nReply to: vishnu.m@siriinfo.com\nHello Guys,Hope you are doing Good, Below are the Positions which i have on priority Basis.If you have any suitable resumes, Kindly share resumes to Vishnu.m@siriinfo.comAzure Data EngineerLocation: Yorkville, TNHire Type: ContractAzure Databricks,ADLS,pysparkExperience in generating data pipelines in Azure ADFExperience in ingesting data from different Sources using in Azure ADFExperience in transforming the data using Azure Databricks & Delta lake and PysparExperience in Azure DevOps CI/CD \u2013 Stories, Bugs, and Issue managementImplement Integration solution in Microsoft AzurePerforms data analysis required to troubleshoot data related issues and assist in the resolution of data issuesExperience working in agile development projects and sprint deliverCollaborate with cross-functional teams to elicit data requirements and architect effective data pipelines.Perform data transformations and cleansing operations to ensure data accuracy and consistency.Evolve existing scripts and notebooks to meet client needs.Monitor, troubleshoot, and optimize data pipelines for seamless functionality.Ensure data solutions align with security and compliance standards Work closely with data architects to harmonize data models with project objectives.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "97_Urgent Role ::Azure Cloud Systems Architect:: Tallahassee, Florida": "Priyanka Singh,\nLargeton Inc\npriyanka.singh@largeton.us\nReply to: priyanka.singh@largeton.us\nHello,I am looking for interesting candidates that are available in the job market. I have new job opportunities from our reputed clients based in the USA. I have attached the job description below regarding the current job requirement. If you are interested, please respond to the email ignore if not interested.TITLE: Azure Cloud Systems ArchitectVISA: No H1B/CPT/OPT DURATION: 11 monthsLOCATION: Hybrid; Tallahassee, FloridaLinkedIn neededThis position will play a key role in assisting in the oversight, management, and execution of cloud services and platforms to support infrastructure, applications, data, and other business needs.QualificationsBachelor\u2019s Degree in Computer Science or another related field. 4 (four) years of experience can substitute for education.2 \u2013 3 years of strong professional experience as a Microsoft Azure cloud systems architect is required.5 - 8 years of professional experience in information systems engineering or another related field is required.2 - 3 years of experience in Enterprise Architecture is required.2 \u2013 3 years of experience with other cloud providers, such as AWS or GCP is highly desired.2 - 3 years of professional experience in administration of Microsoft software is highly desired.5 years of professional experience supporting middleware such as API Management, IT Service Automation tools, and batch job services is highly desired.Candidate must live in or near Tallahassee. Day to Day TasksDesign, upgrade, administer, test, and modify the virtual desktop infrastructure environment.Assist in ongoing modernization and migration efforts for applications, data, and infrastructure.Oversight and management of Microsoft 365 environmentsOversight and management of other Microsoft platforms and/or environments.Oversight and management of identity platforms.Build and manage relationships in a matrixed environment.Recommend and assist with the building and hosting of complex application solutions.Understand, deploy, and manage Microsoft Azure and/or other cloud services.Understand, deploy, and manage Microsoft Power Platform.Find opportunities to refactor solutions to obtain better performance, cost, and efficiency.Enhance infrastructure, application, and system monitoring solutions to prioritize system uptime.Review, advise, and design based on risks, costs, benefits and impact on the enterprise business process and goals.Automate processes wherever possible.Create and manage Azure DevOps projects, git repositories, and pipelines.Troubleshoot issues brought forth by developers and other architects.Present to your peers on findings on new technologies, enterprise-wide issues, and enhancements you have made to better the mission. Thanks & Regards,Priyanka SinghSr. Technical RecruiterEmail: priyanka.singh@largeton.us\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "98_Opening for Cloud DevOps Engineer - Hybrid": "meenakshi,\nkpg99\nmeenakshi@kpgtech.com\nReply to: meenakshi@kpgtech.com\nTitle - Cloud DevOps EngineerLocation \u2013Hybrid in Tampa, FL Duration \u2013 6+ Months Visa - USC with LinkedIn Requirements:1 DevOps2 AWS3 cloudform4 Terraform YOUR CONTRIBUTION: \u2022 Drive automation standards across our testing and production environments and be a key resource as we migrate our monolith and microservice application to the cloud\u2022 Create high-performance, scalable, and fault-tolerant computing systems while researching and providing solutions to challenging technical issues related to DevOps \u2022 Automate the infrastructure creation on cloud like AWS, Azure, etc.\u2022 Design and implement security governance practices for the applications and infrastructure\u2022 Automate pipelines and environments, build resilient systems and create repeatable processes, as well as update monitoring systems to detect service and security issues\u2022 Improve automation degree in integration, deployment, monitoring, and configuration management aspects of developed solutions and development infrastructure \u2022 Design and deploy, in an Agile (Scrum) environment, assets and code that will help, deploy, test, monitor, as well as maintain components and services for next-generation service platforms\u2022 Participate in the design and development process from definition to deployment while leading day-to-day operational support activities for services developed using the DevOps practice \u2022 Verify and monitor on-premises and cloud environments and respond to issues as they arise, adding new monitoring as needed\u2022 Recommend alternate choices and trade-offs for various design decisions while assisting with troubleshooting issues \u2022 Design and implement Fault tolerance architecture for all the applications YOUR EXPERTISE:\u2022 Customer-Centric \u2013 Focus efforts to ensure internal and external customers thrive \u2022 Communication and Persuasion \u2013 Possess solid communication and persuasion skills essential to success in this position. You will represent the Digital IT group effectively and employ negotiation skills that build organizational consensus around a specific course of action while identifying communication barriers and taking action to facilitate mutual understanding. \u2022 Business Acumen \u2013 Acknowledge the impact of your actions in local and global contexts while working to generate sustainable value (economic, social, and environmental) for the organization. In addition, you should understand basic business processes and possess the ability to identify and develop workflows and process maps for continual improvement and waste elimination within the business.\u2022 Technical Expertise \u2013 Understand all IT functions, cloud technologies, and services in use, and become familiar with the current direction in the infrastructure market \u2022 Strategic Thinking and Vision \u2013 Possess a clear sense of the direction of the organization and understand what will create a competitive advantage for the business in the future. Distinguish tactical issues from strategic ones and take action while effectively translating strategic imperatives at the macro level into initiatives and priorities. \u2022 Simplistic Agility \u2013 Foster simplicity and agility to look for ways to streamline work and processes to become more effective and efficient while developing new ideas quickly\u2022 Global Perspective \u2013 Ability to successfully operate in a global IT organization bringing perspective to daily work while leveraging relationships, expertise, and opportunities across borders taking the company\u2019s strategy and priorities into consideration. \u2022 Accountability \u2013 Take initiative and assume personal accountability and ownership for goals, outcomes, and deadlines holding others accountable for achieving individual and organizational objectives. \u2022 Analysis \u2013 Possess critical thinking ability and deal systematically with input from a variety of sources to suggest alternative approaches to unfamiliar situations or concepts, as well as strong analytical skills, including an understanding of business economics and financial resources.\u2022 Supplier Management \u2013 Possess the ability to engage and manage suppliers as needed for discovery, modeling, implementation, and support. YOUR CREDENTIALS: \u2022 Bachelor's Degree in Computer Science or similar engineering discipline from an accredited university\u2022 At least 5 years of professional experience\u2022 Knowledge of deployment and configuration technologies (e.g., Gitlab, Jenkins, Ansible, etc.)\u2022 Knowledge of Containerization and orchestration technologies (e.g., Docker, K8s, etc.)\u2022 Working Experience with Cloud technologies (e.g. AWS, Azure)\u2022 Knowledge of IaC tools (e.g., Terraform, CloudFormation, Azure ARM)\u2022 Knowledge of tools like Helm, Artifactory, SonarQube, etc.\u2022 Knowledge of source control technologies (e.g., Git, subversion)\u2022 Experience operating in virtual environments required\u2022 Experience with both Windows and Unix based operating systems required\u2022 Knowledge of one or more scripting languages (e.g., Python, Ruby, Bash)\u2022 Critical thinker and problem-solving skills\u2022 Team player\u2022 Excellent time-management skills\u2022 Interpersonal and communication skills\u2022 Ability to effectively analyze data and present solid recommendations Regards, Meenakshi BishtTechnical RecruiterD: 609-357-5502E: meenakshi@kpgtech.comNote:: We are going through prime vendor.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "99_Lead Azure Cloud Engineer || Tampa FL || Local || USC GC": "Awkash Kumar,\nLargeton INC\nawkash@largeton.us\nReply to: awkash@largeton.us\nJD ::Visa- USC/GC Our client is a financial services company that provides clearing, settlement, custody, and risk management services for securities transactions. It acts as a central clearinghouse, manages assets, reduces counterparty risk, facilitates trade reporting, and operates technology platforms for efficient processing. Its role is to ensure the smooth functioning of securities markets and enhance transparency in the financial system. Being a member of Technology Research and Innovation (TRI) group, you will be expected to research new technologies independently and recommend appropriate automation solutions for various applications to be deployed on Cloud Environment to include both AWS and Azure. Utilize cloud best practices and standards: contribute to success criteria from design through implementation, including, reliability, cost-effectiveness, performance, maintainability, reuse, extensibility, usability, and scalability; contribute expertise on significant application components, program languages, databases, and operating systems and guide less experienced staff during the build and test phases. Design and build PoTs to make well-informed technology recommendations. Your Responsibilities: Platforms \u2013 Work with development teams to stand up desired cloud environments.Requirements Elaboration \u2013 Identify the needs for build automation and implement appropriate solutions.System Performance \u2013 Contribute to solutions that satisfy performance requirements, tune application performance issues, ensure service uptime and response time meet SLAs.Standards \u2013 Be aware of CICD standards and best practices and utilize them in solutions effectively.Documentation \u2013Develop and maintain system documentation.Support team in managing client expectations and resolving issues on time. Talents Needed for Success: Bachelor\u2019s degree in computer science, Applied Computer Science, or related field.5+ years or related experience.Passion for technology innovation, a curious mind, and an entrepreneur mindset.Ability to present technical information clearly to different management levels.Azure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plusOther experience to include the following technologies: CICD patterns, Terraform.Experience using the following tools: GIT, Bit Bucket, Jira, Confluence, Jenkins. Experience with Python scripting preferred.Working knowledge of AWS is a plus.Knowledge of different software development methodologies (Waterfall, Agile, Scrum, Kanban)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "100_AWS Data Application Infrastructure Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: AWS Data Application Infrastructure Lead - OnsiteWork location:ATLANTA (US:30301), GA CLient : Capgemini DESCRIPTION.Manage AWS Infrastructure for provisioning\ufffdManage and Streamline Deployments of Pipelines to Production\ufffdManage FinOps for Productionalized applications\ufffd.POC and recommendations for product usage and processes\ufffdMust Have Experience in Big 4 Consulting Companies\ufffd Strong Communication Skills and Client Relationship management\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "101_(One Round Interview)-ETL Test Lead with Azure  Remote": "Selvakumar,\nVbeyond Corporation\nselvak@vbeyond.com\nReply to: selvak@vbeyond.com\nGreetings! I hope you are doing well. I am Selva from V-Beyond Corp. We are a global recruitment company with a specialization in the hiring of IT professionals. One of our clients is looking for a ETL Test Lead with Azure \u2013 Remote Role : ETL Test Lead with Azure(ADF, Databrick, Azure Devops)Mode : ContractLocation : Remote As an ETL Test Lead you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards .Responsibilities: Experience of ETL testing, specifically utilizing ADF ,Databricks and Azure, to test data integration workflows ensuring seamless data flow across diverse systems. Knowledge on Azure based application Ability to review test scenarios and test cases Develop and execute comprehensive test plans, test cases, and test scripts for interoperability projects. Ability to test UI based application also added advantage.Collaborate with development, DevOps, and QA teams to ensure seamless integration and deployment processes.Conduct functional, regression, integration, and performance testing on various systems and applications. Identify, document, and track defects, and work with developers to ensure timely resolution.Ensure testing processes align with industry standards and best practices.Participate in Agile ceremonies, including sprint planning, daily stand-ups, and retrospectives.Utilize automated testing tools and frameworks to enhance test coverage and efficiency.Continuously improve test processes and methodologies to support DevOps practices.Stay updated on the latest industry trends and technologies to ensure the adoption of best practice Ensure data exchange processes comply with industry standards and regulations Thanks & RegardsSelva KumarVBeyond Corporation || PARTNERING FOR GROWTHHillsborough, New Jersey, USAEmail ID: SelvaK@vbeyond.comMobile : 862-270-5844Website : www.vbeyond.com Note: Please allow me to reiterate that I chose to contact you either because your resume had been posted to one of the internet job sites to which we subscribe, or you had previously submitted your resume with Vbeyond. I assumed that you are either looking for a new employment opportunity, or you are interested in investigating the current job market.If you are not currently seeking employment, or if you would prefer I contact you at some later date, please indicate your date of availability so that I may honor your request. In any event, I respectfully recommend you continue to avail yourself to the employment options and job market information we provide with our e-mail notices.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "102_DevOps Engineer:: Onsite:: Los Angeles, CA  OR Dallas, TX": "Subhra Shahi,\nUrBench\nshubhra@urbench.com\nReply to: shubhra@urbench.com\nJob Title: DevOps Engineer - Data Engineering Contract: Long Term Client: IBM Location: Los Angeles, CA / Dallas, TX(onsite) VISA: USC,GC,H4EAD Job Summary: We are seeking a skilled DevOps Engineer to join our technology team, focusing on automating, and optimizing our data engineering pipelines. The ideal candidate will have a strong background in software development, automation of CICD using Jenkins in data platforms like Databricks and Snowflake. You will play a crucial role in developing and enhancing CI/CD pipelines, automating tasks, and ensuring seamless data operations across our platforms. Key Responsibilities: Develop and enhance CI/CD pipelines using Jenkins to automate and streamline deployment processes.Write and maintain scripts in Groovy to support automation and integration tasks.Utilize REST APIs to integrate different systems and services, and process JSON responses effectively using Python.Work closely with data engineering teams to understand their Devops needs and implement solutions that enhance data workflow efficiencies.Deploy dathub jobs using Jenkins pipeline.Collaborate with development and operations teams to ensure system reliability, security, and efficiency.Stay updated with the latest industry trends and technologies and apply insights to improve existing systems and workflows. Qualifications: Proven experience as a DevOps Engineer or similar role with a focus on data engineering platforms.Strong programming skills in Groovy, Python, and experience in Jenkins and CI/CD practices.Experience with REST APIs and data manipulation, particularly JSON.Familiar with Databricks and Snowflake.Strong problem-solving skills, attention to detail, and ability to work in a collaborative team environment.Excellent verbal and written communication skills.This role is perfect for someone passionate about automation, data flow, and continuous improvement in data operations environments. Kind Regards, Shubhra Shahi | Technical Recruiter Direct: +15122542396Email: Shubhra@urbench.com https://www.linkedin.com/in/shubhra-shahi-kaushik\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "103_Lead Azure Cloud Engineer": "Deepak Thakur,\nLargeton Inc\ndeepak@largeton.us\nReply to: deepak@largeton.us\nTittle: Lead Azure Cloud Engineer Client :NOT disclosedvisa: USC/GC Location: Tampa ( Hybrid \u2013 3 Days onsite)Interview Process: This is a video-on interview and ensure candidates join 5 minutes prior the interview time. Duration: 6 months contract to hire Client Detail (What does the company do? Recent history? Website? Parent Company? Size of Company?)Our client is a financial services company that provides clearing, settlement, custody, and risk management services for securities transactions. It acts as a central clearinghouse, manages assets, reduces counterparty risk, facilitates trade reporting, and operates technology platforms for efficient processing. Its role is to ensure the smooth functioning of securities markets and enhance transparency in the financial system. Being a member of Technology Research and Innovation (TRI) group, you will be expected to research new technologies independently and recommend appropriate automation solutions for various applications to be deployed on Cloud Environment to include both AWS and Azure. Utilize cloud best practices and standards: contribute to success criteria from design through implementation, including, reliability, cost-effectiveness, performance, maintainability, reuse, extensibility, usability, and scalability; contribute expertise on significant application components, program languages, databases, and operating systems and guide less experienced staff during the build and test phases. Design and build PoTs to make well-informed technology recommendations. Your Responsibilities: Platforms \u2013 Work with development teams to stand up desired cloud environments.Requirements Elaboration \u2013 Identify the needs for build automation and implement appropriate solutions.System Performance \u2013 Contribute to solutions that satisfy performance requirements, tune application performance issues, ensure service uptime and response time meet SLAs.Standards \u2013 Be aware of CICD standards and best practices and utilize them in solutions effectively.Documentation \u2013Develop and maintain system documentation.Support team in managing client expectations and resolving issues on time. Talents Needed for Success: Bachelor\u2019s degree in computer science, Applied Computer Science, or related field.5+ years or related experience.Passion for technology innovation, a curious mind, and an entrepreneur mindset.Ability to present technical information clearly to different management levels.Azure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plusOther experience to include the following technologies: CICD patterns, Terraform.Experience using the following tools: GIT, Bit Bucket, Jira, Confluence, Jenkins. Experience with Python scripting preferred.Working knowledge of AWS is a plus.Knowledge of different software development methodologies (Waterfall, Agile, Scrum, Kanban) Thanks & RegardsDeepak ThakurDirect No-571-463-1404LARGETON INC. 13800 Coppermine Rd,Herndon, VA 20171\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "104_Looking for Data Architect - Snowflake, AWS at Naperville IL And Piscataway NJ_Hybrid": "Teja,\ninsoursysinc\ntejaswi_athili@insoursysinc.com\nReply to: tejaswi_athili@insoursysinc.com\nPosition : Data Architect \u2013 Snowflake, AWS Candidate can work from NAPERVILLE, IL OR from \u2013 Piscataway NJHybrid \u2013 2 days a week Skills : AWS AirByte , Snowflake, DBT, AWS S3 , AWS LambdaLooker -Good to have Responsibilities Lead efforts to ensure data capabilities are operating efficiently, accurately, and with high quality.Collaborate with programs & projects to understand data dependencies, anticipate risks, and identify opportunities. Guide data project decisions based on the direction of data domain roadmap.Contribute data domain expertise to feasibility, complexity assessments of potential future initiatives including data domain analysis.Develops an integrated view of data capabilities and processes, using a repeatable approach, cohesive framework, and available industry best practices and techniques.Provides consulting and makes recommendations to tackle data transformation initiatives, help mature existing data capabilities, or enable new data capabilities.Facilitate the process of defining and publishing data standards, best practices, data capabilities, and data roadmap, synchronized with the overall enterprise future state architecture.Collaborate in creation of conceptual and logical architecture and models following the enterprise guidance to describe a particular domain of data and use these models to inform the physical design of data-related projects.Co-develop data life cycle governance framework including standards, patterns, and controls. QualificationsMinimum eight years of relevant experience as a data architect building large-scale data solutions.Experience in Migration of existing On-Prem Data warehouses to Snowflake.Experience in architecting and large data modernization, data migration, data warehousing \u2013 experience with cloud-based data platforms (like Snowflake)Should have experience in architecture and implementing End to End Modern Data Solutions using AWS, Redshift, S3 Good appreciation and at least one implementation experience on processing substrates in Data Engineering \u2013 such as AWS Glue, ETL Tools, ELT techniquesExperience with defining and operationalizing data strategy, data governance, data lineage and quality standards.Experience designing highly scalable ETL processes with complex data transformations, data formats including error handling and monitoring .Extensive knowledge of data engineering, data integration and data management concepts (i.e. APIs, ETL, MDM, CRUD, Pub/Sub, etc.)Experience with data Modeling.Experience with structured and hierarchical datasets (i.e. JSON, XML, etc.)Engineering experience with large scale system integration and analytics projects. RegardsTejaswi Athili Insoursys | Ph: 9724400065 | tejaswi_athili@insoursysinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "105_Sr Cloud Network Engineer - Bellevue, WA - Contract": "Naveen Kumar,\nTeceze Consultancy Services Private Limited\nnaveen.kumar@teceze.com\nReply to: naveen.kumar@teceze.com\nOverview: Teceze is a leading information technology, consulting and business process services company headquartered in the UK and has offices across 15+ countries. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, Teceze has a dedicated workforce of over 1500+ employees, serving clients in 75+ cities across 6 continents.Teceze is a leading information technology, consulting and business process services company headquartered in the UK and has offices across 15+ countries. A company recognized globally for its comprehensive portfolio of services, strong commitment to sustainability and good corporate citizenship, Teceze has a dedicated workforce of over 1500+ employees, serving clients in 75+ cities across 6 continents. Position: Sr Cloud Network Engineer Location: Bellevue, WA 98006 Job Type: Contract Work Type: Onsite Job Description: Key Responsibilities: Oversee the network onboarding process for new users and systems into Cloud environment Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations. Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms. Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways. Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats. Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents. Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure. Conduct regular network assessments and audits to ensure compliance with internal and external requirements. Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards. Provide technical support and troubleshooting for Cloud network-related issues Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications Experience with network onboarding and provisioning in cloud environments. Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud. Familiarity with security regulations, standards, and compliance requirements for cloud network environments. Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers. Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud. Strong communication and documentation skills for collaborating with cross-functional teams. Experience with automation tools (e.g., Python, Terraform) for network configuration and management Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "106_Terraform Enterprise Architect or Lead with GCP and Terraform experience - Austin, TX- onsite": "james Smith,\nYochana solutions\nsmith@yochana.com\nReply to: smith@yochana.com\nJob Title- Terraform Enterprise Architect or Lead with GCP and Terraform experience Location \u2013 Austin, TX- onsite Duration \u2013 1+year Terraform and GCP Certification is Must 12 to 15 years of Minimum experience 5+ Years of experience in GCP 5+ Years of experience in Terraform Enterprise Architect or Terraform Architect Azure/AWS or other cloud providers experience Docker and other virtualization technologies, Scripting language (bash, PowerShell etc) Expert-level proficiency in designing and implementing cloud infrastructure solutions on the google Cloud Platform (GCP) Utilizing Terraform for infrastructure as code (IaC). Extensive hands-on experience with Terraform, including module development, configuration management, and state management, to automate and streamline infrastructure provisioning and management processes. Deep understanding of GCP&#39;s core services and offerings, such as Compute Engine, Cloud Storage, Cloud SQL, VPCs, and IAM, and how to leverage Terraform to provision and manage these resources effectively. Demonstrated ability to utilize Terraform to orchestrate complex, multi-tiered GCP architectures, incorporating best practices for security, scalability, and cost optimization. Skilled in Version 1.2 troubleshooting Terraform configurations, identifying and resolving infrastructure-related issues, and optimizing Terraform code for improved performance and efficiency. Thanks & Regards James SmithTeam Lead smith@yochana.com Yochana Solutions INCWindsor, Ontario- Canada Farmington hills, MI-48335- USAUSA | CANADA I INDIADirect No: 949-201-1313 W: www.yochana.comPlease follow us on :USA | CANADA | INDIA Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program. Note: This is not an unsolicited mail. If you are not interested in receiving our e-mails then please reply with subject line Remove\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "107_Cloud DevOps Engineer----Only USC or Ead": "preeti,\nKpg tech\npreeti@kpgtech.com\nReply to: preeti@kpgtech.com\nCloud DevOps EngineerLocation: Tampa, FL (2 days onsite) *Local or day 1 relocationDuration: 6 Month CTH \u2013 No SponsorshipRequirements:1DevOps 2AWS 3cloudform 4Terraform YOUR CONTRIBUTION: Drive automation standards across our testing and production environments and be a key resource as we migrate our monolith and microservice application to the cloudCreate high-performance, scalable, and fault-tolerant computing systems while researching and providing solutions to challenging technical issues related to DevOps Automate the infrastructure creation on cloud like AWS, Azure, etc.Design and implement security governance practices for the applications and infrastructureAutomate pipelines and environments, build resilient systems and create repeatable processes, as well as update monitoring systems to detect service and security issuesImprove automation degree in integration, deployment, monitoring, and configuration management aspects of developed solutions and development infrastructureDesign and deploy, in an Agile (Scrum) environment, assets and code that will help, deploy, test, monitor, as well as maintain components and services for next-generation service platformsParticipate in the design and development process from definition to deployment while leading day-to-day operational support activities for services developed using the DevOps practiceVerify and monitor on-premises and cloud environments and respond to issues as they arise, adding new monitoring as neededRecommend alternate choices and trade-offs for various design decisions while assisting with troubleshooting issuesDesign and implement Fault tolerance architecture for all the applicationsYOUR EXPERTISE:Customer-Centric \u2013 Focus efforts to ensure internal and external customers thrive Communication and Persuasion \u2013 Possess solid communication and persuasion skills essential to success in this position. You will represent the Digital IT group effectively and employ negotiation skills that build organizational consensus around a specific course of action while identifying communication barriers and taking action to facilitate mutual understanding. Business Acumen \u2013 Acknowledge the impact of your actions in local and global contexts while working to generate sustainable value (economic, social, and environmental) for the organization. In addition, you should understand basic business processes and possess the ability to identify and develop workflows and process maps for continual improvement and waste elimination within the business.Technical Expertise \u2013 Understand all IT functions, cloud technologies, and services in use, and become familiar with the current direction in the infrastructure market Strategic Thinking and Vision \u2013 Possess a clear sense of the direction of the organization and understand what will create a competitive advantage for the business in the future. Distinguish tactical issues from strategic ones and take action while effectively translating strategic imperatives at the macro level into initiatives and priorities. Simplistic Agility \u2013 Foster simplicity and agility to look for ways to streamline work and processes to become more effective and efficient while developing new ideas quicklyGlobal Perspective \u2013 Ability to successfully operate in a global IT organization bringing perspective to daily work while leveraging relationships, expertise, and opportunities across borders taking the company\u2019s strategy and priorities into consideration. Accountability \u2013 Take initiative and assume personal accountability and ownership for goals, outcomes, and deadlines holding others accountable for achieving individual and organizational objectives. Analysis \u2013 Possess critical thinking ability and deal systematically with input from a variety of sources to suggest alternative approaches to unfamiliar situations or concepts, as well as strong analytical skills, including an understanding of business economics and financial resources.Supplier Management \u2013 Possess the ability to engage and manage suppliers as needed for discovery, modeling, implementation, and support. YOUR CREDENTIALS: Bachelor's Degree in Computer Science or similar engineering discipline from an accredited universityAt least 5 years of professional experienceKnowledge of deployment and configuration technologies (e.g., Gitlab, Jenkins, Ansible, etc.)Knowledge of Containerization and orchestration technologies (e.g., Docker, K8s, etc.)Working Experience with Cloud technologies (e.g. AWS, Azure)Knowledge of IaC tools (e.g., Terraform, CloudFormation, Azure ARM)Knowledge of tools like Helm, Artifactory, SonarQube, etc.Knowledge of source control technologies (e.g., Git, subversion)Experience operating in virtual environments requiredExperience with both Windows and Unix based operating systems requiredKnowledge of one or more scripting languages (e.g., Python, Ruby, Bash)Critical thinker and problem-solving skillsTeam playerExcellent time-management skillsInterpersonal and communication skillsAbility to effectively analyze data and present solid recommendations Thanks & RegardsPreeti UpadhyayTechnical Recruiter Mail id: preeti@kpgtech.com Contact: 609 357 5952 Ext 221\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "108_Azure DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDetails of the role:The roles is designed to be a SME on the design, development, and implementation of DevOps CI/CD pipelines. The client is looking for thought leadership to provide pros and cons of different options and alternatives and provide recommendations for best approach, best practicesKey responsibilities:Understand business and technical requirements of a CI/CD PipelineDesign, develop, and implement CI/CD pipelinesProvide thought leadership on recommendations for improvementRequired Skills:Minimum 4 years of experience with other DevOps tools, such as Github, Jenkins/GitHub Actions, Artifactory, Ansible/Octopus Deploy, AppDynamics, Splunk or equivalentMinimum of 4 years of experience with Dockerfile and image creation, running containers on Kubernetes or using Docker run time environment.Minimum of 4 years of experience in any of the scripting languages like Shell Scripting, Groovy, Python, Perl or PowerShell or equivalentMinimum of 2 years of experience with relational database management skills like MSSQL, MySQL, SQL, Postgres or MongoDB preferred or equivalent.Minimum of 2 years of experience utilizing DevOps within private cloud and public cloud platforms like Microsoft Azure, Google or Amazon AWS cloud services or equivalentAbility to perform as a team member.Good to have skills:Strong written and verbal communication skills\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "109_Azure Solution Architect": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nJob Description -Azure Solution ArchitectLocation: Onsite in Annapolis, MDType: Very Long-Term ContractMust be local to Maryland In this role I need only senior type of candidate who must have 6 years of experience in D-365 AND have experience in ENTRA ID in RESUME and make sure Entra ID introduced in July 2023 so update resume accordingly but must be in resume. please give me the senior type of candidate .Key Responsibilities:Azure Administration:Administer and manage Azure resources, including Virtual Machines, Storage Accounts, Virtual Networks, AKS, Azure Functions, and more.Monitor and optimize Azure costs, configurations, and licensing to ensure cost efficiency.Uphold security best practices, including managing EntraID/Azure AD, role-based access control (RBAC), and Azure Security Center.Ensure architectural integrity and adherence to industry best practices across all Azure services.D365 and Power Platform Management:Oversee and administer D365 applications and the Power Platform (Power Apps, Power Automate, Power BI).Configure, customize, and maintain D365 applications to align with business needs.Implement governance and compliance policies for D365 and Power Platform.DevOps and CI/CD Pipeline Development:Develop and manage CI/CD pipelines for Power Platform and D365 applications using Azure DevOps.Automate deployment processes to ensure seamless integration and delivery of applications.Collaborate with development teams to streamline application development and deployment processes.Disaster Recovery and Business Continuity Planning:Develop and implement disaster recovery (DR) and business continuity plans (BCP) for D365 applications.Configure Azure to support DR and BCP, ensuring minimal downtime and data loss in case of failures.Conduct regular DR/BCP drills to ensure readiness and effectiveness. Backup and Security Management:Ensure all application databases and configurations are securely backed up.Implement and manage backup policies and procedures, including regular testing of backup and restore processes.Maintain the security of application data and configurations, ensuring compliance with data protection regulations.Performance Monitoring and Optimization:Monitor the performance and availability of Azure resources, D365 applications, and Power Platform solutions.Identify and resolve performance bottlenecks and issues.Implement best practices for performance optimization and scalability.Documentation and Reporting:Maintain comprehensive documentation of configurations, processes, and procedures.Generate regular reports on system performance, security, and cost efficiency.Provide training and support to end-users and technical teams as needed.Qualifications:Education: Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.Experience: Minimum of 6 years in Azure administration, DevOps, and D365/Power Platform management.Skills:Strong knowledge of Azure services, including EntraID/Azure AD, Virtual Machines, Storage, Networking, AKS, and Azure DevOps.Experience with disaster recovery and business continuity planning for cloud applications.Proficiency in developing and managing CI/CD pipelines using Azure DevOps or similar tools.Excellent understanding of security best practices and compliance requirements.Strong analytical, problem-solving, and organizational skills.Excellent communication and collaboration skills.Preferred Qualifications:Certifications: Azure Administrator Associate, Azure DevOps Engineer Expert, or Azure Solutions Architect.Technical Skills: Experience with scripting and automation using PowerShell, Azure CLI, or other relevant tools.Methodologies: Familiarity with agile methodologies, specifically Scrum and Kanban.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "110_Azure Architect -Dallas,Texas - Hybrid": "Nikitha,\nAvance Consulting\nnikitha.velmala@avanceservices.us\nReply to: nikitha.velmala@avanceservices.us\nTitle : Azure Architect (E4)Location : Dallas, Texas - HybridRemote : No Technical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects.Position - Solutions Architect (Azure)Designation - Consultant/Senior Consultant/Solutions ArchitectExperience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company.Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions\u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions\u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction.\u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements\u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring\u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.\u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred.\u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "111_Lead Cloud Engineer": "Satyajit Nayak,\ntekinspirations\nsatyajit@tekinspirations.com\nReply to: satyajit@tekinspirations.com\nHello, Hope you are doing well!Please review the JD and share resumesJob Description -Job Title: Lead Cloud EngineerWork Arrangement: Hybrid, 3 days on-siteLocation: New York City (NYC) or New Jersey (NJ) or Florida (FL)Duration: 6 months CTHStrongly preferred: Experience working with a variety of Banking IT software and service providers. Lead experience with working around team of 20. Must have good GCP experience.POSITION SUMMARY:The Cloud Engineer is responsible for architecting and developing cloud-native platforms, leveraging infrastructure as code concepts, and developing software-defined strategies for infrastructure management, application deployment, and services. They are also critical to providing continuous technical support to both internal and external customers and acting as an escalation point for daily management and maintenance of cloud-based infrastructure issues.PRINCIPAL RESPONSIBILITIES:Design state-of-the-art technical solutions on Multi-Cloud infrastructure that address customer's requirements for scalability, reliability, security, and performanceInstall, configure, automate, and monitor various cloud services, Infrastructure as a Service and Platform as a Service (IaaS, PaaS)Configure application-aware infrastructure to automatically handle application requirements, security, and disaster-preparedness functions such as backups and data recoveryPerform operational engineering for activities which include platform upgrades, server patching, monitoring, configuration, and troubleshooting. Automate where possible.Work closely with a team of architects, engineers, and developers to create functional design specificationsCreate and manage all cloud provisioning scripts needed for single instance, environment-based (dev, prod, etc.) or regional-specific resourcesClosely coordinate with teams on additional cloud automation above the infrastructure layerLead and coordinate the work of an integrated project team comprised of developers, subject matter experts, database administrators, system administrators, and system architects to implement and maintain enterprise-level information technology applicationsLead in all cloud technology deployment activities, workflow configuration and development, and third-party system integrationOTHER RESPONSIBILITIES:Comply with all regulations pertaining to BSA, USA PATRIOT ACT, and OFAC. Complete annual BSA training. Report suspicions of criminal activity, or any attempt to avoid BSA reporting requirements on the part of customers or employees, to the AML/BSA Compliance Department. For supervisory positions, ensure BSA training requirements are completed by staff.KNOWLEDGE, SKILLS, AND ABILITIES (KSA):Good understanding of cloud computing technologies across Windows and/or Linux, with demonstrated hands-on experience on one or more of the following domainsCloud Computing Platforms: Amazon Web Services (AWS) platform, Azure Core Platform, Google Cloud Platform (GCP): Compute, Storage and NetworkingInfrastructure as Code : VMware, CloudFormation, ARM templates, Deployment Manager, or TerraformAdministration of virtualized platforms on various cloud providers (public and private)Management of cloud networks and connectivity including VPNs and direct lease lines from datacentersKnowledge of Operating Systems, Virtual Machine environments, vendor market images, and Relational Database Management Systems, Systems Automation platforms and TechnologiesData Platform : SQL, Azure DB, Cosmos DB, HD InsightsLinux systems administration, configuration, troubleshooting and automationWeb Technologies: IIS, Azure Web Apps, PHP, Apache, Tomcat, Cassandra, Kafka, Nginx, HAProxy, MySQL etc.Identity and Authentication : SSO/Federation, AD/Azure AD etc.Extensive knowledge of Middleware & System EngineeringScripting skills (Python/Ruby/Bash/Go/etc.)Programming skills (PowerShell, Python, Node.js, C#, Java OpenSource)Working knowledge of at least one configuration management tool, such as Ansible, Chef, and/or Puppet, Docker and other automation toolsExperience with Active Directory including managing/maintaining several domainsGeneral understanding of Microservices, such as Docker, Kubernetes, and API Management PlatformsDeep understanding of firewalls, NAT, and packet filteringManagement Suite (OMS), certificates and SSL managementKnowledge of backup and disaster recovery processesStrong knowledge of IT compliance, information security, and risk managementStrong knowledge of multiple security concepts and methods such as vulnerability assessments, data classification, privacy assessments, incident response, security policy creation, enterprise security strategies, architectures, and governanceAll aspects of DevOps (source control, continuous integration, deployments, etc.)Excellent troubleshooting skills and effective verbal and written communication skillsEDUCATION AND EXPERIENCE:Bachelor's degree and a minimum of 7 years of related work experienceExperience with cloud management frameworksApproximately 4-7 years of technical architecture experienceExperience with Azure Resource TemplatesImplement security architectures for cloud/hybrid systemsExtensive Python, CFT (JSON/YAML), Terraform skills for building out automation and infrastructure as codeExperience in standing up CICD pipeline, orchestration with Jenkins/bamboo, code development in one of cm technologies with chef, puppet, ansibleExperience with Windows Server management (2012 r2, 2016, 2019, 2022)Experience deploying web and service-based applications in Windows/Linux environmentsKnowledge of network equipment/IOS: routing, switching, and firewallsKnowledge of network protocols such as: DNS, SMTP, SNMP, SSH, SFTPStrong understanding of networking (TCP/IP, OSI model), operating system fundamentals (Windows, UNIX, mainframe), security technologies (firewalls, IDS/IPS, etc.) and application programming/scripting languages (C, Java, Perl, Shell)Experience using Git, JIRAExperience with Docker and container orchestration (Kubernetes)Experience with Management Suite (OMS), certificates and SSL managementExperience with JavaScript Frameworks (Angular), RESTful APIs, JSON, and XML formats is a plusGreat working knowledge of IP networking, VPN's, DNS, and load balancingGeneral understanding of CMS applications, .Net Framework and/or .Net CoreProven technical troubleshooting and performance tuning experience.Experience with monitoring tools, such as Application Insights, Microsoft Operations, Prometheus, Grafana, and Splunk.Strong Jenkins background and experience with Artifactory and build pipelines.Strong understanding of regulatory requirements and compliance issues affecting clients related to privacy and data protection, such as PCI DSS, GLBA, Basel II, EU Data Protection Directive, International Cross Border, and U.S. State Data Privacy Laws.Experience working with a variety of Banking IT software and service providersCERTIFICATES, LICENSES, AND REGISTRATIONS:Required: One or more cloud platform certificationsPreferred: Azure Certification: Azure Solutions Architect Expert, Amazon Certifications: AWS Certified Solutions Architect \u2013 Professional, GCP: Cloud Architect and/or Cloud DevOps EngineerWORK SCHEDULE:Standard 40-hour work week, Monday to Friday 8:00 am to 5:00 pm. Flexible/additional hours (including after-hours/weekends) may be needed.Periodic off-hours on-call support for production and development environmentsRegards, Satyajit nayakSr. Technical RecruiterTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035E: satyajit@tekinspirations.com Linkedin: linkedin.com/in/satyajeet-nayak-85751625b\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "112_Hiring for Cloud Developer - Philadelphia, PA - Day 1 Onsite and Hybrid  - AVEVA - Contract to Hire": "Gangadar Reddy,\nCentraprise\nedula.gangadar@centraprise.com\nReply to: edula.gangadar@centraprise.com\nRole: Cloud DeveloperLocation: Philadelphia, PAHire type: ContractJob description:Contract to HirePlease ensure we have profiles not more than 3 pages, resume should focus on associates' contributions in business growth of client and technology used for the same rather than just highlighting technologies and year of experience.JD:Principal Accountabilities\u2022 To design and develop cutting edge, highly available and scalable features\u2022 To maintain and grow your knowledge in an environment of rapidly changing technology\u2022 To promote cloud technologies and practices throughout AVEVA\u2022 To demonstrate and promote secure practices\u2022 To work as part of an agile development teamBehaviours:\u2022 Thirst for learning and the confidence to put the learning into practice straight away\u2022 Enthusiastic and positive attitude\u2022 Be both highly motivated and motivating\u2022 Enjoy working in a fast and dynamic environment\u2022 Adaptable and flexible\u2022 A strong desire to own and solve challenges\u2022 Excellent communication and inter-personal skillsRecent Solutions include these Azure technologies\u2022 App Services and Function Apps\u2022 Kubernetes\u2022 API Management\u2022 FrontDoor\u2022 Cosmos Db\u2022 Data Explorer\u2022 Event Grid\u2022 Blob Storage\u2022 Application InsightsMust Have Tech SkillsIt is expected that the successful candidate will possess the following skills:\u2022 Proven strong design and coding skills with a reputation for delivering quality results quickly\u2022 Knowledge of Developing using Microservice/Service Oriented Architectures\u2022 REST API Design Principles\u2022 Experience developing in multiple languages. We primarily use C# .NET.\u2022 An understanding of database designAny of the following skills will be an advantage\u2022 Cloud development, especially AWS or Microsoft Azure\u2022 Kubernetes\u2022 Web development using Angular or equivalent\u2022 CI/CD and infrastructure as code\u2022 Serverless development using AWS Lambda or ASP.NET Core API AppsNice to Have Skills:Any of the following skills will be an advantage\u2022 Cloud development, especially AWS or Microsoft Azure\u2022 Kubernetes\u2022 Web development using Angular or equivalent\u2022 CI/CD and infrastructure as code\u2022 Serverless development using AWS Lambda or ASP.NET Core API Apps RegardsSiva Gangadar reddyUS IT RecruiterCentraprise Corp33 Wood Avenue South, Suite 600, Iselin NJ 08830 Desk: 469-639-0369Email: edula.gangadar@centraprise.com We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "113_Need Azure Cloud DevOps Engineer - Local To Illinois": "ayush,\nScalable Systems\nayush.yadav@scalable-systems.com\nReply to: ayush.yadav@scalable-systems.com\nAzure Cloud DevOps Engineer (Azure Focus)Location: Chicago, IL (Important Keyword for location searches)About the RoleWe are seeking a highly motivated Cloud and DevOps Engineer to join our team and play a key role in our cloud migration and automation initiatives. In this role, you will be responsible for architecting, implementing, and supporting enterprise-wide solutions on Microsoft Azure. You will also be responsible for managing and maintaining SQM's hardware and software infrastructure, collaborating with internal and external teams, and ensuring best practices are followed throughout the development lifecycle.Key Responsibilities (Important Keywords for applicant tracking systems (ATS))Azure Cloud Migration (Important Keyword):Lead the planning, design, and execution of cloud migration projects to Azure.Assess existing on-premises infrastructure, applications, and data for cloud readiness.Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed.Execute migration tasks, including data transfer, application reconfiguration, and validation.Infrastructure as Code (IaC) with Terraform (Important Keywords):Design and implement infrastructure using Terraform for repeatable and consistent deployment.Develop and maintain Terraform scripts and modules for provisioning Azure resources.Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines (Important Keywords).CI/CD Management (Important Keywords):Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab.Automate build, test, and deployment processes to enhance efficiency and reliability.Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.Enterprise Tools (Important Keyword):Set up and manage enterprise testing tools (Important Keyword) like Selenium, JUnit, TestNG, and LoadRunner.Integrate testing tools with CI/CD pipelines to enable automated testing.Develop and enforce testing best practices and standards to ensure high-quality deliverables.Provide support activities for queries, issues, access, installation, and configuration of supported tools.Collaboration and Documentation (Important Keywords):Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives.Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows.Provide training and support to team members on cloud and DevOps best practices.QualificationsBachelor's degree in Computer Science, Information Technology, or a related field (Important Keyword).Proven experience in Azure cloud migration projects (Important Keyword).Strong expertise in Terraform and infrastructure as code (IaC) principles (Important Keywords).Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab (Important Keywords).Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner (Important Keywords).Solid understanding of cloud computing concepts, networking, and security in Azure (Important Keywords).Excellent problem-solving skills and attention to detail (Important Keywords).Strong communication and collaboration abilities (Important Keywords).Preferred QualificationsAzure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure DevOps Engineer Expert) (Important Keyword).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "114_Snowflake Architect with AWS--Orlando FL  or Burbank CA-ONSITE": "sharon,\nNorthpole\nsharon@npoleinc.com\nReply to: sharon@npoleinc.com\nHi,please let me know if your interested Role: Snowflake Architect with AWS ExperienceLocation: Orlando FL/Burbank, CA--DAY 1 ONSITEContract JD:Looking for Snowflake architect with AWS Experience Regards,SharonSenior US IT Recruiter (O): 512.999.7446Sharon@npoleinc.com | www.npoleinc.com 3600 Brushy Creek Rd, #12, Cedar Park, TX,78613A Certified Minority-Owned Business(MBE)/Women-Owned Business (WBE) Enterprise\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "115_Urgent Need::AWS Architect ( with Certification) :: San Antonio, TX  - Onsite": "Ashish Kumar,\nTHEMESOFT INC\nashish@themesoft.com\nReply to: ashish@themesoft.com\nJob Title: AWS Architect (Enterprise Architect_)Location: San Antonio, TX - Only local TX candidates is preferred for Experience: 14+ with Architect experience is a must haveCertification: Any AWS Certification is mandatoryMandatory SkillsetOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "116_Terraform Enterprise Architect or Lead with GCP and Terraform experience - Austin, TX- onsite": "James Smith,\nYochana Solutions INC\nsmith@yochana.com\nReply to: smith@yochana.com\nJob Title- Terraform Enterprise Architect or Lead with GCP and Terraform experience Location \u2013 Austin, TX- onsite Duration \u2013 1+year Terraform and GCP Certification is Must 12 to 15 years of Minimum experience 5+ Years of experience in GCP 5+ Years of experience in Terraform Enterprise Architect or Terraform Architect Azure/AWS or other cloud providers experience Docker and other virtualization technologies, Scripting language (bash, PowerShell etc) Expert-level proficiency in designing and implementing cloud infrastructure solutions on the google Cloud Platform (GCP) Utilizing Terraform for infrastructure as code (IaC). Extensive hands-on experience with Terraform, including module development, configuration management, and state management, to automate and streamline infrastructure provisioning and management processes. Deep understanding of GCP&#39;s core services and offerings, such as Compute Engine, Cloud Storage, Cloud SQL, VPCs, and IAM, and how to leverage Terraform to provision and manage these resources effectively. Demonstrated ability to utilize Terraform to orchestrate complex, multi-tiered GCP architectures, incorporating best practices for security, scalability, and cost optimization. Skilled in Version 1.2 troubleshooting Terraform configurations, identifying and resolving infrastructure-related issues, and optimizing Terraform code for improved performance and efficiency. Thanks & Regards James SmithTeam Lead smith@yochana.com Yochana Solutions INCWindsor, Ontario- Canada Farmington hills, MI-48335- USAUSA | CANADA I INDIADirect No: 949-201-1313 W: www.yochana.comPlease follow us on :USA | CANADA | INDIA Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program. Note: This is not an unsolicited mail. If you are not interested in receiving our e-mails then please reply with subject line Remove\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "117_Hiring for Aws ,python Developer at Iselin NJ": "Dushyanth,\nSiriinfo\ndushyanth@siriinfo.com\nReply to: dushyanth@siriinfo.com\nHi Hiring for Aws ,python Developer at Iselin NJ Role Description: AWS, Python DeveloperCompetencies: Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years): 4-6Essential Skills: AWS, Python DeveloperKeywords: AWS, Python Developer Thanks & regardsDushyanth | Sr. IT Recruiter| Email: dushyanth@siriinfo.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "118_SAP || Cloud Computing || BTP || Senior Consultant - Raritan, NJ -Onsite": "narasimha,\nyochana\nnarasimha@yochana.com\nReply to: narasimha@yochana.com\nHello, I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider delivering value to leading companies across the U.S I'm currently staffing for SAP || Cloud Computing || BTP || Senior Consultant. Below you will find the job description, if you are qualified and interested please send me your Updated Word Document Resume. My apologies if this position is not an ideal fit, we'll keep you in mind for other suitable positions and referrals would be appreciated. Thank you in advance Title :- SAP || Cloud Computing || BTP || Senior Consultant Customer location: USA - Raritan, NJ -OnsiteDuration:- Contract Rate: 70-75JOB Description:We are seeking an experienced SAP BTP (Business Technology Platform) Administrator to join our team onsite at our client's location. The ideal candidate will have over 7 years of experience in SAP administration, with a strong focus on SAP BTP. The SAP BTP Administrator will be responsible for managing, monitoring, and optimizing the SAP BTP environment to ensure seamless operations and support the client\u2019s business needs. Key Responsibilities: 1. SAP BTP Administration: - Manage and maintain the SAP BTP environment, ensuring high availability and performance. - Perform regular system monitoring, troubleshooting, and performance tuning. - Implement and manage user access controls, security settings, and data protection measures. 2. System Integration and Deployment: - Handle the deployment and integration of new applications and services on the SAP BTP. - Collaborate with development teams to ensure smooth integration and deployment of custom applications. 3. Maintenance and Support: - Provide ongoing support and maintenance for SAP BTP applications and services. - Manage system updates, patches, and upgrades to ensure the platform is up-to-date and secure. - Address and resolve any technical issues promptly to minimize downtime and disruptions. 4. Performance Optimization: - Monitor system performance and implement measures to optimize performance and scalability. - Analyze system logs and metrics to identify and resolve potential issues. 5. Documentation and Reporting: - Maintain detailed documentation of the SAP BTP environment, including configurations, processes, and procedures. - Generate regular reports on system performance, security, and usage. 6. Collaboration and Communication: - Work closely with client IT teams, project managers, and other stakeholders to understand business requirements and provide effective solutions. - Communicate technical information clearly to non-technical stakeholders. Qualifications: - 7+ years of experience in SAP administration, with a focus on SAP BTP. - Strong understanding of SAP BTP services, architecture, and best practices. - Experience with cloud platforms (e.g., AWS, Azure, Google Cloud) and SAP Cloud Platform. - Proficiency in SAP HANA, SAP Fiori, and other related technologies. - Solid knowledge of security protocols, encryption, and data protection measures. - Excellent problem-solving skills and the ability to troubleshoot complex issues. - Strong communication and interpersonal skills. - Ability to work independently and as part of a team. Preferred Qualifications: - SAP BTP certification. - Experience with Agile and DevOps methodologies. - Familiarity with other SAP products and services (e.g., SAP S4HANA, SAP BW4HANA). Work Environment: - This role requires onsite presence at the client location at NJ, USA Thanks and regardsNarasimhaSenior Resource SpecialistYochana IT Solutions Inc Email:- Narasimha@Yochana.comlinkedin.com/in/narasimha-nandu-0385021a6\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "119_Azure Cloud Security Technical lead Hybrid Chicago,Illinois,": "Rahman khan,\nAvance consulting\nrahman.khan@avanceservices.us\nReply to: rahman.khan@avanceservices.us\nJob Description:We are seeking a highly skilled Azure Cloud Security Technical lead with expertise in Cloudsecurity products, authentication, authorization, in Workforce Identity and Access management(IAM). As a key member of our IAM Security Engineering team, you will play a vital role inensuring the secure and compliant implementation of various solutions in the Cloud focused onIdentity and access Management domain.Requirements1. In-depth knowledge and experience on Azure and AWS Security2. Expertise in different Azure Services, Subscriptions, management groupsNTAC:3NS-203. Design, implement Azure Security solutions to ensure secure and efficientauthentication and authorization processes aligned with industry best practices4. Drive the onboarding of applications, Application registration, enterprise applicationsetup, and role-based access management (RBAC).5. Experience in architecting custom solutions using Java Frameworks on the Azure is amust6. Lead the implementation of Multi-Factor Authentication (MFA) and Single Sign-On (SSO)for enhanced security.7. Proven experience on Azure security such as RBAC, Permissions, actions, identities,Roles, privileged access management8. In-depth knowledge of Azure AD, Azure AD B2B, related authentication/authorizationcomponents and security protocols which including SAML, OAuth, and OpenID9. Strong scripting and automation skills (PowerShell, Azure CLI)10. Experience in architecting custom solutions using Java Frameworks on the Azure is amust11. Expertise in configuring and troubleshooting authentication protocols, including OAuth,OpenID Connect, and SAML for secure authentication and authorization12. Good understanding of Cloud Infrastructure Entitlement Management solutions /Microsoft Entra Permissions Management13. Configure and manage conditional access policies to control access based on specificconditions, locations, and device compliance14. Collaborate with cross-functional teams to support and troubleshoot IAM-related issues,ensuring solutions are secure, compliant, and scalable.15. Understand and implement security best practices for Azure products, services, andsolutions.16. Hands on experience related to DevSecOps, IaC, CI/CD pipeline, automation, andvulnerability scanning tools, Terraform, Powershell, bash script, Azure CLI17. Experience as Full stack application development on technologies like Java, React,JavaScript, SQL and Oracle databases18. Utilize Azure Sentinel for monitoring, creating alerts, and developing automation scriptsfor incident response.19. Provide production support, responding to and resolving security incidents in a timelymanner.20. Establish and maintain identity governance frameworks, including privileged identitymanagement (PIM) for elevated access21. Stay informed of Azure updates, security threats, and industry best practices toenhance our security posture.22. Collaborate with DevOps and development teams, demonstrating a basic understandingof tools and requirements.NTAC:3NS-20Qualifications:1. Bachelors degree in computer science or a related discipline and experience ininformation security, or an equivalent combination of education and work experience.2. Deep knowledge of application or infrastructure systems architecture, usually havingexperience with multiple system technologies.3. Excellent consultative and communication skills, and the ability to work effectively withclient, partner, and IT management and staff.4. Five years of experience in the Information Security role. Three years of experience withcloud and/or technologies5. Cloud security certification preferred6. Strong collaboration skills and a analytical ability7. Certifications on Azure, AWS security will be preferred8. Excellent understanding of cloud security principles9. Ability to work in a dynamic environment and adapt to evolving security challenges.10. Excellent communication and collaboration skills for working with cross-functionalteams.11. Commitment to maintaining a secure, compliant, and scalable IAM solution.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "120_Hiring for Cloud Developer - Philadelphia, PA - Day 1 Onsite and Hybrid  - AVEVA - Contract to Hire": "Gangadar Reddy,\nCentraprise\nedula.gangadar@centraprise.com\nReply to: edula.gangadar@centraprise.com\nRole: Cloud DeveloperLocation: Philadelphia, PAHire type: ContractJob description:Contract to HirePlease ensure we have profiles not more than 3 pages, resume should focus on associates' contributions in business growth of client and technology used for the same rather than just highlighting technologies and year of experience.JD:Principal Accountabilities\u2022 To design and develop cutting edge, highly available and scalable features\u2022 To maintain and grow your knowledge in an environment of rapidly changing technology\u2022 To promote cloud technologies and practices throughout AVEVA\u2022 To demonstrate and promote secure practices\u2022 To work as part of an agile development teamBehaviours:\u2022 Thirst for learning and the confidence to put the learning into practice straight away\u2022 Enthusiastic and positive attitude\u2022 Be both highly motivated and motivating\u2022 Enjoy working in a fast and dynamic environment\u2022 Adaptable and flexible\u2022 A strong desire to own and solve challenges\u2022 Excellent communication and inter-personal skillsRecent Solutions include these Azure technologies\u2022 App Services and Function Apps\u2022 Kubernetes\u2022 API Management\u2022 FrontDoor\u2022 Cosmos Db\u2022 Data Explorer\u2022 Event Grid\u2022 Blob Storage\u2022 Application InsightsMust Have Tech SkillsIt is expected that the successful candidate will possess the following skills:\u2022 Proven strong design and coding skills with a reputation for delivering quality results quickly\u2022 Knowledge of Developing using Microservice/Service Oriented Architectures\u2022 REST API Design Principles\u2022 Experience developing in multiple languages. We primarily use C# .NET.\u2022 An understanding of database designAny of the following skills will be an advantage\u2022 Cloud development, especially AWS or Microsoft Azure\u2022 Kubernetes\u2022 Web development using Angular or equivalent\u2022 CI/CD and infrastructure as code\u2022 Serverless development using AWS Lambda or ASP.NET Core API AppsNice to Have Skills:Any of the following skills will be an advantage\u2022 Cloud development, especially AWS or Microsoft Azure\u2022 Kubernetes\u2022 Web development using Angular or equivalent\u2022 CI/CD and infrastructure as code\u2022 Serverless development using AWS Lambda or ASP.NET Core API Apps RegardsSiva Gangadar reddyUS IT RecruiterCentraprise Corp33 Wood Avenue South, Suite 600, Iselin NJ 08830 Desk: 469-639-0369Email: edula.gangadar@centraprise.com We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "121_Azure Cloud Security Technical lead": "Naresh,\nAvance Consulting\nmaragoni.naresh@avanceservices.com\nReply to: maragoni.naresh@avanceservices.com\nRole:Azure Cloud Security Technical leadLoaction:Chicago,Illinois,United StatesHybridSkills:Azure Cloud Security, DevSecOps, IaC, CI/CD, Technical leadReach Me:maragoni.naresh@avanceservices.comJob Description:We are seeking a highly skilled Azure Cloud Security Technical lead with expertise in Cloudsecurity products, authentication, authorization, in Workforce Identity and Access management(IAM). As a key member of our IAM Security Engineering team, you will play a vital role inensuring the secure and compliant implementation of various solutions in the Cloud focused onIdentity and access Management domain.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "122_Job opening for AWS Cloud Engineer": "Nandhini,\nDiamondpick\nnandhini.a@diamondpick.com\nReply to: nandhini.a@diamondpick.com\nHi Professionals,My name is Nandhini and I am a Staffing Specialist at Diamond pick Inc. I am reaching out to you on an exciting job opportunity with one of our clients.Job Title: AWS Cloud EngineerWork Location :Atlanta, GA (Day one onsite required)Mandatory SkillsAWS Lambda, DynamoDB, CloudWatch, S3 Bucket, APIGateway, CloudFront (required)Job Description10 years of overall Experience in software developmentExperience using various design patterns preferably MVC Singleton Facade and Factory Proficient in OOD and Implementation Design Patterns2 3 years of experience with Cloud Technologies such as OpenShift Google Cloud Azure AWS requiredExperience in other Backend programming languages like Python Node js Golang is a plus2 years experience with Angular JSExpertise in other Frontend Frameworks like React Vue js is a plusMust have experience working in GIT Hibernate JBOSS Apache Tomcat Oracle UML JUnit Mockito Wire Mock and REST APIsSolid understanding of web mobile application architecture and security OAuth JWT Spring SecurityExtensive experience implementing APIs REST via microservicesHands on UI development experience utilizing Responsive DesignProficient in Bootstrap or Foundation frameworks2 years\u2019 experience with unit testing frameworks e g Jasmine ProtractorProficient in OOD and Implementation Design Patterns MVVM etcMust have diverse experience utilizing Java tools in business Web and client server environments including Java Platform J2EE JDBC technologies and Apigee gateway platformExperience or exposure to Database Design and ImplementationKnowledge of Multitier Architecture Rational Visio modeling. RESPONSIBILITIESLeading the technical scrum team and implementing hands on the front ends associated with the technical designs for product project teamsLeading technical efforts in the scrum team including in the creation of quality in our deliveryEnsuring the team develops with quality measuring via clean pipelines and 90 coverageMentoring and elevating your teammates to be stronger engineers by staying ahead with technology evolution and educating the teamTransforming business requirements into application architecturesDetermining feasibility scalability of front-end solutions interacting with business and product owners in order to define technical solutions for customer problemsProduction issue triage management and prevention as neededUI Design reviews for feasibility tech design and impact analysisDevelop sustainable accessibility compliant solutionsLong term technical debt resolutions debt prevention code reviewsAnalysis and implementation of Performance Stability Reliability Architecture initiativesResearch Development of POCs innovative new ideas for customer interactions with DeltaAssisting in defining alternate solutions for the business problems and providing estimates for potential new workstreamsCreating a learning culture by establishing sharing and enforcing\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "123_Azure Cloud Security Technical lead - Chicago,IL": "Tarra Pavan,\nAvance Services\ntarra.p@avanceservices.com\nReply to: tarra.p@avanceservices.com\nHi AllPlease find the below requirement and share suitable profile Azure Cloud Security Technical leadChicago,IL (Hybrid)ContractJD:Qualifications:1. Bachelors degree in computer science or a related discipline and experience ininformation security, or an equivalent combination of education and work experience.2. Deep knowledge of application or infrastructure systems architecture, usually havingexperience with multiple system technologies.3. Excellent consultative and communication skills, and the ability to work effectively withclient, partner, and IT management and staff.4. Five years of experience in the Information Security role. Three years of experience withcloud and/or technologies5. Cloud security certification preferred6. Strong collaboration skills and a analytical ability7. Certifications on Azure, AWS security will be preferred8. Excellent understanding of cloud security principles9. Ability to work in a dynamic environment and adapt to evolving security challenges.10. Excellent communication and collaboration skills for working with cross-functionalteams.11. Commitment to maintaining a secure, compliant, and scalable IAM solution --Best Regards,Tarra PavanTechnical IT Recruiter+1 732 226 8073 Ext : 6431tarra.p@avanceservices.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "124_AWS Solutions Architect || Long Island City, NY (Onsite) || Contract || USC GC Only": "Raveena Mourya,\nDMS Visions Inc\nraveena@dmsvisions.com\nReply to: raveena@dmsvisions.com\nHi, Hope you are doing well !!I have an urgent position. Kindly go through the Job description and let me know if this would be of interest to you.Job Title: AWS Solutions ArchitectLocation: Long Island City, NY (Onsite) Duration: 6+ Months Contract JOB DESCRIPTIONResponsibilities:As a key member of the AWS Cloud Group, you will be responsible for:\u00b7 Innovative Software Development: Designing, developing, and maintaining complex software systems.\u00b7 Comprehensive Analysis: Performing requirements analysis, technical analysis, software design, and peer reviews.\u00b7 Thorough Documentation: Providing detailed documentation for implementation, deployment, and QA teams.\u00b7 Business Integration: Learning our business model and gaining a deep understanding of existing CorVel applications.Skills Needed:\u00b7 Extensive Industry Experience: Over 10 years in the computer industry.\u00b7 Cloud Expertise: At least 7 years specializing in cloud-based architecture with proficiency in AWS and Google Cloud.\u00b7 AWS Leadership: Serve as the subject matter expert for AWS and system architecture, holding an AWS Solutions Architect Professional certification.\u00b7 IaaS Specialist: More than 7 years of extensive experience with Cloud-based IaaS, with a strong emphasis on AWS.\u00b7 Automation Mastery: At least 7 years of experience with automated configuration management tools such as Puppet, Ansible, Terraform, and CloudFormation.\u00b7 Version Control Proficiency: At least 7 years of experience with distributed revision control tools like Git.\u00b7 CI/CD Enthusiast: Recent experience with continuous integration and continuous deployment tools, particularly Jenkins.\u00b7 Monitoring Expertise: Recent experience with monitoring tools for both traditional and dynamic environments, including Nagios, CloudWatch, Prometheus, and Grafana.\u00b7 Database Management: Experience in maintaining relational database systems, specifically PostgreSQL.\u00b7 SOA Architecture: Experience with Service-Oriented Architecture, including Windows Communication Foundation (WCF) and Web API.\u00b7 Web Technologies Veteran: Over 7 years of experience with web technologies, including ASP.Net MVC, HTML, CSS, JavaScript, and JQuery.\u00b7 SQL Proficiency: Over 7 years of experience with SQL, Microsoft T-SQL, and Database Design.\u00b7 .Net Expertise: Experience with .Net C# 4.5 and above.\u00b7 Modern JavaScript Frameworks: Experience with frameworks such as AngularJS, React.js, or Vue.js.\u00b7 Enterprise Solutions Architect: Proven track record of designing and building scalable and extensible enterprise solutions.\u00b7 Educational Foundation: Bachelor's degree or higher in Computer Science or relevant experience.\u00b7 Team Player and Independent Worker: Comfortable working in both team settings and independently.\u00b7 Strong Interpersonal Skills: Excellent communication, analytical, and interpersonal skills. Thanks & Regards,Raveena MouryaUS IT Recruiter, DMS Visions Inc972-325-9476 | dmsvisions.com/ | raveena@dmsvisions.com4645 Avon Lane, Suite 210, Frisco, TX 75033linkedin.com/in/raveena-mourya-766314250\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "125_Lead IaC Terraform (Azure)- Dallas, TX": "kamini,\nMphasis/ VY\nkamini@vysystems.com\nReply to: kamini@vysystems.com\nLead IaC Terraform (Azure)Dallas, TXAs a Lead Terraform Engineer specializing in Terraform, you will play a pivotal role in our IT infrastructure and operations team. You will be responsible for designing, implementing, and maintaining infrastructure as code using Terraform, while collaborating closely with development and operations teams to streamline processes and enhance system reliability.Roles and ResponsibilitiesInfrastructure as Code (IaC): Develop, maintain, and optimize infrastructure code usingTerraform to provision and manage cloud resources.Automation: Design and implement automation scripts and processes for provisioning,configuration, and deployment of infrastructure and applications.Collaboration: Work closely with software development and operations teams tounderstand their requirements and ensure seamless integration of infrastructurechanges.Continuous Integration/Continuous Deployment (CI/CD): Implement and maintainCI/CD pipelines to automate software deployments and releases.Monitoring and Logging: Set up and manage monitoring, logging, and alerting systemsto proactively identify and address issues.Security: Ensure the security of infrastructure by implementing best practices andstaying updated on security vulnerabilities and threats.Performance Optimization: Identify and resolve performance bottlenecks and ensurehigh availability of systems.Documentation: Maintain detailed documentation of infrastructure configurations andprocesses.QualificationBachelor\u2019s degree in Computer Science, Information Technology, or related field (orequivalent work experience).7+ years of hands-on experience in DevOps, with a strong emphasis on Terraform.Proven experience with cloud platforms (e.g., AWS, Azure). Azure will bepreferred.Proficiency in scripting and automation using languages such as Bash, Python, orPowerShell.Familiarity with containerization technologies like Docker and orchestration withKubernetes is a plus.Knowledge of infrastructure monitoring and logging tools (e.g., Prometheus, ELK Stack).Strong problem-solving skills and the ability to work effectively in a collaborative teamenvironment.Excellent communication skills to interact with cross-functional teams and stakeholders.Relevant certifications are a plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "126_Need for DevOps Engineer": "Sunita Rani,\nScalable\nsunita.rani@scalable-systems.com\nReply to: sunita.rani@scalable-systems.com\nDevOps EngineerChicago, IL This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices.Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "127_Azure Cloud Migration Engineer  -- Chicago, IL": "Maroju,\nSoftcom\nmaroju@softcomsystems.com\nReply to: maroju@softcomsystems.com\nRole: Azure Cloud Migration EngineerLocation: Chicago, ILJob Details: This role will be responsible for architecting, implementing and supporting enterprise-wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:\u2022 Azure Cloud Migration:o Lead the planning, design, and execution of cloud migration projects to Azure.o Assess existing on-premises infrastructure, applications, and data for cloud readiness.o Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed.o Execute migration tasks, including data transfer, application reconfiguration, and validation.\u2022 Infrastructure as Code (IaC) with Terraform:o Design and implement infrastructure using Terraform for repeatable and consistent deployment.o Develop and maintain Terraform scripts and modules for provisioning Azure resources.o Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.\u2022 CI/CD Management:o Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab.o Automate build, test, and deployment processes to enhance efficiency and reliability.o Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.\u2022 Enterprise Tools:o Set up and manage enterprise testing tools.o Integrate testing tools with CI/CD pipelines to enable automated testing.o Develop and enforce testing best practices and standards to ensure high-quality deliverables.o Provide support activities for the queries, issues, access, installation and configuration for supported toolso In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.\u2022 Collaboration and Documentation:o Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives.o Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows.o Provide training and support to team members on cloud and DevOps best practices.Qualifications:\u2022 Bachelor\u2019s degree in computer science, Information Technology, or a related field.\u2022 Proven experience in Azure cloud migration projects.\u2022 Strong expertise in Terraform and infrastructure as code (IaC) principles.\u2022 Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.\u2022 Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.\u2022 Solid understanding of cloud computing concepts, networking, and security in Azure.\u2022 Excellent problem-solving skills and attention to detail.\u2022 Strong communication and collaboration abilities.Preferred Qualifications:\u2022 Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure Administrator).\u2022 Experience with other cloud platforms (AWS, Google Cloud).\u2022 Familiarity with containerization and orchestration tools like Docker and KubernetesPreferred tool experience:\u2022 Microsoft Azure DevOps, JIRA, BitBuket, Bamboo, ALM, Load Runner/Performance Center, JMeter, GitHub, Jenkins, Microfocus UFT/LeanFT, Selenium, Eclipse, ServiceNow, Perfecto, SeaLights, Gremlin, ToxiProxy, ChaosMonkey, Selenium Grid.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "128_Looking for Data Architect - Snowflake, AWS at NAPERVILLE, IL OR Piscataway NJ_ Hybrid": "Teja,\ninsoursysinc\ntejaswi_athili@insoursysinc.com\nReply to: tejaswi_athili@insoursysinc.com\nPosition : Data Architect \u2013 Snowflake, AWS Candidate can work from NAPERVILLE, IL OR\u2013 Piscataway NJHybrid \u2013 2 days a week Need Locals in IL or NJ With 15+ Years Of experieance.H1B visas Skills : AWS AirByte , Snowflake, DBT, AWS S3 , AWS LambdaLooker -Good to have Responsibilities Lead efforts to ensure data capabilities are operating efficiently, accurately, and with high quality.Collaborate with programs & projects to understand data dependencies, anticipate risks, and identify opportunities. Guide data project decisions based on the direction of data domain roadmap.Contribute data domain expertise to feasibility, complexity assessments of potential future initiatives including data domain analysis.Develops an integrated view of data capabilities and processes, using a repeatable approach, cohesive framework, and available industry best practices and techniques.Provides consulting and makes recommendations to tackle data transformation initiatives, help mature existing data capabilities, or enable new data capabilities.Facilitate the process of defining and publishing data standards, best practices, data capabilities, and data roadmap, synchronized with the overall enterprise future state architecture.Collaborate in creation of conceptual and logical architecture and models following the enterprise guidance to describe a particular domain of data and use these models to inform the physical design of data-related projects.Co-develop data life cycle governance framework including standards, patterns, and controls. QualificationsMinimum eight years of relevant experience as a data architect building large-scale data solutions.Experience in Migration of existing On-Prem Data warehouses to Snowflake.Experience in architecting and large data modernization, data migration, data warehousing \u2013 experience with cloud-based data platforms (like Snowflake)Should have experience in architecture and implementing End to End Modern Data Solutions using AWS, Redshift, S3 Good appreciation and at least one implementation experience on processing substrates in Data Engineering \u2013 such as AWS Glue, ETL Tools, ELT techniquesExperience with defining and operationalizing data strategy, data governance, data lineage and quality standards.Experience designing highly scalable ETL processes with complex data transformations, data formats including error handling and monitoring .Extensive knowledge of data engineering, data integration and data management concepts (i.e. APIs, ETL, MDM, CRUD, Pub/Sub, etc.)Experience with data Modeling.Experience with structured and hierarchical datasets (i.e. JSON, XML, etc.)Engineering experience with large scale system integration and analytics projects. RegardsTejaswi Athili Insoursys | Ph: 9724400065 | tejaswi_athili@insoursysinc.comwww.linkedin.com/in/tejaswi-attili-35709a1a6\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "129_Job opening for Node AWS developer": "Pavithran.R,\nDiamondpick\npavithran.r@diamondpick.com\nReply to: pavithran.r@diamondpick.com\nHiHope you are doing wellJob Title: Node AWS developerLocation: Atlanta, GA Job Description:Note: Good to have AWS Certificate Roles and Responsibilities:Develop AWS lambdas using Node.js in theProduction issue triage and prevention as needed, release management support from a technical standpoint, and overall backlog maintenanceResearch and Development of AWS POCs Skills Required Highly skilled in NodeJS, Typescript is requiredHighly skilled in AWS lambdaHighly skilled in Cloudwatch scripting requiredHighly skilled with AWS CDK is requiredExperience in working with GraphQL queriesSkill in docker and linux.Excellent understanding of development practices across the development/QA disciplines is requiredExcellent knowledge of Software Development - Involves knowing the general approach to developing software including best practices, concepts, technology, and methodology obtained through formal training and/or work experience.Test Driven Development\u00b7Secure apps that adhere to best practices for information securityBachelor\u2019s degree in computer science, Engineering, or Information Systems or any equivalent combination of experience, education, and/or training in the computer systems development field. Previous airline experience not required, but helpfulCommunication Skills- The ability to communicate verbally and in writing with all levels of employees and management, capable of successful formal and informal communication, speaks and writes clearly and understandably for the audienceIntegrity and Trust - Involves being widely trusted, being seen as a direct, truthful individual, can present the unvarnished truth in an appropriate and helpful manner, keeps confidences, admits mistakes, and doesn't misrepresent him/herself for personal gainTeamwork - Involves working well in a collaborative setting, supporting work team by volunteering for and completing assignments, acting as a positive team member by contributing to discussions, developing and maintaining both formal and informal relationships enterprise-wide, defines success in terms of the entire team through mentoring and knowledge transfer.Technical Expertise - Involves demonstrating a commitment to increasing knowledge and skills in current technical/functional area, keeping up to date on technical developments, staying informed as to industry practices, and knowing how to apply relevant technical processes. Regards,Pavithran. R |US IT RecruiterEmail id \u2013 pavithran.r@diamondpick.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "130_Looking On-site || Apigee Installation Both (On-prem and Cloud)  || Miramar, FL (Onsite)": "Gaurav kumar,\nQuantum World Technologies Inc.\ngaurav.kumar@quantumworldit.com\nReply to: gaurav.kumar@quantumworldit.com\nGreeting, I hope all is well with youFor the following, Quantum World IT is seeking the best consultant profile.Please respond with your most recent resume if you are considering new opportunities. Looking On-site || Apigee Installation Both (On-prem and Cloud) || Miramar, FL (Onsite)Role name:System AdministratorRole Description:Responsibility of / Expectations from the Role Coordinates with App teams and gather deployment requirements.. Lead the design and implementation of API Gateway solutions both on prem and Cloud . Lead analytical and consulting working sessions to solve technical problems faced by the application teams trying to onboard to APIGEE Gateway. Lead Capacity Planning and Management activities to ensure that the APIGEE platform will be able to handle additional transactions in the future. Help troubleshoot and resolve production issues related to the APIGEE platform API Gateway. Lead root cause analysis session to understand cause of issues in Production and come up with solutions that will prevent them from happening in the future. Ensures adherence to SLAs by resolving tickets. Performs system health checks and reports anomaly.Competencies:Digital : APIGEEExperience (Years):8-10Essential Skills:Apigee Installation Both (On-prem and Cloud) Multi node deployment in AWS and On-prem. Integrating APIGEE with authentication system eg ADS Certificate managementDesirable Skills:Key parameters to monitor APIGEE using Grafana/AppD Component troubleshooting Upgrade/maintenance Apigee capacity expansion Apigee X Deployment on sandbox environmentPlease enter the following information is needed for submission and share your visa and Photo id Position Applied for Candidate Full Name [As per Passport] Contact Number Primary & Secondary Email ID Current Location LinkedIn Passport No. Work Authorization Had ever worked with TCS in Past (Required filed) Are you comfortable for on-site Had you gotten covid vaccination doses Rate expectation Gauarv KumarPhone: +1 805 749 5070Email: Gaurav.kumar@quantumworldit.com Quantum World Technologies Inc. 4281 Katella Ave, Suite #102 Los Alamitos CA 90720 USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "131_AWS architect || San Antonio, TX- Onsite": "Divya,\nresource logistics\ndivya@resource-logistics.com\nReply to: divya@resource-logistics.com\nHello,My name is Divya Pandey. and I am a Staffing Specialist at Resource Logistics. I am reaching out to you on an exciting job opportunity with one of our clients COMPLETE JOB DESCRIPTION IS BELOW FOR YOUR REVIEW:Job Title: AWS architect Location: San Antonio, TX- OnsiteHire Type: Contract Rate: 63 c2c Mandatory Skills: Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology Job OverviewA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members. Responsibilities\u00b7 Own and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions\u00b7 Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design.\u00b7 Provide data architectural solutions/designs to project execution teams for implementation.\u00b7 Provide architectural assessments, strategies, and roadmaps for data management.\u00b7 Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features.\u00b7 Project and solution estimation and team structure definition\u00b7 Develop Proof-of-Concept projects to validate new architectures and solutions.\u00b7 Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production.\u00b7 Liaise with offshore team and clients for resolving technical dependencies, issues, and risks.\u00b7 Mentor and provide architectural guidance to multiple teams building innovative applications.\u00b7 Drive common vision, practices and capabilities across teams.\u00b7 Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions\u00b7 Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs\u00b7 Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change. Thanks & Regards,Divya Pandey, Technical RecruiterEmail: Divya@resource-logistics.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "132_Snowflake Architect + AWS": "Sandeep Yadav,\nSmart It Frame\nsandeep@smartitframe.com\nReply to: sandeep@smartitframe.com\nRole :: AWS + Snowflake ArchitectClient :: LTILocation :: Orlando, FL and Burbank, CA (Hybrdi)AWS + Snowflake Architect is responsible for designing, implementing, and managing cloud-based solutions on Amazon Web Services (AWS) and Snowflake. The ideal candidate will have experience with both AWS and Snowflake, as well as a strong understanding of data warehousing and data engineering principles.Responsibilities:\u2022 Design, implement, and manage cloud-based solutions on AWS and Snowflake\u2022 Work with stakeholders to gather requirements and design solutions that meet their needs\u2022 Develop and execute test plans for new solutions\u2022 Monitor and troubleshoot production systems\u2022 Stay up-to-date on the latest AWS and Snowflake features and services Thanks and RegardsSandeep YadavRecruitment and Operations Email : Sandeep@smartitframe.comSmart IT Frame LLC.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "133_AWS Cloud Technical Architect in San Antonio, TX 78288": "Amit,\nTechStar Group\namit.kumar@techstargroup.com\nReply to: amit.kumar@techstargroup.com\nRole: AWS Cloud Technical ArchitectLocation: San Antonio, TX (onsite)Job Type: Contract Job Description:Min Skills: Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL TechnologyA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.ResponsibilitiesOwn and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. Provide data architectural solutions/designs to project execution teams for implementation. Provide architectural assessments, strategies, and roadmaps for data management. Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. Project and solution estimation and team structure definition Develop Proof-of-Concept projects to validate new architectures and solutions. Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. Mentor and provide architectural guidance to multiple teams building innovative applications. Drive common vision, practices and capabilities across teams. Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "134_Cloud Network Engineer with GCP": "Ashwani Raghuvanshi,\nTek Inspirations\nashwani.raghuvanshi@tekinspirations.com\nReply to: ashwani.raghuvanshi@tekinspirations.com\nJob Description -Role: Cloud Network Engineer with GCPPrevious candidates feedback : I need applicants that have current GCP and financial/banking experience, and good communication skills. Interview process : 2-4 Rounds. First Interview will be with PV and the client\u2019s interviews will be videocalls. Location : New York, NYNeed consultants close to Burlington County, NY, and NYC.About the jobRole and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform. Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform. Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture. Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code. Automate the existing code components and integrate technologies to eliminate manual deployment steps. Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases. The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience. Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages. Experience with automating and integrating Serverless cloud provided PaaS solutions. Ability to troubleshoot code and logic errors for cloud-based network services. Understanding of deployment platforms and databases via CI/CD pipeline. Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms. Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus. Proficiency in cloud automation using cloud native CLI/API. Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP. Must have working experience with Cloud Native Networking technologies. Must have experience using PowerShell to configure Cloud Networking components. Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS. Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes. A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team. Able to work under tight timeline and deliver on complex problems. Education:Bachelor's degree in computer science, engineering or a related field, or equivalent work experienceCertifications:CISSP, CCSP, Microsoft MCSE Azure - 400 or 500Desired Skills and ExperienceNETWORK ENGINEER\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "135_Urgent Job Role - Cloud Application Architect - Hybrid": "garima solanki,\nInceptra Solutions\ngarima.solanki@inceptrasolutions.com\nReply to: garima.solanki@inceptrasolutions.com\nRole: Cloud Application ArchitectLocation: Los Angeles, CA (Hybrid ) Duration: Contract\u200b Job Description:\u00b7 The Cloud Application Architect is responsible for leading the creation of a technology framework and providing technical leadership in support of modernization and vulnerability reduction initiatives in cloud computing and automation, with a focus on the design of systems and services that run on cloud platforms natively or as hybrid solutions.\u00b7 Additionally, the Cloud Application Architect will be responsible for ensuring that applications are designed to support the performance, security, monitoring, high availability, and disaster recovery requirements.\u00b7 The ideal candidate will have experience in designing large and complex IT operations in large organizations.\u00b7 The position requires strong leadership, communication, collaboration, and team-building skills, and must be able to collaborate effectively with multiple groups and stakeholders on multiple projects as a technical SME (subject matter expert).\u00b7 Cloud Architecture and Design and Development (50%)\u00b7 Demonstrate knowledge of AWS cloud architecture and implementation features (serverless, authentication, authorization, API design)\u00b7 Demonstrate knowledge of Azure DevOps, GitHub or experience in any other DevOps tool\u00b7 Demonstrate knowledge of CI/CD Pipelines, including pipeline automation for tools such as Checkmarx, SonarQube, NPM audit\u00b7 Demonstrate knowledge of infrastructure-as-code with tools such Terraform, AWS CloudFormation, etc.\u2026\u00b7 Act as a Subject Matter Expert to the organization provide cloud end-to-end reference architectures, including AWS for both native and hybrid cloud solutions.\u00b7 Develop a library of deployable and documented cloud design patterns, based on modernization application portfolio, as a basis for deploying services to the cloud.\u00b7 Demonstrate leadership ability to back decisions with research and the \u201cwhy,\u201d and articulate several options, the pros and cons for each, and communicate solutions with leadership\u00b7 Maintain overall industry knowledge on latest trends, technology, etc.\u00b7 Develop solutions architecture and evaluate architectural alternatives for private, public and hybrid cloud models, including SaaS, PaaS, and other cloud services.\u00b7 Contribute to DevOps development activities and complex development tasks.\u00b7 Define optimal design patterns and solutions for high availability and disaster recovery for applications.\u00b7 Collaborate with cyber-security and data privacy to meet security requirements, and translate requirements to developers, infrastructure, and cloud office teams. Consultation (20%)\u00b7 Drive scope definition, requirements analysis, functional and technical design, application build, product configuration, unit testing, and production deployment.\u00b7 Ensure delivered solutions meet/perform to technical and functional/non-functional requirements.\u00b7 Provide technical expertise and ownership in the diagnosis and resolution of an issue, including the determination and provision of workaround solution or escalation to service owners.\u00b7 Provide guidance on best practices and recommendation on technical governance, expertise related to cloud architectures, deployment, and operations. Thought Leadership (20%)\u00b7 Provide thought leadership in industry and to fellow team members, business stakeholders and program management.\u00b7 Communicate technology framework and solutions to ARB/CARB (cloud & architectural review boards).\u00b7 Advocate and define cloud architecture vision from a strategic perspective, including internal and external platforms, tools, budget, and systems. Mentoring (10%)\u00b7 Act as a mentor to team members and technical staff.\u00b7 Lead the definition and development of cloud reference architecture and management systems.\u00b7 Conduct product and governance work reviews with team members.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "136_Urgent Requirement: Position:Lead Azure with Dot Net Experience(Hybrid) (Locals to Richmond, VA only or within 50 miles from Richmond)": "ATI Team,\nAspire\nrecruiterpro@aspiretechus.com\nReply to: recruiterpro@aspiretechus.com\nPOSITION: Lead Azure with Dot Net Experience(Hybrid) Min 10yrs Exp(Locals to Richmond, VA only or within 50 miles from Richmond)DURATION:12 MonthsADDRESS:VA 23219Lead Azure with Dot Net Experience.looking for an Azure Developer with Dot Net Experience who will play a pivotal role in designing, implementing, and optimizing data solutions within the Microsoft Azure cloud ecosystem.Able to design and plan cloud infrastructure, including computing resources, storage, networking, and security components. Collaborate with other teams to design cloud-native applications and migrate existing applications to the cloud and Implement security measures to safeguard data and applications in the cloud.Minimum QualificationHands-on experience in design, development and testing .NET solutions (.NET Core, MVC.NET, ASP.NET, C#, Web Services (WCF, Web API), JQuery, T-SQL, PL/SQL, etc.) within both on-premises and cloud environments.Hands-on experience with Angular / Blazor or any similar front-end technologies.Hands-on experience with project online API'sExperience with Microservices architecture, Azure Architecture, Domain driven architecture.Experience with Azure native application development, Azure App Services, Azure Kubernetes Service,Azure Container instances, Kubernetes & Containers (Docker), Azure Integration Services (Logic Apps, API Management, Service Bus & Event Grid) ,Azure SQL Database, Azure Web Jobs, SQL Server IaaS, Azure Monitoring and Application InsightsKnowledge and experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM TemplatesCollaborate with architects and other senior developers to define software architecture, making well-informed design decisions that align with VDOT ITD business needsExperience working in agile methodology.Preferred QualificationStrong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtual networks, VPN, and Express RouteStrong scripting skills in PythonExperience working with Azure Data factory, Azure Data Lake.Experience developing power BI reportsRequired/Desired SkillsExperience with ASP.NET, MVC, C#,WEB APIExperience with Angular, Blazor or similar front end technologiesExperience with .NET CoreExperience with Microservices architecture, Azure architecture, Domain driven architectureExperience with Docker and azure cloud services such as AKS, Azure functions, Azure Container Service, Azure App services, Azure API ManagementExperience with Azure cloud databasesExperience with Azure Monitoring, Application InisghtsExperience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM TemplatesAzure certified developerExperience with Infrastructure As CodeExperience with Project Online Rest API'sStrong scripting skills in PythonStrong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtual networks, VPN, and Express RouteExperience working with Azure Data factory, Azure Data Lake.Experience developing power BI reportsThanks & RegardsATI TeamRecruiterpro@aspiretechus.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "137_Cloud Application Architect": "payal rana,\ninceptrasolutions.com\npayal.rana@inceptrasolutions.com\nReply to: payal.rana@inceptrasolutions.com\nHi, Trying to reach you . Please go through the JD::- Role: Cloud Application ArchitectYears of Experience: 15+ yearsLocation: Los Angeles, CA (Hybrid 3 days onsite and 2 days remote)Client: Brillio. Job Description:The Cloud Application Architect is responsible for leading the creation of a technology framework and providing technical leadership in support of modernization and vulnerability reduction initiatives in cloud computing and automation, with a focus on the design of systems and services that run on cloud platforms natively or as hybrid solutions.Additionally, the Cloud Application Architect will be responsible for ensuring that applications are designed to support the performance, security, monitoring, high availability, and disaster recovery requirements.The ideal candidate will have experience in designing large and complex IT operations in large organizations.The position requires strong leadership, communication, collaboration, and team-building skills, and must be able to collaborate effectively with multiple groups and stakeholders on multiple projects as a technical SME (subject matter expert).Cloud Architecture and Design and Development (50%)Demonstrate knowledge of AWS cloud architecture and implementation features (serverless, authentication, authorization, API design)Demonstrate knowledge of Azure DevOps, GitHub or experience in any other DevOps toolDemonstrate knowledge of CI/CD Pipelines, including pipeline automation for tools such as Checkmarx, SonarQube, NPM auditDemonstrate knowledge of infrastructure-as-code with tools such Terraform, AWS CloudFormation, etc.\u2026Act as a Subject Matter Expert to the organization provide cloud end-to-end reference architectures, including AWS for both native and hybrid cloud solutions.Develop a library of deployable and documented cloud design patterns, based on modernization application portfolio, as a basis for deploying services to the cloud.Demonstrate leadership ability to back decisions with research and the \u201cwhy,\u201d and articulate several options, the pros and cons for each, and communicate solutions with leadershipMaintain overall industry knowledge on latest trends, technology, etc.Develop solutions architecture and evaluate architectural alternatives for private, public and hybrid cloud models, including SaaS, PaaS, and other cloud services.Contribute to DevOps development activities and complex development tasks.Define optimal design patterns and solutions for high availability and disaster recovery for applications.Collaborate with cyber-security and data privacy to meet security requirements, and translate requirements to developers, infrastructure, and cloud office teams. Consultation (20%)Drive scope definition, requirements analysis, functional and technical design, application build, product configuration, unit testing, and production deployment.Ensure delivered solutions meet/perform to technical and functional/non-functional requirements.Provide technical expertise and ownership in the diagnosis and resolution of an issue, including the determination and provision of workaround solution or escalation to service owners.Provide guidance on best practices and recommendation on technical governance, expertise related to cloud architectures, deployment, and operations. Thought Leadership (20%)Provide thought leadership in industry and to fellow team members, business stakeholders and program management.Communicate technology framework and solutions to ARB/CARB (cloud & architectural review boards).Advocate and define cloud architecture vision from a strategic perspective, including internal and external platforms, tools, budget, and systems. Mentoring (10%)Act as a mentor to team members and technical staff.Lead the definition and development of cloud reference architecture and management systems.Conduct product and governance work reviews with team members.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "138_Urgent Requirement: Position:Certified Azure Cloud Engineer (HYBRID) (Locals to DMV Area)": "ATI Team,\nAspire\nrecruiterpro@aspiretechus.com\nReply to: recruiterpro@aspiretechus.com\nPOSITION: Certified Azure Cloud Engineer (HYBRID) Min 3yrs Exp(Locals to DMV Area)DURATION:2-18 MonthsADDRESS:Washington DC 20004A279: Certified Azure Cloud EngineerHybrid position - Must be willing and able to report on-site to DC offices 1x every 2 week period.Duties and Responsibilities Architect, design, deploy, and manage Azure Gov IaaS, SaaS & PaaS services, and solutionsHands on role to implement and maintain Azure Gov Cloud tenant, including workloads migration (VMware) from on-perm to cloud.Manage overarching product/platform architectures and ensure design and development of all projects follow the architectural vision.Conduct analysis, design and implementation of infrastructure and cloud data center solutions including computing, networking, routing and security.Design and support solutions in cloud and virtualized computing environments with and understanding of hybrid cloud architectures.Successfully communicate, evangelize and implement the architectural vision across teams.Provide technical leadership and work with team direction on projects with a high level of complexity.Create a well-informed cloud strategy and manage the adaptation process.Cost management of Azure Gov services and implementation of a tagging strategy for Cloud Resources and Assets with an associated taxonomy in support of cost rebilling, reporting, and compliance.Proficient in core Microsoft technologies including Windows, Windows server administration, Active Directory, Identity and Access Management, DNS, SQLDesign, build, upgrade, and decommission various Windows and Linux systems, features, and services including but not limited to: a. Active Directory (Azure AD) b. Azure Storage (Blob, Files, Disks) c. App Services d. BCDR (Azure Backup, Azure Site Recovery, Recovery Vaults) e. Networking (vNets, Subnets, Express Route) f. Virtual MachinesLiaise effectively with 3rd party suppliers and support providers to ensure issues and solutions are provided within expected service levels.Create supporting documentation for various audiences with the ability to multi-task and be creative and innovative in a dynamic, fast paced team environment.Knowledge of RESTful API, Web Services, XML, JSONEducation:1. Bachelor's or Master's Degree in Information Technology or Computer Science2. The equivalent combination of education and successful work experience (10 years) and3. Microsoft AZURE Certification in specialized areas is requiredQualifications:1. Minimum (3) years of experience in design and maintenance of public/Gov cloud solutions with a focus on Microsoft Azure (Gov preferred) IaaS, PaaS service like Database services, Kubernetes etc.2. Understand the terminologies and how to navigate IP subnetting, Network Security Groups, routing, Azure Firewall, load balancing, DNS, and other networking concepts and protocols3. Excellent technical architecture skills, enabling the creation of future-proof, complex global solutions and Strong Experience across Applications Migration (On-Perm VMware) to Azure (Gov Cloud Preferred) .4. Minimum (3) years of experience providing IT administration and/or engineering services managing Windows and Linux Servers and Services.5. Expert knowledge of Azure CLI PowerShell, ARM Templates, Azure Networking, Azure Virtual Networks, and Site-to-Site-VPN.6. Intermediate knowledge of Azure DevOps configuration and Management, Azure Security Center, and Azure Identity Management, including Azure AD, Azure AD B2C, SSO.7. Experience with cloud monitoring including Azure Log Analytics, Azure Monitor, and Application insights.8. Experience implementing DevOPS practices including Infrastructure-As-Code (IAC), Continuous Integration / Continuous Deployment (CI / CD), and automated testing.9. Knowledge of network technologies as they relate to AWS.10. Knowledge of security policies, network security, data security and security event management to adopt cloud security services11. Experience in Terraform and/or Ansible and/or Azure Gov cloud / AWS CloudFormation IaC tools.12. Script Experience - PowerShell, Python, Bash, or similar. Pyspark experience preferred.13. Experience with relational databases such as MS SQLTechnical Skills:1. Ability to exercise independent judgment and take action on it.2. Excellent analytical and creative problem-solving skills.3. Excellent listening, interpersonal, written, and oral communication skills.4. Logical and efficient, with keen attention to detail.5. Highly self-motivated and directed.6. Ability to effectively prioritize and execute tasks while under pressure.7. Strong customer service orientation.8. Experience working in a team-oriented, collaborative environment.Required/Desired SkillsExperience in design and maintenance of public cloud solutions w/ focus on Microsoft Azure IaaS, PaaS service like Database services, Kubernetes, etc.Experience providing IT administration and/or engineering services managing Windows and Linux servers and servicesExcellent technical architecture skills and strong experience across Applications Migration (On-Perm VMware) to Azure (Gov Cloud Preferred)Expert knowledge of Azure CLI PowerShell, ARM Templates, Azure Networking, Azure Virtual Networks, and Site-to-Site-VPN.Intermediate knowledge of Azure DevOps configuration and Management, Azure Security Center, and Azure Identity ManagementExperience with cloud monitoring including Azure Log Analytics, Azure Monitor, and Application insights.Experience implementing DevOps practices including Infrastructure-As-Code (IAC), CI/CD, and automated testingKnowledge of security policies, network security, data security and security event management to adopt cloud security servicesExperience in Terraform and/or Ansible and/or Azure Gov cloud / AWS CloudFormation IaC tools.Script Experience - PowerShell, Python, Bash, or similar. Pyspark experience preferred.Experience with relational databases such as MS SQLBachelor\u2019s or Master\u2019s Degree in Information Technology or Computer ScienceMicrosoft Azure CertificationsThanks & RegardsATI TeamRecruiterpro@aspiretechus.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "139_Cloud Engineer : Dallas TX-Hybrid": "Upama,\nCBS\nupama@cbsllc.us\nReply to: upama@cbsllc.us\nCloud Engineer : Dallas TX-HybridStrong experience in Snowflake and Cloud development Snowflake SnowPro Core certified preferredStrong experience in Data Engineering, should have implemented components of enterprise data platforms/data lakesExperience in DevOps; utilizing CI/CD methodologies, automating infrastructure, code reviewing, script writing, and managing containerization and cloud technologies for system optimization and efficiency\u00b7 Ability to decompose intricate, technical solutions into smaller elements and lead team with clear instruction\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "140_Lead Azure Dot Net Developer": "Deepak Thakur,\nLargeton Inc\ndeepak@largeton.us\nReply to: deepak@largeton.us\nJob Description Role: Lead Azure /Dot Net Developer (740668)Start Date: 07/11/2024End Date: 06/30/2025Location: Richmond, VA 23219Interview Type: Web Cam and In Person InterviewWork Arrangement: Hybrid Client- State of VA, (Department of Transportation)Visa : NO OPT/H1B/CPT Complete Description:VDOT ITD is looking for an Azure Developer with Dot Net Experience who will play a pivotal role in designing, implementing, and optimizing data solutions within the Microsoft Azure cloud ecosystem.Able to design and plan cloud infrastructure, including computing resources, storage, networking, and security components. Collaborate with other teams to design cloud-native applications, migrate existing applications to the cloud, and Implement security measures to safeguard data and applications in the cloud.Minimum Qualification- Hands-on experience in design, development, and testing .NET solutions (.NET Core, MVC.NET, ASP.NET,C#, Web Services (WCF, Web API), JQuery, T-SQL, PL/SQL, etc.) within both on-premises and cloudenvironments.- Hands-on experience with Angular / Blazor or any similar front-end technologies.- Hands-on experience with project online API's- Experience with Microservices architecture, Azure Architecture, Domain driven architecture.- Experience with Azure native application development, Azure App Services, Azure Kubernetes Service, Azure Container instances, Kubernetes & Containers (Docker), Azure Integration Services (Logic Apps, API Management, Service Bus & Event Grid) ,Azure SQL Database, Azure Web Jobs, SQL Server IaaS, Azure Monitoring and Application Insights- Knowledge and experience with configuration management and automation technologies such as Azure DevOPs, PowerShell, Terraform, Chef, and ARM Templates- Collaborate with architects and other senior developers to define software architecture, making well-informed design decisions that align with VDOT ITD business needs- Experience working in agile methodology.Preferred Qualification- Strong knowledge of implementing hybrid connectivity between Azure and on-premises systems using virtualnetworks, VPN, and Express Route- Strong scripting skills in Python- Experience working with Azure Data factory, Azure Data Lake.- Experience developing power BI reports Skill Required / Desired Amount of ExperienceExperience with ASP.NET, MVC, C#,WEB API Required 10 YearsExperience with Angular, Blazor or similar front end technologies Required 7 YearsExperience with .NET Core Required 5 YearsExperience with Microservices architecture, Azure architecture, Domain driven architecture Required 7 YearsExperience with Docker and azure cloud services such as AKS, Azure functions, Azure Container Service, Azure App services, Azure API Management Required 7 YearsExperience with Azure cloud databases Required 7 YearsExperience with Azure Monitoring, Application Insights Required 5 YearsExperience with configuration management and automation technologies such as Azure DevOPs, PowerShell, Terraform, Chef, and ARM Templates Required 3 YearsAzure certified developer Highly desired 3 YearsExperience with Infrastructure As Code Highly desired 3 YearsExperience with Project Online Rest API's Highly desired 3 YearsStrong scripting skills in Python Desired 2 YearsStrong knowledge of implementing hybrid connectivity between Azure and on-premises systems using virtual networks, VPN, and Express Route Nice to have 2 YearsExperience working with Azure Data factory, Azure Data Lake. Nice to have 2 YearsExperience developing power BI reports Nice to have 2 Years Questions DescriptionQuestion 1 Commonwealth of Virginia security policies prohibit the use of offshore IT contractors. Do you attest to the fact that your candidate will physically reside within the US for the duration of the assignment?Question 2 Please list candidate's email address.Question 3 In what city/state does your candidate PERMANENTLY reside?Question 4 Does your candidate agree to attend an onsite interview, if requested? This is REQUIRED.Question 5 Does your candidate agree to work onsite up to 3 days/week? This is REQUIRED.Question 6 How soon after an offer can your candidate start?\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "141_Azure Solution Architect": "Rajeev,\nTek Inspirations LLC\nrajeev.kharwar@tekinspirations.com\nReply to: rajeev.kharwar@tekinspirations.com\nHello all,I hope you are doing well,I have a role of Azure Solution Architect Please let me know if you are comfortable with this role,At the time of submission I need DL and Visa. Job Description -Azure Solution ArchitectLocation: Onsite in Annapolis, MDType: Very Long-Term ContractMust be local to Maryland In this role I need only senior type of candidate who must have 6 years of experience in D-365 AND have experience in ENTRA ID in RESUME and make sure Entra ID introduced in July 2023 so update resume accordingly but must be in resume (must be able to give AI interview ) please give me the senior type of candidate .Key Responsibilities:Azure Administration:Administer and manage Azure resources, including Virtual Machines, Storage Accounts, Virtual Networks, AKS, Azure Functions, and more.Monitor and optimize Azure costs, configurations, and licensing to ensure cost efficiency.Uphold security best practices, including managing EntraID/Azure AD, role-based access control (RBAC), and Azure Security Center.Ensure architectural integrity and adherence to industry best practices across all Azure services.D365 and Power Platform Management:Oversee and administer D365 applications and the Power Platform (Power Apps, Power Automate, Power BI).Configure, customize, and maintain D365 applications to align with business needs.Implement governance and compliance policies for D365 and Power Platform.DevOps and CI/CD Pipeline Development:Develop and manage CI/CD pipelines for Power Platform and D365 applications using Azure DevOps.Automate deployment processes to ensure seamless integration and delivery of applications.Collaborate with development teams to streamline application development and deployment processes.Disaster Recovery and Business Continuity Planning:Develop and implement disaster recovery (DR) and business continuity plans (BCP) for D365 applications.Configure Azure to support DR and BCP, ensuring minimal downtime and data loss in case of failures.Conduct regular DR/BCP drills to ensure readiness and effectiveness. Backup and Security Management:Ensure all application databases and configurations are securely backed up.Implement and manage backup policies and procedures, including regular testing of backup and restore processes.Maintain the security of application data and configurations, ensuring compliance with data protection regulations.Performance Monitoring and Optimization:Monitor the performance and availability of Azure resources, D365 applications, and Power Platform solutions.Identify and resolve performance bottlenecks and issues.Implement best practices for performance optimization and scalability.Documentation and Reporting:Maintain comprehensive documentation of configurations, processes, and procedures.Generate regular reports on system performance, security, and cost efficiency.Provide training and support to end-users and technical teams as needed.Qualifications:Education: Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.Experience: Minimum of 6 years in Azure administration, DevOps, and D365/Power Platform management.Skills:Strong knowledge of Azure services, including EntraID/Azure AD, Virtual Machines, Storage, Networking, AKS, and Azure DevOps.Experience with disaster recovery and business continuity planning for cloud applications.Proficiency in developing and managing CI/CD pipelines using Azure DevOps or similar tools.Excellent understanding of security best practices and compliance requirements.Strong analytical, problem-solving, and organizational skills.Excellent communication and collaboration skills.Preferred Qualifications:Certifications: Azure Administrator Associate, Azure DevOps Engineer Expert, or Azure Solutions Architect.Technical Skills: Experience with scripting and automation using PowerShell, Azure CLI, or other relevant tools.Methodologies: Familiarity with agile methodologies, specifically Scrum and Kanban. Regards,Rajeev kharwarSr.Technical Recruiter DevOps SpecialistTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035Desk # 469-393-0216 | E: rajeev.kharwar@tekinspirations.comWhatsapp : - 7525894499LinkedIN : - https://www.linkedin.com/in/rajeev-kharwar-8869251b5/ Reach out if you have candidates of ( Site Reliability, AWS/Azure, Cloud Computing, GCP, Infrastructure, ETL/Informatica, Data-warehouse, DataStage, Data Modeling, Python, Data Engineer, Data scientist, ML, Data Architect, Hadoop, Big Data ) Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line to remove@tekinspirations.com. And mention all the e-mail addresses to be removed with any e-mail addresses,which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "142_Role::Sr. Oracle DevOps Engineer": "saloni chaurasia,\ntekinspirations\nsaloni.chaurasia@tekinspirations.com\nReply to: saloni.chaurasia@tekinspirations.com\nHello,I Hope you are doing great.Please find below position if you have any matching candidate as per requirment .Please send me updated resume with candidate information.Job Description -Role:: Sr. Oracle DevOps EngineerLocation:: RemoteDuration::6 monthMust have LinkedIn profiles Location: East Coast preferred but must be open to travel for Go Live Basic Outline:They initially had 200 resumes, but they were all heavy on AWS and Azure. They need Oracle OCI more than traditional DevOps candidates. Job Description:Brooks is a growing, fast-paced technology leader of automation solutions which support the dynamic and expanding semiconductor and life sciences lab markets. At Brooks, new ideas, technologies, and ways of thinking are driving our future. Our customer-focused culture encourages employees to embrace innovation and collaborate with one another to achieve new heights. We are looking for our next Sr. Oracle DevOps Engineer as we accelerate our growth journey. What you\u2019ll bring:Bachelor of Science in Computer Science, Information Technology, or a related field.3 to 5 years experience working with enterprise-level SW in DevOps, QA / Release management environments, and or Cloud Engineering environments4 years hands-on experience with Oracle Cloud Infrastructure, Oracle Integration Cloud, and Oracle Fusion ApplicationsStrong proficiency with six or more of the following: Oracle Field Service, Oracle EPM, Oracle OTM/GTM, Oracle DB, ATP, FBDI, SQL, BICC, BIP, scripting skills, GIT repositories, test automation, CI/CD concepts, AWS technologiesInterest in learning new technologies and practical application towards continuous improvementStrong familiarity with Financials, Supply Chain, Manufacturing, Sales Operations, Service, and FP&A business acumen including forecast, demand planning, month end close processes.Ability to work collaboratively across the organization: interpret user requirements, identify the root cause, research possible solutions, identify most feasible solutionsStrong oral, written and presentation communication skillsProficient in prioritizing, multi-tasking and managing multiple projects using work management tools such as Jira, ITSM, MS TeamsComfortable in a fast-paced environment and have an eye for detail and passion for quality and collaborationRegards,Saloni Chaurasia{ Sr.Technical Recruiter }TEK Inspirations LLC Pvt. Ltd. |13573 Tabasco Cat Trail, Frisco, TX 75035, United StatesE-Mail: saloni.chaurasia@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "143_Azure Architect --Chicago, IL Onsite": "Vipul Kumar,\nGlobal Application Solutions\nvipul@globalapplications.com\nReply to: vipul@globalapplications.com\nRole- Azure Architect Contract Location- Chicago, IL \u2013Onsite This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelors degree in Computer Science, Information Technology, or a related field. - Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles. - Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure. - Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities.Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Email : Vipul@globalapplications.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "144_Azure Cloud Engineer": "Raj,\nTek Inspirations llC\nrajeev.kharwar@tekinspirations.com\nReply to: rajeev.kharwar@tekinspirations.com\nHello All,I hope you are doing well,I have a role of Azure Cloud EngineerPlease let me know if you are comfortable with this role, Job Description -Azure Cloud EngineerLocal Maryland candidates.Candidate should not have less than 1 year projects on resume. Qualifications5 years\u2019 experience as Azure Cloud Administrator or Azure Cloud EngineerCertification- Azure Administrator Associate or higherUS local resident with Public Trust Clearance Key Responsibilities include:Azure Administrator- provision, configure, manage, and monitor Azure Virtual Machines (Windows VMs), storage accounts, IaaS, PaaS, and networking components; perform regular OS and security patching, backups/restore, and implement disaster recovery strategies for Azure resources with maintenance, troubleshooting.Azure Security- be able to set up and manage Microsoft Defender for Cloud, Azure Key Vault, network security, data encryption, and related security compliance and remediation.DevOps- be able to set up and integrate related DevOps processes, managing CI/CD pipelines for Azure Applications.Azure Monitoring- be able to set up Azure Monitor, Log Analytics, Network Watcher, and Azure Application Insights with related automated alerts in emails based on pre-configured rules and thresholds and perform related monitoring and troubleshooting.Expert in Azure App Service, Azure SQL Database, Azure Application Gateway (WAF, load balancing), etc., including setup, configuration, and support. Regards,Rajeev kharwarSr.Technical Recruiter DevOps SpecialistTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035Desk # 469-393-0216 | E: rajeev.kharwar@tekinspirations.comWhatsapp : - 7525894499LinkedIN : - https://www.linkedin.com/in/rajeev-kharwar-8869251b5/ Reach out if you have candidates of ( Site Reliability, AWS/Azure, Cloud Computing, GCP, Infrastructure, ETL/Informatica, Data-warehouse, DataStage, Data Modeling, Python, Data Engineer, Data scientist, ML, Data Architect, Hadoop, Big Data ) Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line to remove@tekinspirations.com. And mention all the e-mail addresses to be removed with any e-mail addresses,which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "145_AWS EKS Engineer -- 6+ Months Contract -- Phoenix, AZ (Onsite - Local Only)": "Balaji,\nLorven Technologies\nbalaji.ke@lorventech.com\nReply to: balaji.ke@lorventech.com\nJob Title: AWS EKS Engineer Location: Phoenix, AZ (Onsite - Only Local)Duration: 6+ Months ContractRequired Skills:Experience with CI/ CD, Git (GitHub, GitLab, BitBucket, SVN), Kubernetes, Linux, AWSBachelor's degree in information technology, Computer Science or the equivalent combination of training, education, and experience8 years of experience in managing Kubernetes clusters, with a focus on AWS EKS.Experience with CI/CD tools (Jenkins, GitLab CI, CircleCI).Strong experience with AWS services. Warm Regards,Balaji K E | Technical RecruiterCall: 609-732-3209 Email: balaji.ke@lorventech.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "146_Cloud Software Engineer": "Sonu Uprati,\nValiantIQ Inc\nsuprati@valiantiq.com\nReply to: suprati@valiantiq.com\nJob : Cloud Software Engineer (Golang SME)Client: ToyotaDuration: 1 yearLocation: Plano, Texas (Hybrid) Activities:Create Golang-based microservices and libraries.Perform code reviews.Contribute to our coding standards and practices.Implement cryptographic algorithms for signing and encrypting.Work with different teams for end-to-end testing.Guide and contribute to our architecture and infrastructure.Create and guide internal documentation processes.Requirements:Strong communication skills, specifically giving/receiving constructive feedback in a collaborative setting.Minimum of 4 years of programming experience (i.e., C, C++, C#, Rust, Python, Golang).Good understanding of API design and security practices.Understanding authorization vs authentication.Rest and grpc.Experience with AWS infrastructure and services including deploying applications and securing applications and sensitive data in cloud environment.Experience working with container technologies.Demonstrable ability to architect, build, and operate distributed systems to solve problems.Experience in secure coding.Good understanding of security concepts like TLS, PKI, SAML/OAuth, Key management, and hashing algorithms.Understanding of CI/CD, Artifactory, and SonarQube Thanks & Regards,Sonu UpratiTechnical Recruiter- ValiantIQ Inc.\"Searching Best Minds \u25a0 Searching Best Minds\"Email: suprati@valiantiq.comF. (302) 482-3672Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line for automatic removal. And mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "147_Data Architect with Snowflake, AWS - Hybrid": "Fazal uddin,\nmsys inc\nresume@msysinc.com\nReply to: resume@msysinc.com\nTitle: Data Architect with Snowflake, AWS - HybridLocation: Naperville, IL, USALength: Long termRestriction: W2 or C2CSend resume to: fazal@msysinc.com Description:**** Webcam interview; *** Long term project *** Hybrid \u2013 2 days a week ****Job Description:ResponsibilitiesLead efforts to ensure data capabilities are operating efficiently, accurately, and with high quality.Collaborate with programs & projects to understand data dependencies, anticipate risks, and identify opportunities. Guide data project decisions based on the direction of data domain roadmap.Contribute data domain expertise to feasibility, complexity assessments of potential future initiatives including data domain analysis.Develops an integrated view of data capabilities and processes, using a repeatable approach, cohesive framework, and available industry best practices and techniques.Provides consulting and makes recommendations to tackle data transformation initiatives, help mature existing data capabilities, or enable new data capabilities.Facilitate the process of defining and publishing data standards, best practices, data capabilities, and data roadmap, synchronized with the overall enterprise future state architecture.Collaborate in creation of conceptual and logical architecture and models following the enterprise guidance to describe a particular domain of data and use these models to inform the physical design of data-related projects.Co-develop data life cycle governance framework including standards, patterns, and controls.QualificationsMinimum eight years of relevant experience as a data architect building large-scale data solutions.Experience in Migration of existing On-Prem Data warehouses to Snowflake.Experience in architecting and large data modernization, data migration, data warehousing \u2013 experience with cloud-based data platforms (like Snowflake)Should have experience in architecture and implementing End to End Modern Data Solutions using AWS, Redshift, S3Good appreciation and at least one implementation experience on processing substrates in Data Engineering \u2013 such as AWS Glue, ETL Tools, ELT techniquesExperience with defining and operationalizing data strategy, data governance, data lineage and quality standards.Experience designing highly scalable ETL processes with complex data transformations, data formats including error handling and monitoring .Extensive knowledge of data engineering, data integration and data management concepts (i.e. APIs, ETL, MDM, CRUD, Pub/Sub, etc.)Experience with data Modeling.Experience with structured and hierarchical datasets (i.e. JSON, XML, etc.)Engineering experience with large scale system integration and analytics projects.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "148_Urgent Requirement || AWS Engineer || Cincinnati, OH (Onsite) ||": "Sadhana Gupta,\nITTRAILBLAZERS\nsadhana.g@ittblazers.com\nReply to: sadhana.g@ittblazers.com\nHi,Greeting of the Day!!.This is Sadhana Gupta from IT TRAIL BLAZERS, please have a look at the below role and let me know if you are interested then you can reply to this email with the updated resume with Visa status and salary expectation. Job Title : AWS EngineerLocation : Cincinnati, OH (Onsite)Duration : Contract C2C/1099/W2 Client : Tech M- Confidantial Skills: AWS SYSOPS Job Description Planning and designing the cloud infrastructure with AWSMaintaining and deploying the cloud applicationsTroubleshooting and resolving issues with the cloud infrastructureProficient in deploying, managing, and scaling containerized applications using Amazon EKS. Experience in configuring Kubernetes clusters, managing workloads, and integrating with other AWS services for enhanced container orchestration and microservices architecture.Experienced in deploying, configuring, and managing Amazon RDS instances for various database engines (MySQL, PostgreSQL, SQL Server, etc.). Proficient in tasks such as backup and recovery, performance tuning, and high availability setup using multi-AZ deployments and Read Replicas.Experienced in setting up and configuring Amazon CloudWatch for monitoring and logging AWS resources and applications.Should have experience on creating custom metrics, setting up alarms, and using CloudWatch Logs for centralized logging and troubleshooting.Proficient in managing and administering Linux-based systems, including server configuration, user management, shell scripting, and performance tuning.Ensuring stable and secure Linux environments for running cloud-based applications and services.Designing and implementing secure network solutions that meet business requirements.Creating and configuring virtualized systems in the AWS environment.Performing infrastructure upgrades and updates to maximize system efficiency while minimizing downtime.Deploying applications in AWS using EC2 instances.Creating blueprints using CloudFormation templates for common workloads.Having experience on disaster recovery procedures is an added advantage.Implementing automation using scripting languages (e.g., Python, Perl) to manage AWS services.Building tools for deployment, monitoring, and troubleshooting of system resources in an AWS environment. Knowledge in Java/Python .NET that interact with AWS cloud services by leveraging the AWS APIs. --Thanks & RegardsSadhana GuptaSr. US IT Recruiter(732) 944-0013 (Call or Text)sadhana.g@ittblazers.com510 Thornall St, Suite #306, Edison NJ 08837USA | CANADA | INDIA | HUNGARY | AUSTRALIA www.ittblazers.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "149_urgent new requirement for  AWS Cloud Technical Architect 9800 Fredericksburg Rd, San Antonio, TX 78288   12 months": "rajesh,\ninfoways\nrajesh@info-ways.com\nReply to: rajesh@info-ways.com\nHi, Hope you are doing great. We have an urgent requirement for the below position with our IT Client HCL.Kindly reply back to this email or call me to discuss further. position : AWS Cloud Technical Architectlocation : 9800 Fredericksburg Rd, San Antonio, TX 78288 duration : 12 months Experience level required (Years) : 10 to 15Mandatory required skills : Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology Job OverviewA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.ResponsibilitiesOwn and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. Provide data architectural solutions/designs to project execution teams for implementation. Provide architectural assessments, strategies, and roadmaps for data management. Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. Project and solution estimation and team structure definition Develop Proof-of-Concept projects to validate new architectures and solutions. Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. Mentor and provide architectural guidance to multiple teams building innovative applications. Drive common vision, practices and capabilities across teams. Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change Best RegardsRajeshInfoWaysPhone#: 609-858-0846Email: Visit us at: www.info-ways.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "150_Urgent: Node AWS Developer at Atlanta, GA (Hybrid)": "srinath,\nCalabitek\nsrinath@calabitek.com\nReply to: srinath@calabitek.com\nNode AWS DeveloperLocation: Atlanta, GA (Hybrid)Job Type: ContractSkills:Node JS, Lambda, AWS CDK , Cloud watch, TypescriptPosition Summary:We are seeking a highly skilled and motivated Full Stack Developer to join our dynamic team in Atlanta, GA. This is a hybrid role requiring the candidate to be on-site from day one. The ideal candidate will have strong expertise in Node.js, AWS services, and Typescript, along with a passion for developing scalable and efficient applications.Key Responsibilities:Develop and maintain server-side applications using Node.js.Design and implement AWS Lambda functions for various serverless applications.Utilize AWS Cloud Development Kit (CDK) to define cloud infrastructure as code.Monitor and manage application performance using AWS CloudWatch.Write clean, maintainable, and efficient code in Typescript.Collaborate with cross-functional teams to design, develop, and implement new features.Troubleshoot and debug applications to optimize performance.Ensure code quality and maintainability through code reviews and automated testing.Stay updated with the latest industry trends and technologies to continuously improve skills and project outcomes.Required Skills and Qualifications:Proven experience in Node.js development.Strong knowledge of AWS services, particularly AWS Lambda and AWS CDK.Proficiency in Typescript.Experience with AWS CloudWatch for monitoring and logging.Familiarity with RESTful APIs and microservices architecture.Solid understanding of software development principles and best practices.Ability to work in a fast-paced, collaborative environment.Strong problem-solving skills and attention to detail.Excellent communication and teamwork skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "151_Sr System Cloud Engineer- Richardson, TX": "Janet Miranda,\nMicasa Global\njanet@micasaglobal.com\nReply to: janet@micasaglobal.com\nSenior System EngineerRichardson. TX- Fully Onsite from Day 1!!Contract, 6+ monthsCANDIDATE WILL BE REQUIRED TO ATTEND FACE TO FACE INTERVIEW Who are we looking for?A Sr. Systems Engineer to manage a project for one of our biggest clients in banking domain. The Individual should be passionate about technology, experienced in developing and managing cutting edge technology applications. Technical Skills:5 to 7 years of relevant experience.Background and experience providing DevOps support to Cloud deployed Application Teams is fundamental, experience supporting Machine Learning and or Artificial Intelligence workloads will be BIG plus.Software Development Fundamentals, Problem Solving, Documentation Skills, Verbal Communication, Application Maintenance, Application and System Security, Promotes Team Building and Process Improvement, System Administration.Must have strong public Cloud technology experience (i.e. AWS, Azure, or GoogleCloud). Experience with Infrastructure-as-code concepts and implementation (i.e. Cloud Formation/TerraForm templating) and configuration management systems (i.e. Ansible, Puppet or Chef).Experience with Kubernetes and other Container Cluster management a plus.A deep understanding and familiarity with: Linux, and Web/Application servers - Apache, Nginx, Tomcat, etc.Storage / Cloud Storage backgroundo EBS, S3, EFS (Elastic File System, i.e. NFS on the cloud)Monitoring and logging systems Cloudwatch, Cloudtrail, ElasticSearch, Kibana, Logstash, or Prometheus, Fluent, Grafana.Virtualization and Containers: DockerNetworking knowledge - Firewalls, VPNs, proxies & Load balancersExperience with Python, shell or other scripting language.Experience with Continuous Integration practices and tools such as Jenkins, Intermediate to advanced knowledge of Git and source code version control.Manages and maintains the hardware, software, security, and connectivity to the Internet as well as middleware components.Troubleshoots and resolves system service failures by identifying and analyzing the situation and provides corrective actions.Develops, installs, and tests new applications, hardware and software releases, system upgrades, evaluates and installs patches and resolves software related problems.Monitors systems activities and fine tunes system parameters and configuration to optimize performance and ensure security of systems.Provides senior level expertise on decisions and priorities regarding systems architecture. Engages with application development teams on current infrastructure projects. Evaluates existing solutions and infrastructure and provides recommendations.Approves system designs and functions as a project lead as required.Facilitates the establishment and implementation of standards and guidelines that guide the design of technology solutions including architecting and implementing solutions.Maintains technical knowledge by attending educational workshops or training; reviewing technical publications; establishing personal network contacts in technical societies.Performs other duties as assigned.Process Skills:Excellent written and verbal communication skillsCapable of analyzing requirements and develop software as per project defined software processDevelop and peer review (Initiate/ participate in peer reviews)Behavioral Skills:Resolve technical issues of projects and Explore alternate designsParticipates as a team member and fosters teamwork by inter-group coordination within the modules of the project.Effectively collaborates and communicates with the stakeholders and ensure client satisfactionTrain and coach members of project groups to ensure effective knowledge management activity.Certification:Technical certifications (e.g. AWS, Linux) desirable\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "152_Urgent Requirement for \"Senior Cloud Application Developer\" :: CITIZEN,GC EAD,GREEN CARD ::": "Alok Panigrahi,\nAdventa Tech INC\nalok@adventatech.com\nReply to: alok@adventatech.com\nJob Description -Job Title: Senior Cloud Application DeveloperLocation: RemoteVisa: NO OPT/CPT/H1BDuration: 6 monthsClient: TBDSend candidates with LinkedIn profile links.Summary To design, develop, implement and maintain new IT solutions, or changes/enhancements to existing solutions that align with business initiatives and corporate strategies.Responsibilities:\u2022 Established track record with Kafka technology, with hands-on production experience and a deep understanding of the Kafka architecture and internals of how it works\u2022 Design, develop, and manage Kafka-based data pipelines\u2022 Recent experience with large Event driven financial transactions to external sources (on-prem to cloud)\u2022 Advanced experience and programming knowledge in distributed Java and SpringBoot\u2022 Experience with both Oracle Cloud Infrastructure and Oracle Integration Cloud\u2022 Experience with Azure Cloud, Azure Kubernetes Service, Docker, and Event Hubs\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "153_AWS Data Application Infrastructure Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: AWS Data Application Infrastructure Lead - OnsiteWork location:ATLANTA (US:30301), GAClient : CapgeminiDESCRIPTIONManage AWS Infrastructure for provisioning\ufffdManage and Streamline Deployments of Pipelines to Production\ufffdManage FinOps for Productionalized applications\ufffdPOC and recommendations for product usage and processes\ufffd Must Have Experience in Big 4 Consulting Companies\ufffdStrong Communication Skills and Client Relationship management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "154_Devops Engineer || Chicago, IL": "Shivam Kamboj,\nSiri Info Solutions\nshivam.kamboj@siriinfo.com\nReply to: shivam.kamboj@siriinfo.com\nGreetings. Hope you are doing wellPlease find the JD and share me your resume if you are interested in this position Devops EngineerChicago, IL Only local candidates JD Key Responsibilities: -- Develop and maintain CI/CD pipelines for Azure cloud environment. - Develop IaC for public cloud infrastructure with Terraform scripts using GitHub Actions.- Develop and maintain Github Actions CI/CD pipelines to deploy applications in public cloud.- Familiarity with build and package tools like Gradle would be preferred- Collaborate with cross-functional teams to define, design, and ship new features. - Ensure the performance, quality and security of applications scanning via different tools in the pipelines. - Help maintain code quality, organization, and automatization.#### Requirements: - Bachelor\u2019s degree in Computer Science, Engineering, or related field. - Proven experience as a Azure DevOps Engineer. - Strong proficiency in Terraform, Github and Github Actions.- Strong proficiency in Network Security and best practices in networking. - Proficiency in scripting languages like - Python, Shell/Bash/PowerShell.- Hands-on experience with containerization technologies like Docker and orchestration tools. - Extensive experience with Microsoft Azure services (e.g., Azure DevOps, Azure Container Apps, Azure Functions, etc.). - Familiarity with CI/CD pipelines and tools, preferably Github Actions.- Familiarity with APM tools such as Azure Monitor, Grafana, ELK stack, Prometheus etc.- Excellent problem-solving skills and attention to detail. - Strong communication and collaboration skills. #### Preferred Qualifications: - Strong proficiency in Microsoft Azure and Terraform for IaC along with Github Actions.- Certification in Microsoft Azure or other relevant technologies.- Knowledge of monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack).- Good to have knowledge on configuration tools like Puppet, Chef or Ansible. - Familiarity with Agile/Scrum methodologies. - Good to have experience with other cloud providers (AWS, Google Cloud). RegardsShivam KambojTechnical RecruiterSiri Info Solutions.Mail Id: shivam.kamboj@siriinfo.comDisclaimer: We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce. Note : Any resume submitted by Siriinfo is presented with the understanding that the candidate is being considered for your direct end-client (end-client is the company where the work will be performed). If there is any other company involved between the end-client and your company, please do not submit this resume without our written approval. If you submit the resume to another third party, Siriinfo reserves the right to work with the third party directly.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "155_Cloud Security Engineer Frisco, TX Onsite": "Aashish N,\nYochana IT Solutions\naashish@yochana.com\nReply to: aashish@yochana.com\nHi, This is Aashish from Yochana IT Solutions. We are looking Cloud Security Engineer Frisco, TX Onsite For the below mentioned job description. Kindly forward your resume, rate and contact details for further processCloud Security Engineer Key Responsibilities\u00b7 Develop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.\u00b7 Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.\u00b7 Implement and enforce security controls to protect sensitive data and systems.\u00b7 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.\u00b7 Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.\u00b7 Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. Qualifications\u00b7 Experience with onboarding and provisioning in cloud environments.\u00b7 Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.\u00b7 In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.\u00b7 Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.\u00b7 Hands-on experience with cloud security services like IAM, Cloud Trail, Config, Guard Duty and Security Hub for threat detection and security monitoring.\u00b7 Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.\u00b7 Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.\u00b7 Proficiency in scripting and automation tools (e.g., Python, Terraform).\u00b7 understands onboarding and will be doing onboarding provisioning work on AWS and Azure\u00b7 Strong communication and documentation skills for collaborating with cross-functional teams. Thanks & Regards....Aashish NPResource specialistYochana IT Solutions23000 Commerce DriveFarmington Hills MI 48335Email : aashish@yochana.comLinkedIn :- https://www.linkedin.com/in/aashish-aash-96345b238/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "156_AWS App Synch Typescript developer": "Ashwani Raghuvanshi,\nTek Inspirations\nashwani.raghuvanshi@tekinspirations.com\nReply to: ashwani.raghuvanshi@tekinspirations.com\nJob Description -Interviewer will be Indian so please submit AccordinglyRole: AWS App Synch/Typescript developerClient: Cox Communications(Capgemini) (Candidate must have telecommunications client experience)Duration: 6 monthsLocation: Atlanta, GA (Remote)No H1 CPTPhone/SkypeNeed Candidate Linkedin and having a excellent communications skillsResponsibilities:\u2022 Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.\u2022 Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.\u2022 Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.\u2022 Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.\u2022 Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.\u2022 Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.\u2022 Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.\u2022 Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.\u2022 Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.\u2022 Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.\u2022 Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.\u2022 Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.Requirements:\u2022 Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.\u2022 Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.\u2022 Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.\u2022 Experience coding in Swift is a plus.\u2022 Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.\u2022 Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.\u2022 Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.\u2022 Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.\u2022 Familiarity with Charles Proxy or similar tools for network analysis and debugging.\u2022 Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.\u2022 Experience with CI/CD pipelines and DevOps practices is a plus.\u2022 Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.\u2022 Strong documentation skills to maintain clear and concise technical documentation.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "157_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "158_Hot Requirements : DevOps Architect  Remote Need 12+": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nDevOps ArchitectLPLRemoteTerraform Template creation Terraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication servicesThanks and RegardsPrincy JainPrincy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "159_Requirement Azure 21ViaNet Cloud Security Engineer Remote": "Manoj Nirwan,\nTanisha Systems Inc.\nmanoj@tanishasystems.com\nReply to: manoj@tanishasystems.com\nGood Morning,Hope you are doing well.I have an urgent requirement for Azure 21ViaNet Cloud Security Engineer Remote.Please find the JD below and share updated resume copy and help me with hourly rate.I will really appreciate your quick response.Job Title: Azure 21ViaNet Cloud Security EngineerLocations: RemoteJob Type: Long Term ContractObjective: To enhance and maintain the security of Azure 21Vianet cloud environments and services. To ensure the secure and compliant migration of cloud resources from one region to another, minimizing downtime and protecting against data breaches.Job Description:Key Responsibilities:Define and implement cloud security standards for Azure 21ViaNet deployments.Collaborate with DevOps teams to integrate security controls into IAC code frameworks/pipelines.Help develop a migration strategy for moving from the Isolated Entra ID to a new sychronized directory (AD Connect) and move from Microsoft Entra Domain Services (Azure AD DS) over to native ADAssist Network Team with analyzing acceptable firewall rule policies for communication from China On-prem to China Azure.Monitor cloud security posture and identify misconfiguration issues and compliance risks.Respond to and investigate security incidents and vulnerabilities within the Azure 21Vianet environment.Develop and implement security protocols for the migration process.Monitor migration activities to ensure security compliance and report any incidents or vulnerabilities.Required Skills: Familiarity with Azure 21VianetStrong understanding of cloud infrastructure, networking, and security in Azure.Experience with cloud migration tools and methodologies.Knowledge of regulatory compliance requirements related to data security and privacy.Proficiency in cloud security services, particularly within Azure 21Vianet.Experience with cloud security posture management tools.Knowledge of application development security and database security.Certifications:Relevant security certifications such as CISSP, CISM, or cloud-specific certifications are advantageousThanks & Regards:Manoj NirwanTanisha Systems Inc.Iselin, NJ 08830Desk: (732) 944-0249manoj@tanishasystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "160_Cloud Network Engineer with GCP": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nJob Description -Cloud Network Engineer with GCPLocation : New York, NYNeed consultants close to Burlington County, NY, and NYC.About the jobRole and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform. Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform. Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture. Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code. Automate the existing code components and integrate technologies to eliminate manual deployment steps. Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases. The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience. Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages. Experience with automating and integrating Serverless cloud provided PaaS solutions. Ability to troubleshoot code and logic errors for cloud-based network services. Understanding of deployment platforms and databases via CI/CD pipeline. Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms. Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus. Proficiency in cloud automation using cloud native CLI/API. Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP. Must have working experience with Cloud Native Networking technologies. Must have experience using PowerShell to configure Cloud Networking components. Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS. Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes. A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team. Able to work under tight timeline and deliver on complex problems. Education:Bachelor's degree in computer science, engineering or a related field, or equivalent work experienceCertifications:CISSP, CCSP, Microsoft MCSE Azure - 400 or 500Desired Skills and ExperienceNETWORK ENGINEER\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "161_Devops Engineer \\\\ Dallas, TX \\\\ TCS": "sagar kamboj,\nTCS\nsagar.kamboj@siriinfo.com\nReply to: sagar.kamboj@siriinfo.com\nDevops Engineer \\\\ Dallas, TX \\\\ TCS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "162_Sr. System Cloud Engineer || Chandler, AZ - Charlotte, NC || Local Candidate with DL only || 15+ Only": "Sachin Nayak,\nAdventa\nsachin@adventatech.com\nReply to: sachin@adventatech.com\nJob Description - I need 15+ years or EX-wells fargo (FOCUS ON AZ and NC location) Banking Background needed10 openings , Submit only candidates who can speak about their experience. Please vet them before submission.Sr. System Cloud Engineer -Chandler, AZ / Charlotte, NC / (On-Site)Location: Chandler, AZ / Charlotte, NC / (On-Site)Duration: 12 monthsVisa: Any Excellent communication skills Desired candidate for this role must to support the on time, and on budget build out of private cloud on Wells Fargo\u2019s data centers.Review and analyze complex technical challenges, as well as escalated support issues related to core business solutions that require in depth evaluation of multiple factors, such as alternatives, enhancements, periodic systems reviews, or improvements to existing systems.Make decisions on technical changes and enhancements.Strong experience with Automation - Ansible and Terraform tools and scripting to automate by using shell scripts.Strong implementation experience in Pivotal Cloud Foundry (PCF)/Tanzu App Service (TAS) and Container platformsExperience in building and rollout Configuration drift and CI/CD on container platforms.Consult with engineering team on change design requiring solid understanding of technical process controls or standards that across all platforms/clusters.Collaborate and consult with technical peers, colleagues, and mid to more experienced level managers to resolve systems support issues and achieve goals!Work closely with internal engineering teams to adopt and adapt to best practices and assist partners with troubleshooting with onboarding, migrations to new cloud platforms and day-to-day break-fix activities and upgrades as needed.Adventa Tech Inc. (An E-Verified Company)24718 Tribe Square #306, Dulles, VA 20166 Desk: +1 (571)-463-1794 EXT: 147E-Mail: sachin@adventatech.comLinkedin : linkedin.com/in/sachin-nayak-a36943221\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "163_Required || Azure Devops Engineer with OpenAI, LLM || 100% Remote || NOH1B": "Akhilesh,\nDMS VISIONS\nakhilesh@dmsvisions.com\nReply to: akhilesh@dmsvisions.com\nHi,Hope you are doing well,This is a contract position, Please share with me Updated resume of yours or your consultant if the profile matches this requirement. 100% Remote Azure DevOps EngineerLocation: REMOTE - Prefer East Coast candidates but CST will workDuration: 6 months +No H1b We are seeking a talented and self-motivated Senior DevOps to support Azure OpenAI. You'll play a crucial role in designing, implementing, and maintaining the infrastructure and deployment processes for AI models. Your focus will be on ensuring seamless integration, continuous delivery, and efficient management of OpenAI services within an Azure environment. You will be assisting in developing our core team and helping to deliver a highly available platform for software solutions while collaborating with our larger Technology team globally. This is an exciting opportunity to play a leading role in changing the way that Elsevier products are delivered within the company and to our customers, devising and implementing a modern approach to infrastructure engineering that enables continuous delivery and releases features on-demand. The role holder will have excellent technical and project management skills as you\u2019ll be part of the Technology Infrastructure and Operations (TIO) team supporting, designing, migrating, and implementing products globally deployed within the Microsoft Azure Cloud Qualifications\u2022 5+ years of Systems Engineering experience\u2022 BS Engineering/Computer Science or equivalent experience required Key TasksCollaborate with software engineers, testers, and system administrators to ensure smooth integration of OpenAI services into applications. Foster a culture of teamwork and continuous improvementSet up monitoring, logging, and alerting for OpenAI services. Optimize resource utilization, scalability, and performanceDevelop and maintain infrastructure patterns supporting promotion and deprecation of OpenAI and other LLM models within an API configurationBuilding and maintaining a set of tools that enable automation for creating and supporting Azure subscriptions in a large-scale tenantImproving existing Continuous integration across multiple product pipelinesOversee the day- to-day remediation of critical issues and building processes and efficiencies to eliminate the recurrence of issueMaintain the health and security of infrastructure by monitoring and patchingAbility to deliver TIO strategy in partnership with the business platforms needsParticipate in off hours on-call support schedule Key Skills / Experience3 \u2013 5 years' experience with administering Microsoft Azure subscriptions in an Enterprise environmentUnderstand Azure deployment processes such as Azure Blueprints, ARM templates and PowerShell ModulesProficiency with the Azure Command Line (CLI) interfaceDeep knowledge of Azure Active Directory, IAM and RolesPracticed in Azure infrastructure \u2013 Virtual Machines, Azure Storage, Azure App Service and Database offeringsUnderstand Azure VNETS, Express route, Internet networking, Virtual Private Networks and DNSFamiliarity of Azure Cognitive Services and its component offeringsAbility to use Terraform in an enterprise environment, including module creation and useAdvanced Linux and Windows server administration experienceDocker and Containerization, Kubernetes a plusGit and source control proceduresAn understanding of coding practices and of DRY principlesAuthoring scripts with Bash and PythonExecuting and tracking tasks in an Agile environmentContinuous Integration systems such as Jenkins and GitHub ActionsAbility to work on several work streams with excellent time managementAble to formulate and execute solutions to take into consideration the needs of multiple stakeholdersA positive, constructive approach with an emphasis on collaboration and good executionAble to mentor and lead other members of the teamADDITIONAL INFORMATION FROM MANAGER: To get started, can you give me a quick overview of your team and what you do? Core Engineering Enablement is a team focused on central support of Cloud services at Elsevier. The team is expanding to include support of the Azure Cloud specifically the deployment of OpenAI and other Generative AI systems The person who will be successful in this role will be somebody who?This will be a new position within the Elsevier Core Engineering team supporting Generative AI infrastructure in Azure. What would you say are the TOP 3 must-have skills you\u2019re looking for? (Measurable skills, technologies, etc.) a.) Experience with Microsoft Azure in an Enterprise setting. Support for multiple subscriptions and tenantsb.) Terraformc.) Familiarity of OpenAI or other Large Language Model (LLM) systems How many years of experience are you looking for? As you start to go through resumes, what would stand out to you as the BEST candidate versus an AVERAGE candidate? Candidates that can describe their use and experience of the technologies listed above. Resumes with just bulleted technologies are considered less Does your team work standard core hours or does that vary? The majority of the team works 9AM \u2013 5PM Eastern Do you need someone with a particular degree or certification? AZ-104 \u2013 Azure Administrator Associate (Strong preference) AI-900 \u2013 Azure AI Fundamentals (Nice to have) AI-102 \u2013 Designing and Implementing a Microsoft Azure AI Solution (Nice to have) What are some of the performance indicators you\u2019ll be looking for once this person ramps up? (Meeting deadlines, quality of work, etc.) Meeting Deadlines, quality of work. Customer communications. Forward thinker and planner. Ability to work independently from a Jira backlog. Interaction with peers and take direction from Sr Engineer and Technical Lead With some of the other huge tech companies out there, competition for talent at all levels can get pretty tough at times. When suppliers start sourcing, what would you say are some of the main selling points that would get candidates interested in your role over another? Thank youAkhilesh@dmsvisions.comDirect :- 972-645-0322, Ext :- 110DMS Visions, INC\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "164_Azure DevOps Engineer with OpenAI or Other Large Language Model (100% remote) || No H1B": "Himanshu,\nDMS Visions INC\nhimanshu@dmsvisions.com\nReply to: himanshu@dmsvisions.com\nHi,Hope you are doing well.Please find the job description given below and let me know your interest.Title: Azure DevOps Engineer with OpenAI or Other Large Language Model (100% Remote)Location: 100% RemoteDuration: 6+ MonthsVisa \u2013 No H1BJob Description:Qualifications\u2022 5+ years of Systems Engineering experience\u2022 BS Engineering/Computer Science or equivalent experience required Key TasksCollaborate with software engineers, testers, and system administrators to ensure smooth integration of OpenAI services into applications. Foster a culture of teamwork and continuous improvementSet up monitoring, logging, and alerting for OpenAI services. Optimize resource utilization, scalability, and performanceDevelop and maintain infrastructure patterns supporting promotion and deprecation of OpenAI and other LLM models within an API configurationBuilding and maintaining a set of tools that enable automation for creating and supporting Azure subscriptions in a large-scale tenantImproving existing Continuous integration across multiple product pipelinesOversee the day- to-day remediation of critical issues and building processes and efficiencies to eliminate the recurrence of issueMaintain the health and security of infrastructure by monitoring and patchingAbility to deliver TIO strategy in partnership with the business platforms needsParticipate in off hours on-call support scheduleKey Skills / Experience3 \u2013 5 years' experience with administering Microsoft Azure subscriptions in an Enterprise environmentUnderstand Azure deployment processes such as Azure Blueprints, ARM templates and PowerShell ModulesProficiency with the Azure Command Line (CLI) interfaceDeep knowledge of Azure Active Directory, IAM and RolesPracticed in Azure infrastructure \u2013 Virtual Machines, Azure Storage, Azure App Service and Database offeringsUnderstand Azure VNETS, Express route, Internet networking, Virtual Private Networks and DNSFamiliarity of Azure Cognitive Services and its component offeringsAbility to use Terraform in an enterprise environment, including module creation and useAdvanced Linux and Windows server administration experienceDocker and Containerization, Kubernetes a plusGit and source control proceduresAn understanding of coding practices and of DRY principlesAuthoring scripts with Bash and PythonExecuting and tracking tasks in an Agile environmentContinuous Integration systems such as Jenkins and GitHub ActionsAbility to work on several work streams with excellent time managementAble to formulate and execute solutions to take into consideration the needs of multiple stakeholdersA positive, constructive approach with an emphasis on collaboration and good executionAble to mentor and lead other members of the team If you are interested, please share your updated resume and suggest the best number & time to connect with you.Thanks & Regards, Himanshu GuptaUS IT RECRUITER, DMS VISIONS INC9726455552 | 4704679946 | dmsvisions.comhimanshu@dmsvisions.comLinkedIn: linkedin.com/in/himanshu-gupta-1888a12074645 Avon Lane, Suite 210, Frisco, TX 75033\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "165_Sr. AWS Engineer": "Rahul Singh,\nTek Inspirations LLC\nrahul.singh@tekinspirations.com\nReply to: rahul.singh@tekinspirations.com\nJob Description -Sr. AWS EngineerLocation: 100% Remote Tenure: 6 month contractMUST HAVE SKILLS:1. Heavy hitter when it comes to AWS2. AWS Professional Certification (Solutions Architect Professional)3. Fluent in TypeScript and the AWS CDK. Software developer background before moving into DevOps, is a huge bonus.4. GitHub Actions (in particular, moving workloads from Jenkins to GitHub Actions).5. Experience as a thought leader / technical lead.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "166_AWS Data Application Infrastructure GA - Hybrid Role": "Hemant Rawat,\nconvextech\nhemant@convextech.com\nReply to: hemant@convextech.com\nHi,Hope you are doing good.!!Please let me know if you are interested in the below position Title: AWS Data Application Infrastructure Location: GA - Hybrid Role Duration: 6+ Months MOI: Phone/Skype VISA: NO H1B/CPT --------------------------JOB DESCRIPTION -----------------------Manage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processes Thanks and Regards Hemant RawatUS IT Recruiter |Convex Tech Inc.Email: hemant@convextech.comwww.linkedin.com/in/hemantrawatt\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "167_Need AWS Data Application Infrastructure Lead at GA  Hybrid Role": "Sharon W,\nHAN IT\nsharon@hanstaffing.com\nReply to: sharon@hanstaffing.com\nAWS Data Application Infrastructure Lead Location:ATLANTA (US:30301), GA - Hybrid roleContract Role Need MIn 15+ years of Experience DESCRIPTION Manage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience in Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship management Please share Visa, DL copy and below details to proceed. Legal Name:Current Location: (City, State & Zip Code):Home location:Relocate:Bill Rate:CTH After 3 Months:Travelling AvailabilityAvailability to Start:Phone/Mobile Number:Skype ID:Email Address:Visa Type:Visa Expiration Date:In process of getting a green card?Hiring Status: Are you working directly with the contractors visa holder:If not indicate # of layers and names of the company:Indicate if the Candidate has worked in CG before and where:Ex-Capgemini Employee or Ex-Contractor?:LinkedIn Account: (If available)Time slots for an interview:Contractor approved to share its resume to client:Skills summary:\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "168_Job opening for DevOps + Scrum in  Palo Alto or San Ramon CA": "Aayush Jain,\nResource logistics\naayush@resource-logistics.com\nReply to: aayush@resource-logistics.com\nHi Greetings Hi It\u2019s Aayush Jain from Resource Logistics. Please review the job description below. If you are interested in this position, please forward your updated resume and let me know your work authorization for immediate consideration and preferred time to discuss this opportunity further. Job Role DevOps Engineer + Scrum CertifiedLocation: Palo Alto or San Ramon CA(Onsite) Duration: ContractJob description:\u2022 Monitor production systems in multiple data centers.\u2022 Manage critical incident, including escalation, debugging, fix and root cause analysis.\u2022 Help R&D debug production issues.\u2022 End-to-end K8s application modernization and delivery via on K8S G6R\u2022 Decentralized \"enforced state\" GitOps approach.\u2022 Automated certificate management, DNS, and MTLS\u2022 Unified, auto scaling, easier to adopt and faster e2e develop to deploy framework and strategy.\u2022 Reduced go to market timelines (build to prod )\u2022 Managed DR with reduced costs\u2022 Cloud agnostic and support across 4+1 & Sovereign Cloud deployments\u2022 SGS, ISBN SecOps, and SAP HASI approved processes\u2022 Managed multi-stage backups (storage, Velero, DB-native)\u2022 Process automation and implementation.\u2022 Manage public cloud environments including Terraform, GCP, and/or AWS.\u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault.\u2022 Kubernetes containerization.\u2022 Operate system internals, file system structures and machine architectures in a Linux operating environment; and\u2022 Apache, DNS, Send mail, SSH, TCP/IP, and NFS.\u2022 Support services before they go live through activities such as system design consulting, developing software platforms and frameworks, capacity planning and launch reviews.\u2022 Scale systems sustainably through mechanisms like automation and evolve systems by pushing for changes that improve reliability and velocity.\u2022 Work with product team to define SLA, SLO, monitoring/telemetry pattern and stability across from customer standpoint.\u2022 Help product team gain momentum and velocity for hassle free development across BTP, Hyperscaler, K8S.\u2022 Build, Manage, Scale and Right size/TCO initiatives across SAP Hyperscaler(4+1) strategy towards cloud spend management.\u2022 Engage in and improve the whole lifecycle of services\u2014from inception and design, through to deployment, operation, and refinement.Qualifications\u2022 Development experience in Bash/Python/Go/Groovy, Perl and Shell scripting.\u2022 Process automation and implementation.\u2022 Experience with public cloud environments and toolsets including Terraform, GCP, and/or AWS.\u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault.\u2022 Kubernetes containerization experience.\u2022 Experience with operating system internals, file system structures and machine architectures in a Linux operating environment; and Apache, DNS, Send mail, SSH, TCP/IP, and NFS.\u2022 5-7 years of experience working in a public cloud, automation, and Unix internals\u2022 > 4 year devops/SRE experience.\u2022 Familiar with SRE operation and on-call procedure.\u2022 Familiar with log analysis, such as Splunk or ELK.\u2022 Scrum master certification preferred.\u2022 BA/BS degree in MIS/CS or equivalent experience\u2022 Experienced working in a dynamic, fast-paced environment with well-developed practices and procedures.\u2022End-to-end K8s application modernization\u2022Manage public cloud environments including Terraform, GCP, and/or AWS.\u2022 CI/CD and DevOps tools including Jenkins, Artifactory, Docker, and/or Vault\u2022 SGS, ISBN SecOps, and SAP HASI approved processes\u2022 Managed multi-stage backups (storage, Velero, DB-native)Artificial intelligenceScrum Master Certified\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "169_Azure DevOps Kubernetes Engineer  location: Plano TX--only locals--onsite": "kalyani,\ndazzlon\nkalyanig@dazzlon.com\nReply to: kalyanig@dazzlon.com\nHello,Hope you are doing Good.Position: Azure DevOps Kubernetes Engineerlocation:Plano, TXNeed locals\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "170_Required Lead Azure Cloud Engineer Tampa, FL- Hybrid 3 days onsite- MUST BE IN TAMPA --USC,GC ONLY": "Pankaj,\nkpg99\nps@kpgtech.com\nReply to: ps@kpgtech.com\nHi,Hope you are doing well.Please find the job description below and let me know your interest.Position: Lead Azure Cloud EngineerLocation: Tampa, FL- Hybrid 3 days onsite-Duration: 6+ MonthsMode of Interview: Phone and VideoJob Description: Bachelor\u2019s degree in computer science, Applied Computer Science, or related field.5+ years or related experience.Passion for technology innovation, a curious mind, and an entrepreneur mindset.Ability to present technical information clearly to different management levels.Azure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plusOther experience to include the following technologies: CICD patterns, Terraform.Experience using the following tools: GIT, Bit Bucket, Jira, Confluence, Jenkins.Experience with Python scripting preferred.Working knowledge of AWS is a plus.Knowledge of different software development methodologies (Waterfall, Agile, Scrum, Kanban)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "171_JOB | Cloud Security Engineers| TX onsite": "Manoj Rathee,\nSunray Enterprise, Inc.\nmanoj@sunraycorp.com\nReply to: manoj@sunraycorp.com\nHi Dear,I hope my mail finds you in good health and doing well!We currently have the JOB POSITION listed below available.Kindly go through the job description and share your latest updated RESUME, visa copy, and photo ID so that I can submit the profile to the clientJob Position :- Cloud Security Engineers Locations :- Frisco, TX \u2013 Day 1 ONSITEDuration :- Long TermJob Description:-Key Responsibilities\u00b7 Develop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.\u00b7 Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.\u00b7 Implement and enforce security controls to protect sensitive data and systems.\u00b7 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.\u00b7 Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.\u00b7 Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. Qualifications\u00b7 Experience with onboarding and provisioning in cloud environments.\u00b7 Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.\u00b7 In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.\u00b7 Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.\u00b7 Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.\u00b7 Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.\u00b7 Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.\u00b7 Proficiency in scripting and automation tools (e.g., Python, Terraform).\u00b7 understands onboarding and will be doing onboarding provisioning work on AWS and Azure\u00b7 Strong communication and documentation skills for collaborating with cross-functional teams. Best Regards,\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "172_Cloud Software Engineer": "David,\nFluxtek solutions\ndavid@fluxteksol.com\nReply to: david@fluxteksol.com\nJob Title:Cloud Software EngineerLocation: Plano, TXDuration: Long Time Job Description Minimum of 4 years of programming experience (i.e., C, C++, C#, Rust, Python, Golang)MUST BE STRONG IN CODING: GOLAND, GO & PYTHONExperience in API design and security practices.Experience with AWS infrastructure and services including deploying applications and securing applications and sensitive data in cloud environment.Warm RegardsDavidUS IT RecruitingFluxtek Solution IncEmail : david@fluxteksol.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "173_Azure DevOps Engineer": "harsha,\nnorthite\nharsha@northite.com\nReply to: harsha@northite.com\nLocations:Princeton, NJ, USA;New York, NY, USAMinimum Experience:4Maximum Experience:6Experience:4 to 6 YearsLocation:Princeton / New YorkJob Description: Azure DevOps Engineer(4-5 Years Azure experience, Expertise in deploying .Net based apps, Docker/Kubernetes/GH Actions/Azure DevOps)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "174_Looking On-site || Apigee Installation Both (On-prem and Cloud) || Miramar, FL (Onsite)": "Rhitik Gaur,\nQuantum World It\nrhitik.gaur@quantumworldit.com\nReply to: rhitik.gaur@quantumworldit.com\nGreeting,I hope all is well with youFor the following, Quantum World IT is seeking the best consultant profile.Please respond with your most recent resume if you are considering new opportunities.Looking On-site || Apigee Installation Both (On-prem and Cloud) || Miramar, FL (Onsite)Role name:System AdministratorRole Description:Responsibility of / Expectations from the Role Coordinates with App teams and gather deployment requirements.. Lead the design and implementation of API Gateway solutions both on prem and Cloud . Lead analytical and consulting working sessions to solve technical problems faced by the application teams trying to onboard to APIGEE Gateway. Lead Capacity Planning and Management activities to ensure that the APIGEE platform will be able to handle additional transactions in the future. Help troubleshoot and resolve production issues related to the APIGEE platform API Gateway. Lead root cause analysis session to understand cause of issues in Production and come up with solutions that will prevent them from happening in the future. Ensures adherence to SLAs by resolving tickets. Performs system health checks and reports anomaly.Competencies:Digital : APIGEEEssential Skills:Apigee Installation Both (On-prem and Cloud) Multi node deployment in AWS and On-prem. Integrating APIGEE with authentication system eg ADS Certificate managementDesirable Skills:Key parameters to monitor APIGEE using Grafana/AppD Component troubleshooting Upgrade/maintenance Apigee capacity expansion Apigee X Deployment on sandbox environmentPlease enter the following information is needed for submission and share your visa and Photo id Position Applied for Candidate Full Name [As per Passport] Contact Number Primary & Secondary Email ID Current Location LinkedIn Passport No. Work Authorization Had ever worked with TCS in Past (Required filed) Are you comfortable for on-site Had you gotten covid vaccination doses Rate expectation Thanks & RegardsRhitik GaurDirect : +1 805 973 0148Email: Rhitik.gaur@quantumworldit.comQuantum World Technologies Inc.4281 Katella Ave, Suite #102 Los Alamitos CA 90720 USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "175_AWS Cloud Engineer Little Rock, AR 72201 LOCALS, ONSITE": "srikanth,\nHCL Global\nsrikanth.g@hclglobal.com\nReply to: srikanth.g@hclglobal.com\nHands-on experience migrating/implementing significant applications on AWS Cloud platformsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyExperience with high volume, mission critical applications and their interdependencies with other applications and databasesInfrastructure to Code scripting using Cloud Formation, or other languages, and AMIMonitoring, maintenance, and support of the infrastructure within AWS environmentsExperience using monitoring solutions like CloudWatch and CloudTrailExperience configuring CrowdStrike and TenableExperience and knowledge of AWS network-level logging configuration and managementConfiguration and maintenance of AWS dashboardsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesExperience designing and implementing common shared services across enterprise applicationsStrong analytical skills and ability to resolve complex business/IT problemsMicrosoft Windows Server System Administrator experienceGenTax architecture and application maintenance experienceFamiliarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Strong communication skills, both oral and writtenAvailability to provide 24 hour supportExperience using Incident Management software solutions like JiraAWS Solutions Architect certification\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "176_Terraform Architect with GCP  Austin, TX ( Remote )": "Gopi Chand,\nYochana IT\ngopi@yochana.com\nReply to: gopi@yochana.com\nHello, I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider with 14 plus years of experience delivering value to leading companies across the U.S and Canada. I'm currently staffing for Terraform Architect with GCP \u2013 Austin, TX ( Remote ). Below you will find the job description, if you are qualified and interested please send me your Updated Word Document Resume. My apologies if this position is not an ideal fit, we'll keep you in mind for other suitable positions and referrals would be appreciated. Thank you in advance. Role: Terraform Architect with GCPLocation: Austin, TX ( Remote )Vendor / Client: Direct Vendor Duration: Contract Job Summary: 12 to 15 years of Minimum experience 5+ Years of experience in GCP 5+ Years of experience in Terraform Enterprise Architect or Terraform Architect Must have lead experience Azure/AWS or other cloud providers experience Docker and other virtualization technologies, Scripting language (bash, PowerShell etc) Expert-level proficiency in designing and implementing cloud infrastructure solutions on the google Cloud Platform (GCP) Utilizing Terraform for infrastructure as code (IaC). Extensive hands-on experience with Terraform, including module development, configuration management, and state management, to automate and streamline infrastructure provisioning and management processes. Deep understanding of GCP&#39;s core services and offerings, such as Compute Engine, Cloud Storage, Cloud SQL, VPCs, and IAM, and how to leverage Terraform to provision and manage these resources effectively. Demonstrated ability to utilize Terraform to orchestrate complex, multi-tiered GCP architectures, incorporating best practices for security, scalability, and cost optimization. Skilled in Version 1.2 troubleshooting Terraform configurations, identifying and resolving infrastructure-related issues, and optimizing Terraform code for improved performance and efficiency. Thanks and regardsGopi ChandTechnical Team Lead Yochana IT Solutions Inc23000 Commerce Dr., Farmington hills, MI-48335gopi@Yochana.com || www.yochana.comLet\u2019s Challenge The Status Quo\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "177_) Job Opportunity for AWS (12+ Exp) Cloud Solutions Engineer - Onsite to Arkansas": "Priyanshi,\nParin Technologies LLC\npriyanshi@parintec.com\nReply to: priyanshi@parintec.com\nPosition Title: AWS Cloud EngineerLocation: Little Rock, AR 72201ONSITEDuration: 12 months+Interview: AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems. OverviewClient is migrating COTS systems from the current on premise data center to the AWS Cloud environment. The AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center.Roles and Responsibilities\u2022 Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.\u2022 Plan, design, and implementation of workload migrations from on premises to AWS Cloud.\u2022 Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.\u2022 Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.\u2022 Setup and maintain Gateway Load Balancer and Internet Gateway.\u2022 Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.\u2022 Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.\u2022 Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.\u2022 Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.\u2022 Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.\u2022 Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.\u2022 Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.\u2022 Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.\u2022 Participate in the selection of new technologies while maintaining departmental information technology and security standards.\u2022 Effectively communicate with Management, co-workers, AMS, and business partners.\u2022 Perform other duties as assigned.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "178_Azure Cloud Security Hybrid Chicago,Illinois,": "Rahman khan,\nAvance consulting\nrahman.khan@avanceservices.us\nReply to: rahman.khan@avanceservices.us\nJob Description:We are seeking a highly skilled Azure Cloud Security Technical lead with expertise in Cloudsecurity products, authentication, authorization, in Workforce Identity and Access management(IAM). As a key member of our IAM Security Engineering team, you will play a vital role inensuring the secure and compliant implementation of various solutions in the Cloud focused onIdentity and access Management domain.Requirements1. In-depth knowledge and experience on Azure and AWS Security2. Expertise in different Azure Services, Subscriptions, management groups3. Design, implement Azure Security solutions to ensure secure and efficientauthentication and authorization processes aligned with industry best practices4. Drive the onboarding of applications, Application registration, enterprise applicationsetup, and role-based access management (RBAC).5. Experience in architecting custom solutions using Java Frameworks on the Azure is amust6. Lead the implementation of Multi-Factor Authentication (MFA) and Single Sign-On (SSO)for enhanced security.7. Proven experience on Azure security such as RBAC, Permissions, actions, identities,Roles, privileged access management8. In-depth knowledge of Azure AD, Azure AD B2B, related authentication/authorizationcomponents and security protocols which including SAML, OAuth, and OpenID9. Strong scripting and automation skills (PowerShell, Azure CLI)10. Experience in architecting custom solutions using Java Frameworks on the Azure is amust11. Expertise in configuring and troubleshooting authentication protocols, including OAuth,OpenID Connect, and SAML for secure authentication and authorization12. Good understanding of Cloud Infrastructure Entitlement Management solutions /Microsoft Entra Permissions Management13. Configure and manage conditional access policies to control access based on specificconditions, locations, and device compliance14. Collaborate with cross-functional teams to support and troubleshoot IAM-related issues,ensuring solutions are secure, compliant, and scalable.15. Understand and implement security best practices for Azure products, services, andsolutions.16. Hands on experience related to DevSecOps, IaC, CI/CD pipeline, automation, andvulnerability scanning tools, Terraform, Powershell, bash script, Azure CLI17. Experience as Full stack application development on technologies like Java, React,JavaScript, SQL and Oracle databases18. Utilize Azure Sentinel for monitoring, creating alerts, and developing automation scriptsfor incident response.19. Provide production support, responding to and resolving security incidents in a timelymanner.20. Establish and maintain identity governance frameworks, including privileged identitymanagement (PIM) for elevated access21. Stay informed of Azure updates, security threats, and industry best practices toenhance our security posture.22. Collaborate with DevOps and development teams, demonstrating a basic understandingof tools and requirements.Qualifications:1. Bachelors degree in computer science or a related discipline and experience ininformation security, or an equivalent combination of education and work experience.2. Deep knowledge of application or infrastructure systems architecture, usually havingexperience with multiple system technologies.3. Excellent consultative and communication skills, and the ability to work effectively withclient, partner, and IT management and staff.4. Five years of experience in the Information Security role. Three years of experience withcloud and/or technologies5. Cloud security certification preferred6. Strong collaboration skills and a analytical ability7. Certifications on Azure, AWS security will be preferred8. Excellent understanding of cloud security principles9. Ability to work in a dynamic environment and adapt to evolving security challenges.10. Excellent communication and collaboration skills for working with cross-functionalteams.11. Commitment to maintaining a secure, compliant, and scalable IAM solution.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "179_Looking for Azure Kubernetes Services Expert in Need Local To IL Candidates": "Khursheed,\nRHG\nresponseph@resourcehuntgroup.com\nReply to: responseph@resourcehuntgroup.com\nHi Professional, Hope you're doing great! Azure Kubernetes Services Expert Chicago ILNeed Local to IL CandidatesJD:To provide design, engineering, development, planning, and administration of Azure Kubernetes AKS clusters for a set of critical business applications.Work closely with the application, engineering, security, and operations teams to engineer and build Kubernetes and Azure PaaS & IaaS solutions within an agile and modern enterprise-grade operating model.Design solutions for Kubernetes based architecture while making sure the best practices are followedAbility to capture the entire infrastructure as a code Implement the principles of SREDesign deployment architecture for IaaS / SaaS /PaaS that will address our client\u2019s business needs and lead teams that will implement them for our clients.Experience with workload migration automation toolsInfrastructure provisioning and management (Terraform, Ansible, Chef, Puppet, Chef, YAML, Containers, Azure Container Registry, Docker, DockerHub)DevOps and Agile methodologies, processes and toolsExceptional written and verbal communication skillsConfidently articulates all aspects of the solution and persuasively communicates value to the client.Works individually, within teams, or as a leader, to determine customer requirements in complex and ambiguous environments.Self-motivated with strong analytical and presentation skills.Ability to work with geographically dispersed teams and possess cross-cultural competenceAttention to detail and high-quality deliverablesTo analyze the training needs of developers, operational teams within our client organizationAbility to monitor, evaluate and review the quality and effectiveness of training, assessment and outcomes of courses deliveredDevelopment of operating model for the above environments and facilitating handover to business and IT operations teamsIntegration of and administration of relevant technology, people and processes within relevant IT compliance, regulatory and cybersecurity requirements.Working with application teams, business teams, vendors and clients to produce, develop and review cloud and cloud security strategy. Must haves in Resume: Working experience in designing, deploying and maintaining Kubernetes.Preferably in Azure (Azure Kubernetes Service - AKS).Performance optimization, monitoring and logging for AKS clusters.Creating and maintaining container images using Docker.Hands-on terraform, Ansible and YAML.Well versed in Linux operating system. Best regards, Khursheed\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "180_Need AWS Databricks Architect at Onsite-Bostan-MA-12 Months Contract": "Venki,\nNAM INFO INC\nvenki@nam-it.com\nReply to: venki@nam-it.com\nHi Greetings from NAM Info Inc!!! Please find the below immediate requirements from my Client. Please revert me back with updated copy of resume, Visa status and contact details Note: Send resumes to Venki@nam-it.com for quick submittals. As per client policy video call discussion is mandatory before submitting profile to client Position: AWS Databricks ArchitectLocation: Onsite-Bostan-MADuration: 12 Months Contract JD:Advise on architecture and scaling strategies for Databricks environments.Set up data storage, notebooks, and compute resources to fulfill business use cases. Technical Support:Serve as a point of contact for technical issues related to Databricks.Monitor usage, report on cost management, and participate in cost planning.Develop and maintain expertise in Databricks, related technologies, and data replication products.Proficiency in Python and PySpark.Strong knowledge of SQL.Experience in building and optimizing complex data pipelines in Azure Cluster Management:Serve as a point of contact for technical issues related to the Databricks environment. Thanks & Regards...Venki RSenior Executive (Staffing)Nam Info Inc Email : Venki@nam-it.comWebsite - www.nam-it.com2525 US Highway 130, Building D, Suite 2-Cranbury, NJ-08512 \u201cThe harder you work, the luckier you get\u201d Follow us on: USA | CANADA | INDIA MBE Certified Company , E Verify Company Note: This is not an unsolicited mail. Under Bill 1618 Title III passed by the 105th USA Congress this email cannot be considered as spam as long as we include our contact information and an option to be removed from our emailing list. If you have received this message in error or, are not interested in receiving our emails, please accept our apologies. To be removed from our mailing list, please reply with the subject line \"REMOVE\". All removal requests will be honored ASAP. We sincerely apologize for any inconvenience caused to you\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "181_DFA AWS Cloud Solutions Engineer (742175)": "Vikas Sharma,\nSita Consulting Services\nvikas@scstech.us\nReply to: vikas@scstech.us\nAnticipated Start/End Date: August 1st to 8/31/2025, with possibility to extend additional years. On-Site Work OverviewDFA is migrating the Arkansas Integrate Revenue System Tax (AIRS Tax) and Driver Services/Motor Vehicle (AIRS DSMV) COTS systems from the current on premise data center to the AWS Cloud environment. The DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center. Roles and Responsibilities\u2022 Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.\u2022 Plan, design, and implementation of workload migrations from on premises to AWS Cloud.\u2022 Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.\u2022 Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.\u2022 Setup and maintain Gateway Load Balancer and Internet Gateway.\u2022 Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.\u2022 Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.\u2022 Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.\u2022 Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.\u2022 Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.\u2022 Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.\u2022 Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.\u2022 Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.\u2022 Participate in the selection of new technologies while maintaining departmental information technology and security standards.\u2022 Effectively communicate with Management, co-workers, AMS, and business partners.\u2022 Perform other duties as assigned.Skill MAtrixRequired / DesiredAmountof ExperienceHands-on experience migrating/implementing significant applications on AWS Cloud platformsRequired4Years AWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Required4YearsExperience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerRequired4Years Experience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyRequired4YearsExperience with high volume, mission critical applications and their interdependencies with other applications and databasesRequired4Years Infrastructure to Code scripting using Cloud Formation, or other languages, and AMIRequired4YearsMonitoring, maintenance, and support of the infrastructure within AWS environmentsRequired4Years Experience using monitoring solutions like CloudWatch and CloudTrailRequired2YearsExperience configuring CrowdStrike and TenableRequired2Years Experience and knowledge of AWS network-level logging configuration and managementRequired4YearsConfiguration and maintenance of AWS dashboardsRequired4Years Experience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesRequired4YearsExperience designing and implementing common shared services across enterprise applicationsRequired4Years Strong analytical skills and ability to resolve complex business/IT problemsRequired4YearsMicrosoft Windows Server System Administrator experienceRequired GenTax architecture and application maintenance experienceDesired Familiarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Highly desired Strong communication skills, both oral and writtenHighly desired Availability to provide 24 hour supportHighly desired Experience using Incident Management software solutions like JiraHighly desired AWS Solutions Architect certificationHighly desired\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "182_AWS Solution Architect": "Archana Kalyankar,\nHanker Systems\narchanak@hankersystems.com\nReply to: archanak@hankersystems.com\nHello, I am a Recruiter at Hanker Systems. I am reaching out to you on an exciting job opportunity with one of direct client.Title: AWS Cloud Solution Engineer.Location: little Rock, ARExperience: 20 plus YearsNeed only Locals.Required skillsHands-on experience migrating/implementing significant applications on AWS Cloud platformsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyExperience with high volume, mission critical applications and their interdependencies with other applications and databasesInfrastructure to Code scripting using Cloud Formation, or other languages, and AMIMonitoring, maintenance, and support of the infrastructure within AWS environmentsExperience using monitoring solutions like CloudWatch and CloudTrailExperience configuring CrowdStrike and TenableExperience and knowledge of AWS network-level logging configuration and managementConfiguration and maintenance of AWS dashboardsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesExperience designing and implementing common shared services across enterprise applicationsStrong analytical skills and ability to resolve complex business/IT problemsMicrosoft Windows Server System Administrator experienceGenTax architecture and application maintenance experienceFamiliarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Strong communication skills, both oral and writtenAvailability to provide 24 hour supportExperience using Incident Management software solutions like JiraAWS Solutions Architect certification ThanksArchanak@hankersystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "183_JD regarding AWS Cloud Solutions Engineer (Onsite)": "Pavan Kalyan,\nUnicorn Technologies\npavan@unicorntek.com\nReply to: pavan@unicorntek.com\nHiHope you are doing good,We Have Below Requirement Role : AWS Cloud Solutions Engineer (Onsite)Client : State of ArkansasOverviewDFA is migrating the Arkansas Integrate Revenue System Tax (AIRS Tax) and Driver Services/Motor Vehicle (AIRS DSMV) COTS systems from the current on premise data center to the AWS Cloud environment. The DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center.Roles and Responsibilities:\u2022 Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.\u2022 Plan, design, and implementation of workload migrations from on premises to AWS Cloud.\u2022 Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.\u2022 Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.\u2022 Setup and maintain Gateway Load Balancer and Internet Gateway.\u2022 Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.\u2022 Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.\u2022 Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.\u2022 Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.\u2022 Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.\u2022 Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.\u2022 Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.\u2022 Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.\u2022 Participate in the selection of new technologies while maintaining departmental information technology and security standards.\u2022 Effectively communicate with Management, co-workers, AMS, and business partners.\u2022 Perform other duties as assigned. Thanks and Regards, Pavan KalyanTechnical Recruiter Unicorn Technologies LLCDesk: 4708702980 EXT :- 118pavan@unicorntek.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "184_AWS Cloud Solutions Engineer - Onsite": "Raghu,\nmsys inc\nresume@msysinc.com\nReply to: resume@msysinc.com\nTitle: AWS Cloud Solutions Engineer - OnsiteLocation: Little Rock, AR, United StatesLength: Long termRestriction: W2 or C2CSend resume to : raghu@msysinc.com Description:Interview Type: Skype **** very long term project initial PO till 8/31/2025 with multiyear extensions **** *** Onsite ***Short Description:DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system wide capabilities and workflows to efficiently manage and secure these very large systems.OverviewDFA is migrating t) COTS systems from the current on premise data center to the AWS Cloud environment.The DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center.Roles and Responsibilities:Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACLs and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPCs in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, coworkers, AMS, and business partners.Perform other duties as assigned.Required Skills:Hands on experience migrating/implementing significant applications on AWS Cloud platforms 4 YearsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3 4 YearsExperience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACLs, Routing Tables, Gateway Load Balancer 4 YearsExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDuty 4 YearsExperience with high volume, mission critical applications and their interdependencies with other applications and databases 4 YearsInfrastructure to Code scripting using Cloud Formation, or other languages, and AMI 4 YearsMonitoring, maintenance, and support of the infrastructure within AWS environments 4 YearsExperience using monitoring solutions like CloudWatch and CloudTrail 2 YearsExperience configuring CrowdStrike and Tenable 2 YearsExperience and knowledge of AWS network level logging configuration and management 4 YearsConfiguration and maintenance of AWS dashboards 4 YearsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologies 4 YearsExperience designing and implementing common shared services across enterprise applications 4 YearsStrong analytical skills and ability to resolve complex business/IT problems 4 YearsMicrosoft Windows Server System Administrator experienceDesired Skills:GenTax architecture and application maintenance experienceHighly Desired Skills:Familiarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Strong communication skills, both oral and writtenAvailability to provide 24 hour supportExperience using Incident Management software solutions like JiraAWS Solutions Architect certification\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "185_urgent new requirement for  AWS Technical Lead  9800 Fredericksburg Rd, San Antonio, TX 78288  12 months": "rajesh,\ninfoways\nrajesh@info-ways.com\nReply to: rajesh@info-ways.com\nHi, Hope you are doing great. We have an urgent requirement for the below position with our IT Client HCL.Kindly reply back to this email or call me to discuss further. position : AWS Technical Leadlocation : 9800 Fredericksburg Rd, San Antonio, TX 78288duration : 12 months Mandatory required skills \" AWS Sagemaker, AWS Neptune, GCP Vertex AI Job Description: The DevOps Engineer's primary responsibility is to ensure the successful implementation and management of AI/ML services/components for USAA Enterprise. This includes AWS Sagemaker, AWS Neptune, GCP Vertex AI. High level job responsibilities:-Will work with the engineering team and the infrastructure team in order to achieve all the milestones needed for the scope of work.Work with implementation engineers to ensure platform components are working as intended for day to day use.Work with infrastructure engineers/team to ensure platform components are implemented in a manner befitting USAA's policy and guardrails.Monitor and improve AI/ML components in AWS/GCP to ensure the continued quality delivery of functions in the AI/ML space.Candidate needs to be able to independently work and be self-driven in to successfully enable and implement the AWS Sagemaker, Neptune and GCPDirectly working with a fast paced team and stakeholders ranging from engineers all the way to the EMG group.This candidate will need to assess the scope of work and be able to come up with action items/plans with minimal.Best RegardsRajeshInfoWaysPhone#: 609-858-0846Email: Visit us at: www.info-ways.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "186_Required:::::: Senior AWS Cloud Security Engineer   :::  Location::: LIttle Rock, Arkansas": "Ravi kumar,\n1Point system LLC\nravi@1pointsys.com\nReply to: ravi@1pointsys.com\nTitle:: Senior AWS Cloud Security EngineerLocation::: LIttle Rock, Arkansas We have a long term position available for a very Senior AWS Cloud Security with experience migrating Very Large systems to the Cloud. Extreme knowledge of security and government standards such as IRS Pub 1075, PII, and NIST frameworks. As well as AWS services like CloudWatch, CloudTrail, GuardDuty and Security Hub. AWS Solutions Architect certification is desirable. State government experience is a plus. Short Description:AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the Client. Roles and Responsibilities Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, co-workers, AMS, and business partners.Perform other duties as assigned. Required/Desired Skills SkillRequired /DesiredAmount of ExperienceCandidates years of experienceLast used.Hands-on experience migrating/implementing significant applications on AWS Cloud platformsRequired4 Years AWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Required4 Years Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerRequired4 Years Experience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyRequired4 Years Experience with high volume, mission critical applications and their interdependencies with other applications and databasesRequired4 Years Infrastructure to Code scripting using Cloud Formation, or other languages, and AMIRequired4 Years Monitoring, maintenance, and support of the infrastructure within AWS environmentsRequired4 Years Experience using monitoring solutions like CloudWatch and CloudTrailRequired2 Years Experience configuring CrowdStrike and TenableRequired2 Years Experience and knowledge of AWS network-level logging configuration and managementRequired4 Years Configuration and maintenance of AWS dashboardsRequired4 Years Experience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesRequired4 Years Experience designing and implementing common shared services across enterprise applicationsRequired4 Years Strong analytical skills and ability to resolve complex business/IT problemsRequired4 Years Microsoft Windows Server System Administrator experienceRequired GenTax architecture and application maintenance experienceDesired Familiarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Highly desired Strong communication skills, both oral and writtenHighly desired Availability to provide 24 hour supportHighly desired Experience using Incident Management software solutions like JiraHighly desired AWS Solutions Architect certificationHighly desired Best Regards,Ravi Kumar Recruitment Team Lead \u2013 1Point System LLCravi@1pointsys.com || www.1pointsys.com115 Stone Village Drive \u2022 Suite C \u2022 Fort Mill, SC \u2022 29708 An E-Verified company | An Equal Opportunity Employer\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "187_Hexaware; Urgently looking for Cloud Security ArchitectinReston VA (Hybrid Role)": "Anubhav,\nHMG America\nanubhav@hmgamerica.com\nReply to: anubhav@hmgamerica.com\nOne of our direct clients is looking for Cloud Security Architect in Reston VA. Below is the detailed job description.Role:- Cloud Security Architect Rate- $75/hr. C2C MaxClient: Freddie Mac Via HexawareLocation-Reston VA Day 1 onsite in hybrid set up.NOTE:- Need strong Cloud (AWS only) security and architecture experience.Job Description \u2013The Cloud Security Architect (CSA) will leverage broad technical knowledge of cloud security best practices of key public cloud offerings of providers such AWS, Azure, and GCP to establish secure design patterns, to architect integrations among cloud and/or on-premises infrastructures. This individual must be able to assist in ensuring the security and compliance of the cloud environment based on enterprise cloud security policies, standards, and procedures. The CSA will ensure that solutions operating on the cloud comply with enterprise security requirements in both off-premises and hybrid environment models. The position will work with Enterprise Architects and Application Dev teams to come up with Security Architecture for applications and enterprise tech capabilities migrating to Cloud. Must-Haves: Required qualifications to be successful in this role: 10 years of total IT experience with the following must haves: \u2022 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect \u2022 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u2022 Excellent written and communication skills to report, document and communicate security architecture \u2022 Excellent coordination skills and must be detail oriented Nice-to-Haves: \u2022 Cloud agnostic security architecture experience a plus \u2022 1-2 years of experience in working with NIST assessments of business applications \u2022 Container Security experience to protect container workloads during build and run-time \u2022 API Security architecture experience with industry standard API Gateways \u2022 Security engineering/administration background leveraging SIEM, Network firewalls, host-based security, and security configuration \u2022 One or more industry standard security certification such as CISSP, CCSP or relevant GIAC certifications (ANY) \u2022 One or more Cloud Service Providers Security Specialty Certifications such as AWS Security Specialty or Azure AZ-500 Certification \u2022 The group of skills related to Security including designing and evaluating security systems, identifying security threats, securing computers, assessing vulnerability, etc. \u2022 The group of skills related to Relationship Management including managing and engaging stakeholders, customers, and vendors, building relationship networks, contracting, etc. \u2022 Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand \u2022 The group of skills related to Risk Assessment and Management including evaluating and designing controls, conducting impact assessments, identifying control gaps, remediating risk, etc. \u2022 Experience identifying and determining levels of risk to an organization's networks and systems using cybersecurity techniques \u2022 Working with people with different functional expertise respectfully and cooperatively to work toward a common goal \u2022 Skilled in cloud technologies and cloud computing \u2022 The group of skills related to Influencing including negotiating, persuading others, facilitating meetings, and resolving conflict Key Areas of Responsibility: \u2022 Partner with Enterprise/Portfolio Architecture team and Business Units development squads to collaboratively develop security architectures/designs leveraging approved patterns that ensure applications migrating from on-premise to Cloud, achieving high standards of security practices and compliance. \u2022 Drive the development and adoption of cloud security standards, best practices, and technologies within Enterprise IT infrastructure \u2022 Liaise on security-related issues with internal business stakeholders, InfoSec, Enterprise Architecture, and application development squads \u2022 Work to develop, enhance and document security architecture, security policies, patterns, procedures, guidelines and standards required to design cloud-based solutions \u2022 Educate application, portfolio and solution architects on secure solution design and industry best security practices \u2022 Work on assessments of compliance and standards including and not limited to NIST, FedRAMP, FIPS, etc. \u2022 Support threat modeling and update application security architecture as needed. \u2022 Support application development squads with Security implementations and issues Thanks & Regards,Anubhav Jain IT RecruiterHMG America LLCanubhav@hmgamerica.comhttp://www.hmgamerica.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "188_Looking for Devops with Openshift": "Bharath,\nVy systems\nbharath.r@vysystems.com\nReply to: bharath.r@vysystems.com\nHi,Looking for Devops with openshiftJob title:Devops with openshiftLocation:Berkeley Heights,NJBuild, manage, and deploy CI/CD pipelines.Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline.Implementing various development, testing, automation tools, and IT infrastructure.Optimize and automate release/development cycles and processes.Be part of and help promote our DevOps culture.Identify and implement continuous improvements to the development practice. What you need to have:6+ years expertise with Microsoft Azure cloud-based infrastructure and services6+ years hands-on experience with CI/CD (YAML) pipelines for Azure DevOps6+years hands-on experience with K8S environment including Helm Charts & Config Maps.6+ years sound knowledge on scripting in languages such as PowerShell, bash scripting.5+ years' experience in containerization with Azure Kubernetes, Docker etc.5+ years' experience with Infrastructure as Code5+ years high proficiency with Windows-based systems and Linux-based systems Ability to write and update documentation.Demonstrate a logical, process orientated approach to problems and troubleshooting. Regards,BHARATH- Vy\u200bSystemsEmail ID:bharath.r@vysystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "189_Urgent Role ::DFA AWS Cloud Solutions Engineer :: Little Rock, AR(Onsite) ::": "Ravi singh,\nVYZE INC\nravi.singh@vyzeinc.com\nReply to: ravi.singh@vyzeinc.com\nRole: DFA AWS Cloud Solutions EngineerLocation: Little Rock, AR(Onsite)Visa: NO OPT,CPT,H1B15+ years experience Submit candidates linkedin id and visa copy.ResponsibilitiesProvide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, co-workers, AMS, and business partners.Perform other duties as assigned\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "190_Immediate Job opportunity for AWS Architect with DevOps Knowledge REMOTE": "Ravikanth,\nRohaTech LLC\nravi@rohatech.com\nReply to: ravi@rohatech.com\nDear Job Applicants,I hope this email finds you well.My name is RKSetty, and I am reaching out to share an exciting job opportunity at RohaTech.Job Title: AWS Architect with DevOps KnowledgeLocation: Remote (Future Onsite)Rate: $60 per hourClient: LPL (Financial background required)Job Type: ContractOverview: We are seeking a highly skilled AWS Architect with strong DevOps knowledge to join our team. The ideal candidate will have extensive experience in designing, implementing, and maintaining AWS infrastructure and services, coupled with deep expertise in DevOps practices and tools.Key Responsibilities:Design and implement scalable, secure, and resilient AWS infrastructure using best practices.Create and manage Terraform templates and modules for infrastructure provisioning.Develop and maintain Helm templates for Kubernetes deployments.Write Python, Shell, and Bash scripts to automate workflows and tasks.Develop and manage GitHub actions, including code scans with Sonar and AppSec tools.Implement and manage branching strategies and GitOps practices.Design and develop Python APIs and integrate them with AWS services.Architect and implement DevOps strategies and solutions, including CI/CD pipelines.Deploy and manage authentication services in AWS Lambda and EKS.Collaborate with cross-functional teams to define and implement effective solutions.Must-Have Skills:Extensive experience with core AWS services, including Gateway, Lambda, and authentication patterns.Proficiency in Terraform for infrastructure as code.Strong knowledge of Helm for Kubernetes management.Expertise in Python development and scripting (Python, Shell, Bash).Hands-on experience with GitHub, including GitHub actions and security scans (Sonar, AppSec).Deep understanding of branching strategies and GitOps principles.Experience in DevOps architecture design and strategy.Knowledge of serverless authentication services and deployment strategies in AWS Lambda and EKS.Qualifications:Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.Minimum of 5 years of experience in AWS architecture and DevOps roles.Proven experience in the financial sector is mandatory.Strong problem-solving skills and the ability to work under pressure.Excellent communication and teamwork skills.Preferred Certifications:AWS Certified Solutions ArchitectAWS Certified DevOps EngineerCertified Kubernetes Administrator (CKA)To Apply: Please send your updated resume and a cover letter outlining your relevant experience and why you are a good fit for this role to ravi@rohatech.com.If you or someone with same skills and if he/she is interested in this opportunity, please feel free to reach out to me at ravi@rohatech.com for further details.OR Refer and Earn $200 up on conformation Please provide below skill matrx Terraform Template creation ? How many Years Terraform module write up ?AWS Resources including Gateway, Lambda, Authentication pattern?Helm template?Python or Shell scripting, Bash scripting?Python development?Github, Github action includes Scans like Sonar, AppSec ?Branching strategy?GitOps?Python API development?Experience in DevOps Architecture design and strategy.?how to deploy same authentication in Lambda and EKS ?Server less authentication services ?Submission Details Full Name:- Phone Number:- Email:- Linkedin:-Current Location:- Open to Relocation:-Passport Number:- Visa Status:-Visa Proof:-Dl Proof:-DOB(Including Year):- Total Experience:-US Experience:-Relevant Experience:-Complete Education:-Availability to Join:-Rate:-Employer Details RKSetty (HE/Him)RaviRECRUITMENT MANAGER ROHATECH LLCEMAIL: Ravi@rohatech.comINDIA PHONE; 7036-0122-91USAPHONE: 602 -666-8766https://www.linkedin.com/in/ravikanth-n/https://www.linkedin.com/in/raviroha/CONFIDENTIALITY NOTICE: The contents of this email message and any attachments are intended solely for the addressee(s) and may contain confidential and/or privileged information and may be legally protected from disclosure. If you are not the intended recipient of this message or their agent, or if this message has been addressed to you in error, please immediately alert the sender by reply email and then delete this message and any attachments. If you are not the intended recipient, you are hereby notified that any use, dissemination, copying, or storage of this message or its attachments is strictly prohibited.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "191_Remote DevOps Gitlab Architect at Remote (Ask is to support EST timezone)": "Sumit Kashyap,\nHMG America LLC\nsumit@hmgamerica.com\nReply to: sumit@hmgamerica.com\nTitle: Senior Gitlab Consultant OR ArchitectLocation: Remote (Ask is to support EST timezone)Skills: Gitlab in depth knowledge,Administration,Configuration,CI/CD knowledge,Linux (please refer to JD below)Job Description:GitLab Central Team We are DevOps team which design and maintain GitLab environment for external customer. It s a central SCM platform with a few thousands active users. In our line of work, we do AO, support and development of integration with customer s ecosystem. Team is scattered across globe and consists with tree main time zones (Practicing Follow-the-sun model).Senior position requirement:Education: Bachelor or Master degree in IT Or a very strong capability to show his advanced skills in previously mentioned key competencies and left an impression during the interview. GitLab Gitlab in depth knowledge Administration Configuration CI/CD knowledge General key competencies: English (capability express themselves, capability to discuss various technical topics) Be autonomous On-call duty AO related key competencies: Linux Automation (Ansible) Monitoring (Grafana, Prometheus) Docker DevOps key competencies: General SCM knowledge Git CI/CD knowledge GitLab Python (optional Java) Optional competencies: HAProxy Nginx Redis --Thanks & RegardsSumit KashyapHMG AMERICA LLCE: sumit@hmgamerica.comDirect number - 7327905639Board number - 7327905001, EXT. 117W: www.hmgamerica.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "192_Sr AWS Cloud Engineer - 15+ MUST  NEED LOCALS TO Little Rock, AR- VISA VISA USC GC GCEAD H4": "deepak,\nAdifice Technologies\ndeepak@adificeusa.com\nReply to: deepak@adificeusa.com\nTitle: Senior AWS Cloud EngineerLocation: Little Rock, AR ONSITE Day 1Duration: 12 months+C2CEnd Client - State of Arkansas Need 15+ years of experience is must Job Description:AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems. The AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the Service Center.Roles and Responsibilities\u2022 Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.\u2022 Plan, design, and implementation of workload migrations from on premises to AWS Cloud.\u2022 Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.\u2022 Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.\u2022 Setup and maintain Gateway Load Balancer and Internet Gateway.\u2022 Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.\u2022 Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.\u2022 Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.\u2022 Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.\u2022 Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.\u2022 Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.\u2022 Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.\u2022 Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.\u2022 Participate in the selection of new technologies while maintaining departmental information technology and security standards.\u2022 Effectively communicate with Management, co-workers, AMS, and business partners.\u2022 Perform other duties as assigned. Required SkillsHands-on experience migrating/implementing significant applications on AWS Cloud platformsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyExperience with high volume, mission critical applications and their interdependencies with other applications and databasesInfrastructure to Code scripting using Cloud Formation, or other languages, and AMIMonitoring, maintenance, and support of the infrastructure within AWS environmentsExperience using monitoring solutions like CloudWatch and CloudTrailExperience configuring CrowdStrike and TenableExperience and knowledge of AWS network-level logging configuration and managementConfiguration and maintenance of AWS dashboardsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesExperience designing and implementing common shared services across enterprise applicationsStrong analytical skills and ability to resolve complex business/IT problemsMicrosoft Windows Server System Administrator experienceGenTax architecture and application maintenance experienceFamiliarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Strong communication skills, both oral and writtenAvailability to provide 24 hour supportExperience using Incident Management software solutions like JiraAWS Solutions Architect certification\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "193_AWS developer with App Synch and Typescript developer": "Ashwani Raghuvanshi,\nTek Inspirations\nashwani.raghuvanshi@tekinspirations.com\nReply to: ashwani.raghuvanshi@tekinspirations.com\nJob Description -Interviewer will be Indian so please submit Accordingly Role: AWS App Synch/Typescript developerClient: Cox Communications(Capgemini) (Candidate must have telecommunications client experience)Duration: 6 months (Remote)Location: Atlanta, GA ( Local candidates will be highly preferred)No H1 CPTPhone/SkypeNeed Candidate Linkedin and having a excellent communications skills Responsibilities:\u2022 Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.\u2022 Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.\u2022 Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.\u2022 Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.\u2022 Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.\u2022 Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.\u2022 Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.\u2022 Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.\u2022 Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.\u2022 Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.\u2022 Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.\u2022 Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.Requirements:\u2022 Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.\u2022 Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.\u2022 Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.\u2022 Experience coding in Swift is a plus.\u2022 Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.\u2022 Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.\u2022 Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.\u2022 Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.\u2022 Familiarity with Charles Proxy or similar tools for network analysis and debugging.\u2022 Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.\u2022 Experience with CI/CD pipelines and DevOps practices is a plus.\u2022 Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.\u2022 Strong documentation skills to maintain clear and concise technical documentation.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "194_AWS Technical Lead San Antonio TX Onsite C2H": "Malavika,\nYochana\nmalavika@yochana.com\nReply to: malavika@yochana.com\nHi,This is Malavika from Yochana IT Solutions, I Recently found your resume in one of our Job portal and We are looking for \u201cAWS Lead (Onsite)\u201dwith one of our client, I have included the job information below, If you are interested, please share your updated resume.Job Title: AWS LEAD (TECHNICAL LEAD)Position Type: CTH Location: San Antonio, TX Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the details AWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration USAA Experience If Any - (Yes/No) - Eagle ID \u2013 Reporting Manager Project/Area of Work Vendor/Implementation Total Experience USAA (WHEN-WHEN) Thanks & Regards,Malavika,Resource Specialist,Yochana IT Solutions INC,Farmington Hills, MI,malavika@yochana.com Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program.Note: If you are not interested in receiving our e-mails then please reply with subject line \u201cRemove\u201d.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "195_AWS Cloud solution Engineer 15+ year of experience candidates only with AWS Solution Architect Certification": "Rosalin Palo,\nGLOBAL IT FAMILY LLC\nrosalin@globalitfamily.com\nReply to: rosalin@globalitfamily.com\nRequisition Class: SARV6 : SAR2 : A1 : SC3Region Name: State of ArkansasTitle/Role: DFA AWS Cloud Solutions Engineer (742175)Start Date: 08/01/2024End Date: 06/30/2025Worksite Address: 1515 W. 7th St., Room 503, Little Rock, AR 72201Work Arrangement: Onsite DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems. Anticipated Start/End Date: August 1st to 8/31/2025, with possibility to extend additional years. On-Site Work OverviewDFA is migrating the Arkansas Integrate Revenue System Tax (AIRS Tax) and Driver Services/Motor Vehicle (AIRS DSMV) COTS systems from the current on premise data center to the AWS Cloud environment. The DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center. Roles and Responsibilities\u2022 Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.\u2022 Plan, design, and implementation of workload migrations from on premises to AWS Cloud.\u2022 Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.\u2022 Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.\u2022 Setup and maintain Gateway Load Balancer and Internet Gateway.\u2022 Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.\u2022 Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.\u2022 Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.\u2022 Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.\u2022 Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.\u2022 Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.\u2022 Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.\u2022 Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.\u2022 Participate in the selection of new technologies while maintaining departmental information technology and security standards.\u2022 Effectively communicate with Management, co-workers, AMS, and business partners.\u2022 Perform other duties as assigned. SkillRequired / DesiredAmountof ExperienceHands-on experience migrating/implementing significant applications on AWS Cloud platformsRequired4YearsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Required4YearsExperience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerRequired4YearsExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyRequired4YearsExperience with high volume, mission critical applications and their interdependencies with other applications and databasesRequired4YearsInfrastructure to Code scripting using Cloud Formation, or other languages, and AMIRequired4YearsMonitoring, maintenance, and support of the infrastructure within AWS environmentsRequired4YearsExperience using monitoring solutions like CloudWatch and CloudTrailRequired2YearsExperience configuring CrowdStrike and TenableRequired2YearsExperience and knowledge of AWS network-level logging configuration and managementRequired4YearsConfiguration and maintenance of AWS dashboardsRequired4YearsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesRequired4YearsExperience designing and implementing common shared services across enterprise applicationsRequired4YearsStrong analytical skills and ability to resolve complex business/IT problemsRequired4YearsMicrosoft Windows Server System Administrator experienceRequired GenTax architecture and application maintenance experienceDesired Familiarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Highly desired Strong communication skills, both oral and writtenHighly desired Availability to provide 24 hour supportHighly desired Experience using Incident Management software solutions like JiraHighly desired AWS Solutions Architect certificationHighly desired Question 4ON-SITE Work Only. If your candidate is not local is he/she ready to relocate to the Little Rock, Arkansas area to begin work Day 1?Question 5What city and state is your candidate currently located?Thanks & Regards,Rosalin PaloGlobal IT FamilyEmail: rosalin@globalitfamily.com www.globalitfamily.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "196_DevOps Architect :: Remote": "NIGESH,\nERP MARK\nnigesh@erpmark.com\nReply to: nigesh@erpmark.com\nHiWe have priority Job requirement from our Direct customer and here I am checking your interest/ Availability for the same. Please review.Role: DevOps ArchitectLocation: Remote Authentication( how a API key or Lambda authorizer setup enables authentication)EKS , Scripting (Python) , Lambda Authorize, Terraform Terraform Template creationTerraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication services\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "197_Role: AWS Architect (AWS expert + Python)": "tejasree,\nITBrainiac.Inc\ntejasree@itbtalent.com\nReply to: tejasree@itbtalent.com\nRole: AWS Architect (AWS expert + Python)Location: Charlotte ( 5 day onsite) \u2013 local or near by1.Terraform Template creation2.Terraform module write up3.AWS Resources including Gateway, Lambda, Authentication pattern4.Helm template5.Python or Shell scripting, Bash scripting6.Python development7.Github, Github action includes Scans like Sonar, AppSec 8.Branching strategy9.GitOps10.Python API development11.Experience in DevOps Architecture design and strategy.12.how deploy same authentication in Lambda and EKS13.Server less authentication servicesRegards:Tejasree Echuru \u2013 Sr. IT RecruiterITBrainiac IncPrinceton Forrestal Village116 Village Blvd, Suite 200 Princeton, NJ 08540| New Jersey-08540EMAIL : tejasree@itbtalent.comWebsite : www.itbrainiac.com LinkedIn URL: https://www.linkedin.com/in/echuru-tejasree/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "198_Cloud Engineer": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Cloud EngineerDesign and implement scalable, secure, and reliable cloud infrastructure solutions. Develop architectural blueprints and detailed documentation for cloud environments. Deploy, manage, and maintain cloud environments (e.g., AWS, Google Cloud, Azure). Monitor cloud infrastructure and services to ensure optimal performance and availability. Develop and maintain automation scripts using tools like Terraform, Ansible, or CloudFormation. Automate deployment, scaling, and management of cloud resources. Implement and manage security best practices for cloud environments. Ensure compliance with industry standards and regulatory requirements. Monitor and optimize cloud expenditures to ensure cost-effectiveness. Work closely with development, operations, and security teams to ensure seamless integration and operation of cloud services. Provide technical support and guidance for cloud-related issues. Stay up-to-date with the latest cloud technologies and industry trends. Propose and implement improvements to enhance the efficiency and reliability of cloud infrastructure. Required Skills and Qualifications: Proficiency with cloud platforms such as AWS, Google Cloud, or Azure. Strong experience with infrastructure-as-code tools like Terraform, CloudFormation, or Ansible. Knowledge of networking concepts, including VPCs, subnets, security groups, and VPNs. Familiarity with containerization and orchestration tools like Docker and Kubernetes. Engineering, or a related field. Proven experience as a Cloud Engineer or in a similar role. Experience in cloud architecture, deployment, and management. Preferred Qualifications: Relevant certifications such as AWS Certified Solutions Architect, Google Cloud Professional Cloud Architect, or Microsoft Certified: Azure Solutions Architect Expert. Experience with DevOps practices and tools. Knowledge of scripting languages like Python, Bash, or PowerShell. Educational Background: Bachelor\u2019s or Master\u2019s degree in Computer Science, Information Technology.Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "199_AWS Data Application Infrastructure Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: AWS Data Application Infrastructure Lead - OnsiteWork location:ATLANTA (US:30301), GAClient : CapgeminiDESCRIPTIONManage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience in Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship management\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "200_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "201_Urgent position of AWS Architect (Enterprise Architect_) in San Antonio TX": "Shashank,\nIDC Technologies\nshashank.singh@idctechnologies.com\nReply to: shashank.singh@idctechnologies.com\nJob Title: AWS Architect (Enterprise Architect_)Location: San Antonio, TX Mandatory Skillset Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "202_C2C  -- AWS Cloud Technical Architect -- San Antonio TX": "Pinki Kumari,\nIntellicept Corporation\npinki@intellicept.com\nReply to: pinki@intellicept.com\nAWS Cloud Technical ArchitectC2CSan Antonio TX Job OverviewA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.Responsibilities\u00b7Own and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions \u00b7Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. \u00b7Provide data architectural solutions/designs to project execution teams for implementation. \u00b7Provide architectural assessments, strategies, and roadmaps for data management. \u00b7Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. \u00b7Project and solution estimation and team structure definition \u00b7Develop Proof-of-Concept projects to validate new architectures and solutions. \u00b7Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. \u00b7Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. \u00b7Mentor and provide architectural guidance to multiple teams building innovative applications. \u00b7Drive common vision, practices and capabilities across teams. \u00b7Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions \u00b7Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs \u00b7Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change Thanks & RegardsPinki KumariTalent AdvisorEmail: pinki@intellicept.comCell:- (848) 999-4581Intellicept Corporation\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "203_Senior Cloud Engineer GCP": "rahul sharma,\n3B Staffing LLC\nrahul.sharma@3bstaffing.com\nReply to: rahul.sharma@3bstaffing.com\nHiRole- Senior Cloud Engineer (GCP)Location \u2013 RemoteMode- ContractVisa \u2013 NO H1B ,OPT,CPT Job Description :: Responsibilities/DutiesWork with Back-End & Front-End developers to support integration and deployment effortsDevelop and maintain reusable templates and code to automate solution delivery and deploymentEnsure the availability, performance, security, and scalability of the production system by implementing proactive system monitoringProfessional work experience in GCP administration with strong fundamentals in computing, networking, security, and storageWrite readable, maintainable infrastructure as code with minimal technical oversightCommunicate with Collinwood's client employees and clients about task status, availability, and questions about technology RequiredBachelor's Degree or equivalent experience5+ years of professional experience with GCPMust have a professional GCP Certification2 years of experience with TerraformExperience with CI/CD automationExperience with ETL/ELT on the GCP platformExperience deploying web applications to GCPExperience with GCPData EncryptionNetworking Security and FirewallsIAMLogging and MonitoringKMSExperience with DockerExperience working in a distributed, agile environment planning and executing on projects using Agile methodologiesConfident with use of Git for version controlExperience with Dataflow, BigQuery, and Looker PreferredExperience with BigDataExperience with DatabricksGCP Data Loss Prevention (DLP)2 years of experience with Azure or AWSExperience in database architecture, business intelligence, big data, machine learning, and/or advanced analytics\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "204_AWS App Synch | Typescript developer || Atlanta GA": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nJob Description -AWS App Synch/Typescript developer6 monthsAtlanta, GA ( Local candidates will be highly preferred)RemoteNo H1 CPTPhone/SkypeNeed Candidate Linkedin and having a excellent communications skillsResponsibilities:\u2022 Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.\u2022 Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.\u2022 Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.\u2022 Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.\u2022 Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.\u2022 Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.\u2022 Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.\u2022 Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.\u2022 Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.\u2022 Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.\u2022 Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.\u2022 Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.Requirements:\u2022 Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.\u2022 Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.\u2022 Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.\u2022 Experience coding in Swift is a plus.\u2022 Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.\u2022 Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.\u2022 Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.\u2022 Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.\u2022 Familiarity with Charles Proxy or similar tools for network analysis and debugging.\u2022 Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.\u2022 Experience with CI/CD pipelines and DevOps practices is a plus.\u2022 Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.\u2022 Strong documentation skills to maintain clear and concise technical documentation.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "205_Immediate Need - AWS Architect (AWS expert + Python) - Start ASAP": "Michael Karoff,\nITBrainiac Inc\nmkaroff@itbtalent.com\nReply to: mkaroff@itbtalent.com\nRole: AWS Architect (AWS expert + Python)Location: Charlotte ( 5 day onsite) \u2013 local or near byContract Max Rate: 60-70 Hr1. Terraform Template creation2. Terraform module write up3. AWS Resources including Gateway, Lambda, Authentication pattern4. Helm template5. Python or Shell scripting, Bash scripting6. Python development7. Github, Github action includes Scans like Sonar, AppSec 8. Branching strategy9. GitOps10. Python API development11. Experience in DevOps Architecture design and strategy.12. how deploy same authentication in Lambda and EKS13. Server less authentication servicesRegards,Michael Karoff - Delivery Manager ITBrainiac IncPrinceton Forrestal Village116 Village Blvd, Suite 200Princeton, NJ 08540Email ID: mkaroff@itbtalent.comwww.itbrainiac.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "206_Azure Cloud Architect ||Remote (part time) || weekly: 10hrs": "Dinesh T,\nDminds\ndinesht@dminds.com\nReply to: dinesht@dminds.com\nJob Title: Azure Cloud ArchitectLocation: Remote (part time)weekely: 10hrsRate: 25/hr on c2c maxDescription:Technical CapacityArchitect and design Azure environments, infrastructure, and services that enable migration of IT infrastructure, services, applications, and systems from an on-prem environment to AzureCollaborates with IT security teams to implement and manage cloud security policies, standards, and best practicesDevelop and implement scalable and resilient cloud architecture solutions within an Azure Gov Cloud environmentDevelop and implement migration strategies for Microsoft Office business tools and solutions from local on premises hosted environment to Azure cloud tenantsArchitect and administer Microsoft Entra IDCreate and implement a migration plan for each infrastructure service, system, and/or application that will be transitioned to AzureCreate architecture and data flow diagrams for hybrid cloud environmentsDesign and deploy infrastructure as code (IaC) using tools like Terraform, Azure Resource Manager (ARM) templates, and PowerShellEnsure systems, applications, and data are high availability, backed up, and/or replicated to meet organizational requirements for disaster and business recoveryAnalyze and ensure that proper monitoring and alerting systems are in place for systems, services, and applicationsEvaluate and recommend Azure services based on business requirements and industry best practicesCollaborate with software developers, system administrators, and other stakeholders to integrate Azure solutions into existing systems and applicationsEnsure seamless interoperability between on-premises and cloud environmentsImplement and enforce security requirements to protect Azure-based systems and dataDefines and maintains Azure Governance policies including Subscription Management, Cost Management, Security, Resource Consistency, Identity Baseline, Deployment Acceleration, etcEnsures compliance of architectural and engineering policies, standards, and proceduresStays current with emerging cloud technologies and trends and advises on the adoption of new Azure features and servicesWorks closely with development teams to support DevOps practices and implement continuous integration and continuous deployment (CI/CD) pipelinesLeads technical discussions and presentations for internal teams and stakeholdersRecommends strategies to streamline systems for effectiveness and efficiency, considering client needs, team dynamics, and division and corporate missions. Serves as a quality control and assurance officer for guidance and outputs to ensure consistently high standardsMaintains and expands technical knowledge in communities of practice, knowledge management, and other technical interests by attending educational workshops, reviewing professional publications, establishing personal networks, and participating in professional societies and Chemonics practice networksContributes to the expansion of Chemonics\u2019 knowledge management system (Tech Hub)Consistently produces work products in conformance with Chemonics\u2019 and clients\u2019 standardsApproaches problem solving collectively with senior staff and internal and external clients to achieve a mutually beneficial resultManagement and LeadershipParticipates in the development of the department\u2019s strategic plans, training materials and toolsEffectively trains and mentors staff on Chemonics tools and systemsFacilitates meetings and is responsible for developing agendas to achieve stated group goals, documenting progress, agreements, and issues and providing follow up assistance to groupsDemonstrates leadership, versatility, and integrityPerforms other duties and responsibilities as required Requirements:To perform this job successfully, an individual must be able to perform each essential duty and responsibility satisfactorily. The qualifications listed below are representative of the required knowledge, skills, and/or abilities needed to perform the principal duties.Bachelor\u2019s degree in computer science or related field, or equivalent work experienceMinimum 8-10 years\u2019 experience in enterprise cloud architecture in medium to large companiesProfessional certification in Azure, such as Azure Solutions Architect Expert or similar preferredStrong understanding of cloud computing technologies, business drivers, and emerging computing trendsProficient in Azure services, including but not limited to Azure Active Directory, Azure Virtual Machines, Azure App Services, Azure Kubernetes Service, Azure Purview, Azure Private Link/Private Endpoint, and Azure SQL DatabaseExperience with cloud security, networking, and disaster recovery best practicesStrong knowledge of infrastructure as code (IaC) tools such as Azure Resource Manager (ARM) templates or TerraformExperience working with Microsoft Windows Server 2016/2019/2022Experience with network security best practices and configurationsStrong troubleshooting skills and attention to detailStrong written and verbal communication skillsKnowledge of company\u2019s main client and its operations preferredAbility to solve technical, managerial, or operational problems and evaluate options based on relevant information, resources, well-rounded experience, and knowledgeExperience living or working in developing countries a plusDemonstrated ability to communicate clearly and concisely, both orally and in writing, and lead presentations, training courses, and effective meetingsSpanish or French language capability preferredDemonstrated leadership, versatility and integrity\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "207_Role AWS Architect Onsite Charlotte NC  Contract C2C  W2": "Junaid,\nITBrainiac Inc\nsyed.junaid@itbtalent.com\nReply to: syed.junaid@itbtalent.com\nJob Description:Title : AWS Architect Location : Charlotte NC Onsite 5 Day\u2019sContract : Long Term Terraform Template creationTerraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how deploy same authentication in Lambda and EKSServer less authentication servicesRegards,Syed Junaid - IT RecruiterITBrainiac IncPrinceton Forrestal Village116 Village Blvd, Suite 200Princeton, NJ 08540Email ID: syed.junaid@itbtalent.comPhone # 609-357-9408 & Ext -103www.itbrainiac.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "208_AWSTechnical Lead": "Bhavya Rapelii,\nAvance\nbhavya.r@avanceservices.com\nReply to: bhavya.r@avanceservices.com\nAWS Technical LeadSan Antonio,Texas,Onsite Skills: -AWS Sagemaker, AWS Neptune, GCP Vertex AI Job Description: The DevOps Engineer's primary responsibility is to ensure the successful implementation and management of AI/ML services/components for USAA Enterprise. This includes AWS Sagemaker, AWS Neptune, GCP Vertex AI. High level job responsibilities:-Will work with the engineering team and the infrastructure team in order to achieve all the milestones needed for the scope of work.Work with implementation engineers to ensure platform components are working as intended for day to day use.Work with infrastructure engineers/team to ensure platform components are implemented in a manner befitting USAA's policy and guardrails.Monitor and improve AI/ML components in AWS/GCP to ensure the continued quality delivery of functions in the AI/ML space.Candidate needs to be able to independently work and be self-driven in to successfully enable and implement the AWS Sagemaker, Neptune and GCPDirectly working with a fast paced team and stakeholders ranging from engineers all the way to the EMG group.This candidate will need to assess the scope of work and be able to come up with action items/plans with minimal.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "209_Looking for  AWS Cloud Architect": "Naresh,\nGITF\nnaresh@globalitfamily.com\nReply to: naresh@globalitfamily.com\nTitle: AWS Cloud ArchitectLocation : Little Rock, ARDuration : Long termResponsibilities:Hands-on experience migrating/implementing significant applications on AWS Cloud platformsAWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerExperience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyExperience with high volume, mission critical applications and their interdependencies with other applications and databasesInfrastructure to Code scripting using Cloud Formation, or other languages, and AMIMonitoring, maintenance, and support of the infrastructure within AWS environmentsExperience using monitoring solutions like CloudWatch and CloudTrailExperience configuring CrowdStrike and TenableExperience and knowledge of AWS network-level logging configuration and managementConfiguration and maintenance of AWS dashboardsExperience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesExperience designing and implementing common shared services across enterprise applicationsStrong analytical skills and ability to resolve complex business/IT problemsMicrosoft Windows Server System Administrator experience GenTax architecture and application maintenance experienceFamiliarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Strong communication skills, both oral and writtenAvailability to provide 24 hour support Experience using Incident Management software solutions like Jira AWS Solutions Architect certificationThanks & Regards,NareshGlobal IT FamilyEmail: naresh@globalitfamily.comhttp://globalitfamily.com/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "210_NEED -   AWS Role with Sagemaker, Neptune,  GCP, Vertex -San Antonio, TX 78288": "Ayushi,\nCygnuspro\nayushi@cygnuspro.com\nReply to: ayushi@cygnuspro.com\nHi,This is Ayushi Kaushik from Cygnus Professionals. I have an urgent job requirement matching your profile. If You are Interested, Please reply with updated resume at ayushi@cygnuspro.com or directly reach me Role: AWS Role with Sagemaker, Neptune, GCP, Vertex Location: San Antonio, TX 78288 Duration: Long Term Contract JD : Job Description: The DevOps Engineer's primary responsibility is to ensure the successful implementation and management of AI/ML services/components for USAA Enterprise. This includes AWS Sagemaker, AWS Neptune, GCP Vertex AI. High level job responsibilities:-Will work with the engineering team and the infrastructure team in order to achieve all the milestones needed for the scope of work.Work with implementation engineers to ensure platform components are working as intended for day to day use.Work with infrastructure engineers/team to ensure platform components are implemented in a manner befitting USAA's policy and guardrails.Monitor and improve AI/ML components in AWS/GCP to ensure the continued quality delivery of functions in the AI/ML space.Candidate needs to be able to independently work and be self-driven in to successfully enable and implement the AWS Sagemaker, Neptune and GCPDirectly working with a fast paced team and stakeholders ranging from engineers all the way to the EMG group.This candidate will need to assess the scope of work and be able to come up with action items/plans with minimal. Thanks & RegardsAyushi KaushikCygnus Professionals3490 US Highway 1Princeton, NJ 08540 E: ayushi@cygnuspro.comLinkedIn: https://www.linkedin.com/in/ayushi-kaushik-032706217\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "211_AWS Architect - Enterprise level-- (P&amp;C Domain)": "Bhavya Rapelii,\nAvance\nbhavya.r@avanceservices.com\nReply to: bhavya.r@avanceservices.com\nAWS Architect - Enterprise level-- (P&C Domain)San Antonio,Texas,Onsite Skills:AWS ARCHITECT Api integration IAASJob Description Mandatory SkillsetOverview:Looking for a talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidates will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidates will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidates will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "212_Cloud Security Engineers -Frisco, TX  Day 1 ONSITE": "Stutie Sharrma,\nResource Logistics\nstuti@resource-logistics.com\nReply to: stuti@resource-logistics.com\nRole: - Cloud Security Engineers Location: - Frisco, TX \u2013 Day 1 ONSITECloud Security Engineer Key Responsibilities\u00b7 Develop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.\u00b7 Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.\u00b7 Implement and enforce security controls to protect sensitive data and systems.\u00b7 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.\u00b7 Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.\u00b7 Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. Qualifications\u00b7 Experience with onboarding and provisioning in cloud environments.\u00b7 Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.\u00b7 In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.\u00b7 Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.\u00b7 Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.\u00b7 Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.\u00b7 Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.\u00b7 Proficiency in scripting and automation tools (e.g., Python, Terraform).\u00b7 understands onboarding and will be doing onboarding provisioning work on AWS and Azure\u00b7 Strong communication and documentation skills for collaborating with cross-functional teams.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "213_10+ Azure Devops at Chicago, IL  Only share local candidate": "rahul,\nK&K Global Talent and Solutions Inc.\nrahul.k@knkglobaltalents.com\nReply to: rahul.k@knkglobaltalents.com\nHi ,I hope you are doing great! I am a Lead Recruitment Consultant and I help top IT Tech Consulting companies develop their teams.Please find below job description and let me know your ThoughtsJob Title: Azure Devops Location: Chicago, ILContract Develop IaC for public cloud infrastructure with Terraform scripts using GitHub Actions.- Develop and maintain Github Actions CI/CD pipelines to deploy applications in public cloud.- Familiarity with build and package tools like Gradle would be preferred- Collaborate with cross-functional teams to define, design, and ship new features. - Ensure the performance, quality and security of applications scanning via different tools in the pipelines. - Help maintain code quality, organization, and automatization.#### Requirements: - Bachelor\u2019s degree in Computer Science, Engineering, or related field. - Proven experience as a Azure DevOps Engineer. - Strong proficiency in Terraform, Github and Github Actions.- Strong proficiency in Network Security and best practices in networking. - Proficiency in scripting languages like - Python, Shell/Bash/PowerShell.- Hands-on experience with containerization technologies like Docker and orchestration tools. - Extensive experience with Microsoft Azure services (e.g., Azure DevOps, Azure Container Apps, Azure Functions, etc.). - Familiarity with CI/CD pipelines and tools, preferably Github Actions.- Familiarity with APM tools such as Azure Monitor, Grafana, ELK stack, Prometheus etc.- Excellent problem-solving skills and attention to detail. - Strong communication and collaboration skills. #### Preferred Qualifications: - Strong proficiency in Microsoft Azure and Terraform for IaC along with Github Actions.- Certification in Microsoft Azure or other relevant technologies.- Knowledge of monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack).- Good to have knowledge on configuration tools like Puppet, Chef or Ansible. - Familiarity with Agile/Scrum methodologies. - Good to have experience with other cloud providers (AWS, Google Cloud).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "214_Python AWS Developer": "Aman,\nResource Logistics\naman@resource-logistics.com\nReply to: aman@resource-logistics.com\nPython AWS DeveloperChicago, IL- HybridFulltime Permanent We are seeking a skilled AWS & Python Developer to join our dynamic team. The ideal candidate should possess a strong background in both Amazon Web Services (AWS) and Python development, with a focus on creating scalable and efficient solutions for our organization. The role involves designing, developing, and maintaining cloud-based applications while leveraging AWS services and implementing Python code for various functionalities.Key Responsibilities:AWS Cloud Development:Design, implement, and deploy scalable, reliable, and secure cloud-based applications on AWS.Utilize a variety of AWS services such as EC2, S3, Lambda, RDS, and others to meet project requirements.Collaborate with cross-functional teams to define and implement AWS architecture best practices.Python Development:Develop and maintain Python applications, scripts, and utilities to support business processes and application functionalities.Write efficient, reusable, and modular code to meet project requirements.Implement and maintain APIs using Python frameworks like Flask or Django.Integration and Automation:Integrate AWS services with existing systems and applications.Develop automation scripts and tools to streamline deployment, monitoring, and maintenance processes.Implement CI/CD pipelines for continuous integration and deployment.Database Management:Work with various databases, including relational databases and NoSQL databases (e.g., DynamoDB).Design and optimize database schemas to ensure performance and scalability.Monitoring and Troubleshooting:Set up monitoring and logging for applications and infrastructure on AWS.Identify and resolve issues, troubleshoot performance bottlenecks, and optimize system performance. Qualifications:Bachelor's degree in Computer Science, Information Technology, or a related field.Experience developing UI using AngularJS or ReactJS, CSS, HTML & JavascriptProven experience as a Python Developer with a strong understanding of AWS services.Hands-on experience with cloud-based application development and deployment on AWS.Familiarity with containerization technologies such as Docker and orchestration tools like Kubernetes.Proficiency in using version control systems (e.g., Git).Solid understanding of software development principles, best practices, and design patterns. Preferred Skills:Experience with serverless computing using AWS Lambda.Knowledge of infrastructure as code tools such as Terraform or AWS CloudFormation.Familiarity with DevOps practices and tools. Personal Attributes:Strong problem-solving and analytical skills.Excellent communication and collaboration abilities.Ability to work independently and as part of a team.Commitment to continuous learning and staying updated on industry trends.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "215_Azure ML Architect in Santa Clara CA On Site": "sanjeev,\nSaransh, Inc.\nsanjeev.m@saranshinc.com\nReply to: sanjeev.m@saranshinc.com\nHi,Please let me know if you have any profiles for this roleRole: Azure ML Architect Location: Santa Clara, CA(Onsite) Need localsVisa :: H1B, H4EAD, L2,L1, E3Note : Azure ML Architect with experience on Data Bricks Development platform.Prior Experience \u2022 Hands on experience on working in Azure cloud platform \u2022 Experience in designing and implementing scalable and secure Azure cloud solutions using Data Bricks ML platform. \u2022 Understanding of implementation of architecture within Azure Big Data Analytics Al tools. \u2022 Expert level in Designing and Architect solutions in Azure Data factory, Azure Databricks, Azure Datalake, Delta Lake and Azure synapse analytics implementation. \u2022 Good to have Experience in Azure cloud technologies like PySpark, Synapse, ADF, Databricks, Python, Scala and SQL. \u2022 Have good experience configuring Microservices using Docker, Kubernetes on Azure Data Bricks \u2022 Extensive Experience on working on Azure AI services including Data Bricks and Azure cognitive services \u2022 Experience in creating and deploying code libraries using functions and classes in Python in AI product-focused development. \u2022 Well-versed in software development and code quality best industry practices. \u2022 Experience working with Agile Development methodologies. Responsibilities: \u2022 Primary responsibility is to Architect, design and develop AI application on Azure cloud and Databricks. \u2022 Lead the azure cloud architecture design for various applications depending on business requirements \u2022 Hands-on experience in one or all the following areas: LLM, NLP, DL (Deep Learning), ML (Machine Learning), object detection/classification, tracking, etc. \u2022 Work with business users to translate functional requirements into data specifications \u2022 Collaborate with data scientists and business users to transform domain know-how into functional requirements. \u2022 Model Training and Evaluation: Train and fine-tune language models using appropriate machine-learning frameworks and tools. \u2022 Research and Innovation: Stay up to date with the latest advancements in the field of natural language processing and machine learning. Collaboration and Communication: \u2022 Collaborate effectively with cross-functional teams, including researchers, DevOps architects/engineers and project managers. \u2022 Documentation and Knowledge Sharing: Document code, algorithms, and processes to ensure effective knowledge transfer and maintain codebase quality. Requirements: \u2022 B.E./ B. Tech / M. Tech/ MCA (Computer Science/Electronics & Communication/Electrical), a master's in computer science, artificial intelligence, or a related field, or a specialization in natural language processing is preferred. \u2022 4-6 years of IT experience with a min of 3+ years in Azure Data Bricks for development and Azure Services. \u2022 Strong Expertise in the Azure cloud solutions. \u2022 Strong programming skills in Python. \u2022 Familiarity with statistical, ML and DL techniques and models \u2022 Hands-on experience in one or all the following areas: LLM, NLP, DL (Deep Learning), ML (Machine Learning), object detection/classification, tracking, etc. \u2022 Experience with ML / DL frameworks (e.g., Sklearn, TensorFlow, PyTorch) is needed. \u2022 Experience with large-scale data processing, including data collection, cleaning, and preprocessing. \u2022 Proficiency in working with UNIX/Linux environments and command-line tools. \u2022 Strong analytical and problem-solving abilities. \u2022 Excellent communication skills, interpersonal, written, visual (presentations), and verbal. Certifications \u2022 Databricks - Machine learning professional (highly preferred) Questionnaire : 1. Years of experience in Databricks Machine Learning 2. Have you deployed any models using Databricks model serving 3. Have you deployed a model with unstructured data using Databricks model serving Sanjeev MSaransh, Inc.,5 Independence Way, Suite # 225,Princeton, NJ 08540Sanjeev.m@saranshinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "216_DevOps Engineer": "Ashwani Raghuvanshi,\nTek Inspirations\nashwani.raghuvanshi@tekinspirations.com\nReply to: ashwani.raghuvanshi@tekinspirations.com\nJob Description -Title \u2013 DevOps Engineer Location \u2013 Hybrid \u2013 in Manassas, VA 3 days per week onsiteDuration \u2013 6+ months Contract MOI: Phone then VideoPay rate - $55/hr.Visa: No H1B/CPTClient: Swift LinkedIn should be 2 years old The Swift Manager is asking that you provide him with a very brief summary, 1 or 2 paragraphs only, (NO BULLET POINTS)Visa copy, DL copy Related Certificate copyOne reporting manager references with their official email ID & their Linkedin profiledescribing the goal or goals for each project listed below, and what specifically was your primary role and/or contribution for each project listed below, he is ready to Interview you immediately, Required Technical Skills and Knowledge:We are seeking a skilled DevOps Engineer with a strong background in DevOps practices, CI/CD pipeline creation, optimization, and process automation. The ideal candidate will have a deep understanding of Linux operating systems, Jenkins, Docker, CloudBees, and Python programming. Additionally, this role requires the candidate to spend 50% of their time in product testing to ensure our solutions meet the highest quality standards.Key Responsibilities:\u00b7 CI/CD Pipeline Development and Optimization:\u00b7 Process Automation\u00b7 Containerization and Orchestration\u00b7 Jenkins and CloudBees Management\u00b7 Python Programming\u00b7 Collaboration and Communication\u00b7 Product Testing Skills\u00b7 DEV-OPS - 6+ years of experience needed\u00b7 Linux/RHEL OS - 6+ years of experience needed\u00b7 Python - 6+ years of experience needed\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "217_we have a job opportunity for GCP DevOps Architect based at Remote": "uday,\nMagicforce\nuday@magicforce.us\nReply to: uday@magicforce.us\nHi, Greeting from Magic Force\u2026!!! My name is Uday Kumar we have a job opportunity for GCP DevOps Architect based at Remote please find the job description below, if you are available and interested, please send us your word copy of resume with following details to uday@magicforce.us Job Title: GCP DevOps Architect Location: RemoteDuration: 1+ YearJob description:Mandatory skills\u00b7 Proven experience as a Cloud Engineer or similar role in cloud computing.\u00b7 Experience with Google Cloud Platform services.\u00b7 Knowledge of scripting languages such as Python and Bash.\u00b7 Familiarity with DevOps tools like Jenkins, Ansible, Docker, Kubernetes.\u00b7 Understanding of networking and security concepts.Good to have skills: -GCP certification is a plus.Responsibilities: -Design and build cloud infrastructure and platform for cloud-native systems.Implement DevOps practices such as infrastructure as code, continuous integration, and automated deployment.Monitor and manage cloud performance and cost optimization.Knowledge on Python, Google Cloud Services, Terraform, VPC-SC Ensure cloud architecture meets security compliance policies.Collaborate with team members to improve the company\u2019s cloud use.Participate in all phases of software development from inception to implementation and support.Collaborate with cross-functional teams to define, design, and ship new features.Continuously discover, evaluate, and implement new technologies to maximize development efficiency.Work on bug fixing and improving application performance. If you are interested in this position, kindly fill the details and revert back me with Any ID Proof Copy.Full Name Work Authorization and validity Present location Passport Number Contact Number E-mail Address DOB LinkedIn ID Last 4 digits of SSN Availability to join the project Overall, USA IT experienceYear\u2019sOverall relevant IT experienceYear\u2019sBill rate$/hr all-inclusive onEducation Detail\u2019s Bachelor\u2019s and Master\u2019s (University/Stream/Year of passing)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "218_AWS Architect - Enterprise level - (P&amp;C domains)": "Bhavya Rapelii,\nAvance\nbhavya.r@avanceservices.com\nReply to: bhavya.r@avanceservices.com\nAWS Architect - Enterprise level - (P&C domains)San Antonio,Texas,Onsite Skills:AWS ARCHITECT Api integration IAAS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "219_100% Remote--Job opening---Devops Architect": "Priyanka,\nSwifttechinc\npriyanka@swifttechinc.com\nReply to: priyanka@swifttechinc.com\nHi,DevOps ArchitectRemoteAuthentication( how a API key or Lambda authorizer setup enables authentication)EKS , Scripting (Python) , Lambda Authorize, Terraform Terraform Template creationTerraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication services Thanks & Regards,Priyanka GandhiTechnical RecruiterWeb : http://www.swifttechinc.com/ Corporate Headquarters, 4950,N O'Connor Road, Suite # 207Irving , TX 75062\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "220_AWS Technical Architect || Whippany, NJ": "Hitakshi,\nTestingXperts\nhitakshi@testingxperts.com\nReply to: hitakshi@testingxperts.com\nTittle: AWS Technical Architect (Java Spring Boot + Microservices + Solution Design)Location: Whippany, NJ hybrid Monday and Thursday 1. Strong design & development knowledge in Spring Boot Microservices architecture.2. Strong development knowledge & Designer experience working in AWS.3. Participate in project analysis, design, implementation and commissioning phases.4. Analyze the technical impact specific to each business requirements/project requirements5. Understanding of design and architectural patterns6. Willingness to own and resolve problems, POC work and other activity.7. Strong experience in Microservices (Decompose, Strangler, Saga, Event sourcing, CQRS, Tx Messaging). Knowledge of a microservices architecture.8. Strong experience in creating HLSD, Low level designer and having exposure to present the design to steering committee/working group.9. Understanding of microservice design and architectural patterns10. Experience in DDD, BDD, TDD.11. Experience in working in an Agile environment and a good understanding of Agile processes.12. Enhance delivery systems with Continuous Integration & Deployment.13. Ability to design, develop and implement scalable, elastic microservice based platforms.14. Demonstrated knowledge of PCI and Security Coding Standards. PCF/Docker/Kubernetes Jenkins.15. Demonstrated knowledge of OAuth token-based authentication protocol.16. Troubleshoot issues and solve problems where needed17. Ability to do the pipeline setup & Integrations with other applications and infrastructure.18. Willingness to own and resolve problems, POC work and other activity.Excellent communication skills and a team player. Liaise with Clients, SMEs and Business Associates.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "221_Looking for Azure Kubernetes Services Expert Writer Need Local To IL Candidates": "Khursheed,\nResourcehunt Group\nresponseph@resourcehuntgroup.com\nReply to: responseph@resourcehuntgroup.com\nAzure Kubernetes Services ExpertChicago ILNeed Local to IL CandidatesJD:To provide design, engineering, development, planning, and administration of Azure Kubernetes AKS clusters for a set of critical business applications.Work closely with the application, engineering, security, and operations teams to engineer and build Kubernetes and Azure PaaS & IaaS solutions within an agile and modern enterprise-grade operating model.Design solutions for Kubernetes based architecture while making sure the best practices are followedAbility to capture the entire infrastructure as a code Implement the principles of SREDesign deployment architecture for IaaS / SaaS /PaaS that will address our client\u2019s business needs and lead teams that will implement them for our clients.Experience with workload migration automation toolsInfrastructure provisioning and management (Terraform, Ansible, Chef, Puppet, Chef, YAML, Containers, Azure Container Registry, Docker, DockerHub)DevOps and Agile methodologies, processes and toolsExceptional written and verbal communication skillsConfidently articulates all aspects of the solution and persuasively communicates value to the client.Works individually, within teams, or as a leader, to determine customer requirements in complex and ambiguous environments.Self-motivated with strong analytical and presentation skills.Ability to work with geographically dispersed teams and possess cross-cultural competenceAttention to detail and high-quality deliverablesTo analyze the training needs of developers, operational teams within our client organizationAbility to monitor, evaluate and review the quality and effectiveness of training, assessment and outcomes of courses deliveredDevelopment of operating model for the above environments and facilitating handover to business and IT operations teamsIntegration of and administration of relevant technology, people and processes within relevant IT compliance, regulatory and cybersecurity requirements.Working with application teams, business teams, vendors and clients to produce, develop and review cloud and cloud security strategy. Must haves in Resume: Working experience in designing, deploying and maintaining Kubernetes.Preferably in Azure (Azure Kubernetes Service - AKS).Performance optimization, monitoring and logging for AKS clusters.Creating and maintaining container images using Docker.Hands-on terraform, Ansible and YAML.Well versed in Linux operating system.ThanksKhursheed\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "222_Azure DevOps Architect": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Azure DevOps Architect 1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "223_Lead Azure or Dot Net Developer_ Local candidates to Virginia only": "Saeeda,\nGlobal IT Family\nsaeeda@globalitfamily.com\nReply to: saeeda@globalitfamily.com\nRequisition Class: 2022SAAPP : CLOUDARV1 : CLOUDAR4 : Region 1: VirginiaRegion Name: Virginia Information Technology AgencyTitle/Role: VDOT Lead Azure /Dot Net Developer (740668)Start Date: 07/11/2024End Date: 06/30/2025Worksite Address: 1221 E. Broad St. Richmond, VA 23219Agency Interview Type: Both Web Cam and In Person InterviewWork Arrangement: Hybrid VDOT ITD is looking for an Azure Developer with Dot Net Experience who will play a pivotal role in designing, implementing, and optimizing data solutions within the Microsoft Azure cloud ecosystem.Able to design and plan cloud infrastructure, including computing resources, storage, networking, and security components. Collaborate with other teams to design cloud-native applications and migrate existing applications to the cloud and Implement security measures to safeguard data and applications in the cloud.Minimum Qualification- Hands-on experience in design, development and testing .NET solutions (.NET Core, MVC.NET, ASP.NET,C#, Web Services (WCF, Web API), JQuery, T-SQL, PL/SQL, etc.) within both on-premises and cloudenvironments.- Hands-on experience with Angular / Blazor or any similar front-end technologies.- Hands-on experience with project online API's- Experience with Microservices architecture, Azure Architecture, Domain driven architecture.- Experience with Azure native application development, Azure App Services, Azure Kubernetes Service, Azure Container instances, Kubernetes & Containers (Docker), Azure Integration Services (Logic Apps, API Management, Service Bus & Event Grid) ,Azure SQL Database, Azure Web Jobs, SQL Server IaaS, Azure Monitoring and Application Insights- Knowledge and experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM Templates- Collaborate with architects and other senior developers to define software architecture, making well-informed design decisions that align with VDOT ITD business needs- Experience working in agile methodology.Preferred Qualification- Strong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtualnetworks, VPN, and Express Route- Strong scripting skills in Python- Experience working with Azure Data factory, Azure Data Lake.- Experience developing power BI reports SkillRequired / DesiredAmountof ExperienceExperience with ASP.NET, MVC, C#,WEB APIRequired10YearsExperience with Angular, Blazor or similar front end technologiesRequired7YearsExperience with .NET CoreRequired5YearsExperience with Microservices architecture, Azure architecture, Domain driven architectureRequired7YearsExperience with Docker and azure cloud services such as AKS, Azure functions, Azure Container Service, Azure App services, Azure API ManagementRequired7YearsExperience with Azure cloud databasesRequired7YearsExperience with Azure Monitoring, Application InisghtsRequired5YearsExperience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM TemplatesRequired3YearsAzure certified developerHighly desired3YearsExperience with Infrastructure As CodeHighly desired3YearsExperience with Project Online Rest API'sHighly desired3YearsStrong scripting skills in PythonDesired2YearsStrong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtual networks, VPN, and Express RouteNice to have2YearsExperience working with Azure Data factory, Azure Data Lake.Nice to have2YearsExperience developing power BI reportsNice to have2Years Question 2Please list candidate's email address.Question 3In what city/state does your candidate PERMANENTLY reside?Question 4Does your candidate agree to attend an onsite interview, if requested? This is REQUIRED.Question 5Does your candidate agree to work onsite up to 3 days/week? This is REQUIRED.Question 6How soon after an offer can your candidate start?Thanks & Regards,Saeeda ShaikhGlobal IT Family LLCEmail: saeeda@globalitfamily.com www.globalitfamily.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "224_Direct Client : Lead Azure Dot Net Developer Richmond VA Hybrid": "SRIKANTH REDDY,\nDIA Software Solutions\nsreddy@diasoftwaresolutions.com\nReply to: sreddy@diasoftwaresolutions.com\nHello,I hope this message finds you well! I am reaching out to you on an exciting Direct client State opportunity with one of our clients. Can you please go through the requirements and let me know if you are interested in this position?Role: Lead Azure /Dot Net DeveloperLocation: Richmond VA- Hybrid Duration: 1+ Year contract positionMode of Interview: Both Web Cam and In Person InterviewResponsibilitiesAzure Developer with Dot Net Experience who will play a pivotal role in designing, implementing, and optimizing data solutions within the Microsoft Azure cloud ecosystem.Able to design and plan cloud infrastructure, including computing resources, storage, networking, and security components. Collaborate with other teams to design cloud-native applications and migrate existing applications to the cloud and Implement security measures to safeguard data and applications in the cloud.Minimum Qualification: - Hands-on experience in design, development and testing .NET solutions (.NET Core, MVC.NET, ASP.NET,C#, Web Services (WCF, Web API), JQuery, T-SQL, PL/SQL, etc.) within both on-premises and cloudenvironments.- Hands-on experience with Angular / Blazor or any similar front-end technologies.- Hands-on experience with project online API's- Experience with Microservices architecture, Azure Architecture, Domain driven architecture.- Experience with Azure native application development, Azure App Services, Azure Kubernetes Service, Azure Container instances, Kubernetes & Containers (Docker), Azure Integration Services (Logic Apps, API Management, Service Bus & Event Grid) ,Azure SQL Database, Azure Web Jobs, SQL Server IaaS, Azure Monitoring and Application Insights- Knowledge and experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM Templates- Collaborate with architects and other senior developers to define software architecture, making well-informed design decisions that align with VDOT ITD business needs- Experience working in agile methodology.Preferred Qualification - Strong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtualnetworks, VPN, and Express Route- Strong scripting skills in Python- Experience working with Azure Data factory, Azure Data Lake.- Experience developing power BI reportsRequired Skills: SkillRequired / DesiredAmountof ExperienceExperience with ASP.NET, MVC, C#,WEB APIRequired10Years Experience with Angular, Blazor or similar front end technologiesRequired7YearsExperience with .NET CoreRequired5Years Experience with Microservices architecture, Azure architecture, Domain driven architectureRequired7YearsExperience with Docker and azure cloud services such as AKS, Azure functions, Azure Container Service, Azure App services, Azure API ManagementRequired7Years Experience with Azure cloud databasesRequired7YearsExperience with Azure Monitoring, Application InisghtsRequired5Years Experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM TemplatesRequired3YearsAzure certified developerHighly desired3Years Experience with Infrastructure As CodeHighly desired3YearsExperience with Project Online Rest API'sHighly desired3Years Strong scripting skills in PythonDesired2YearsStrong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtual networks, VPN, and Express RouteNice to have2Years Experience working with Azure Data factory, Azure Data Lake.Nice to have2YearsExperience developing power BI reportsNice to have2Years THANKS & REGARDSSRAVAN KUMAR | DIA SOFTWARE SOLUTIONS LLC.Austin, TX 78727 | |skumar@diasoftwaresolutions.com|Diasoftwaresolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "225_DevOps Engineer  Location: Phoenix, AZ(Need local candidates only)": "sirisha,\nnorthite\nsirisha@northite.com\nReply to: sirisha@northite.com\nJob Title-DevOps EngineerLocation: Phoenix, AZ(Need local candidates only)Duration: 12 Months5+ years of experienceAzure DevOps Git / Git-ActionsTerraformExperience in setting up CI/CDExperience in azure services admin (ADF, Logic Apps, Blob, ADLS, Key-Vault, etc.)Experience in Databricks admin (Workspaces, Unity Catalog, Volumes, Ext volumes, etc.)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "226_Need! AWS Solution Architect || Onsite_ ARKANSAS": "vikas,\nDVG Tech Solutions\nvikas@dvgts.com\nReply to: vikas@dvgts.com\nHi,Hope you are doing good! Role: DFA AWS Cloud Solutions Engineer Lead/Architect.Location: 1515 W. 7th St., Room 503, Little Rock, AR 72201 - Onsite from Day oneExp: 13+Yrs.Must have _AWS Solutions Architect certification Overview: DFA is migrating the Arkansas Integrate Revenue System Tax (AIRS Tax) and Driver Services/Motor Vehicle (AIRS DSMV) COTS systems from the current on premise data centre to the AWS Cloud environment. The DFA AWS Cloud Solutions Engineer is responsible for analysing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center. Roles and Responsibilities Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor, and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, Guard Duty, Security Hub, Systems Manager, Network Firewall,WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, co-workers, AMS, and business partners.Perform other duties as assigned. Thanks, and regards.VikasSr. US IT Recruiter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "227_AWS Cloud Solutions Engineer Lead Architect Little Rock AR": "Srinu,\nDVGTS TECH SOLUTIONS\nsrinu@dvgts.com\nReply to: srinu@dvgts.com\nGood Morning All, Hope you are doing well,Please find the attached requirement and let me know once if you have any resources on it. Role: DFA AWS Cloud Solutions Engineer Lead/Architect.Client: State of ArkansasLocation: 1515 W. 7th St., Room 503, Little Rock, AR 72201 - Onsite from Day one (LOCAL TO AR)Exp: 13+Yrs.Start Date: 08/01/2024End Date: 06/30/2025 Must have _AWS Solutions Architect certification Overview: DFA is migrating the Arkansas Integrate Revenue System Tax (AIRS Tax) and Driver Services/Motor Vehicle (AIRS DSMV) COTS systems from the current on premise data centre to the AWS Cloud environment. The DFA AWS Cloud Solutions Engineer is responsible for analysing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center. Roles and Responsibilities Provide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor, and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, Guard Duty, Security Hub, Systems Manager, Network Firewall,WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, co-workers, AMS, and business partners.Perform other duties as assigned. Best,SrinuEmail: Srinu@dvgts.comMobile : (609) 490 4102Desk: (609) 888 6198 Ext: 118DVG Tech Solutions LLC.(a WBE/MBE and E-Verify Employer)666 Plainsboro Rd, Suite 1010, Plainsboro NJ 08536Phone: (609) 888 6198 | Fax: (609) 228 6192\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "228_GCP-GKE Engineer with Terraform Experience - Phoenix, AZ -- PP no must": "Prashanth,\nBrillius\nprashanthn@brillius.com\nReply to: prashanthn@brillius.com\nJob Title: GCP/GKE Engineer with Terraform ExperienceLocation: Phioenix, AZ Onsit/Hybrtid roleContract C2C Job Description:We are seeking a talented GCP/GKE Engineer with strong experience in deploying and managing infrastructure on Google Cloud Platform, particularly using Google Kubernetes Engine (GKE). The ideal candidate should also have extensive hands-on experience with Terraform for infrastructure as code (IaC) automation.Responsibilities:Design, deploy, and manage applications and services on GCP using best practices for cloud infrastructure architecture.Implement and manage Kubernetes clusters using GKE, ensuring scalability, security, and reliability.Develop and maintain Terraform scripts/modules for infrastructure provisioning and configuration management.Collaborate with cross-functional teams (DevOps, developers, and architects) to design and implement cloud solutions.Monitor, troubleshoot, and optimize cloud infrastructure and applications to ensure high availability and performance.Implement CI/CD pipelines for automated deployment using tools like Jenkins, GitLab CI/CD, etc.Ensure compliance with security standards and best practices in cloud environments.Document infrastructure and processes, and participate in knowledge sharing sessions.Requirements:Bachelor\u2019s degree in Computer Science, Engineering, or a related field (or equivalent experience).Proven experience as a GCP/GKE Engineer with hands-on experience deploying and managing Kubernetes clusters in production environments.Strong proficiency with Terraform and infrastructure as code principles.Experience with Docker containers and container orchestration tools.Solid understanding of networking concepts, cloud security best practices, and monitoring tools.Familiarity with CI/CD pipelines and related tools.Excellent problem-solving skills and ability to work independently as well as part of a team.Strong communication skills and ability to collaborate effectively with stakeholders at all levels.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "229_Urgent Need - Technical Project Manager (Cloud Migration - AWS)": "Jon,\nSmarttechlink\njon@smarttechlink.com\nReply to: jon@smarttechlink.com\nHi, We do have a priority requirement with one of our clients. Kindly review and let me know if you have any questions.Technical Project Manager (Cloud Migration - AWS) Location: Los Angeles, CA ( 3 days work from office )Responsibilities:- Manage cloud migration and enablement projects, specifically in AWS environments.- Coordinate with technical teams to ensure smooth and efficient project execution.- Develop and maintain project timelines, budgets, and documentation.- Identify risks and issues, proposing solutions to mitigate them.Qualifications:- Experience in managing cloud migration projects, preferably in AWS.- Understanding of cloud architecture and services.- Strong project management skills with a focus on delivering results.- Proactive and responsible individual with excellent problem-solving abilities.Thanks and Regards jon@smarttechlink.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "230_Python Spark AWS - Columbus, OH": "vishnu,\nprocorpsystems\nvishnu.g@procorpsystems.com\nReply to: vishnu.g@procorpsystems.com\nHello,Job Title : Python Spark AWSLocation : Columbus, OH \u2013 non local candidates accepted Client: Mphasis-JPMCJob Responsibilities:Develop and maintain data platforms using Python, Spark, and PySpark.Handle migration to PySpark on AWS.Design and implement data pipelines.Work with AWS and Big Data.Produce unit tests for Spark transformations and helper methods.Create Scala/Spark jobs for data transformation and aggregation.Write Scaladoc-style documentation for code.Optimize Spark queries for performance.Integrate with SQL databases (e.g., Microsoft, Oracle, Postgres, MySQL).Understand distributed systems concepts (CAP theorem, partitioning, replication, consistency, and consensus).Skills:Proficiency in Python, Scala (with a focus on functional programming), and Spark.Familiarity with Spark APIs, including RDD, DataFrame, MLlib, GraphX, and Streaming.Experience working with HDFS, S3, Cassandra, and/or DynamoDB.Deep understanding of distributed systems.Experience with building or maintaining cloud-native applications.Familiarity with serverless approaches using AWS Lambda is a plus\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "231_Remote: IAM Engineer with AWS,Ping": "Rasitha,\nStellar IT Solutions\nrasithab@stellarit.com\nReply to: rasithab@stellarit.com\nHi, This is Rasitha from Stellar IT Solutions. I am reaching out to you on an exciting job opportunity with one of our clients. Position: IAM Engineerlocation: RemoteTop Skills' Details-5+ years of experience within InfoSec and IAM-Strong understanding of COTS Products-Experience with HYPR-Basic Development skills-Can manager an Enterprise Wide Deployment-1-2 years of AWS Experience-Experience with PING products is a plusSecondary Skills - Nice to HavesJob DescriptionThis person will be installing. getting all users registered, resolving technical and non technical related issues that come up, and planning for deployment. They need to be able to manage an enterprise wide deployment. They will be working with the team and stakeholders building integrations with the systems and working with the vendors to get them fixed.Additional Skills & Qualifications-Very Strong Technically-Can problem solve well-Proactive-Can figure out challengesExample they're looking for:-They're trying to get AWS access its not going through then they need to figure out why and get the right access and people on it.REQUIREMENTS FROM MANAGER VIA EMAIL1. Hypr engineer with Ping experience2. Experience with development with Java or Python and scripting3. Experience integrating Hypr with desktop agent (Windows and Mac)4. Experience with ADCS and certificates5. Prior experience deploying a passwordless solution is an advantage6. Prior experience deploying, maintaining and expanding COTS applications ispreferred7. Identity and Access Management experience - with a required skill set of full stackJava development and test automation experience8. Experience using AWS technologies for building, deploying and maintainingapplications9. Building new APIs, and leveraging 00B product APIs10. Experience with integrating with technologies like Splunk and Apigee11. At least 5 years of experience12. No candidates from West CoastRegards, Rasitha Begum, Technical Recruiter, Email: rasithab@stellarit.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "232_Hiring For Machine Learning Engineer with AWS Sagemaker for Hexaware": "Alok Kumar Verma,\nTeamware Solutions\nalokkumar.v@twsol.com\nReply to: alokkumar.v@twsol.com\nRole: Machine Learning Engineer with AWS SagemakerProject Location: Reston, VA( Once In a month)Employment Type : FTE /C2C Job Description:#1 - As a SageMaker V2 Migration and Custom Container Engineer, your primary responsibility will be to support the migration to SageMaker V2 and manage custom container environments. You will work closely with our Analytics teams to understand their specific needs for Python and R container images, resolve dependencies, address vulnerabilities, and ensure smooth deployment on SageMaker and Domino Data Lab.#2 - As an MLOps Implementation Engineer, your primary responsibility will be to understand and implement our pre-developed MLOps strategies across various tools, including Domino Data Lab and AWS SageMaker. You will play a crucial role in containerizing these models and developing automated processes to deploy them into AWS Batch, Amazon Elastic Container Registry (ECR), or Amazon Elastic Kubernetes Service (EKS). Additional info :Key Responsibilities:- Interact with Analytics teams to gather requirements for Python and R container images.- Resolve dependencies for the required Python and R packages.- Identify and address any security vulnerabilities in the container images.- Develop and maintain custom container images to ensure compatibility with SageMaker and Domino Data Lab.- Test and validate container images to ensure they meet performance and reliability standards.- Collaborate with cross-functional teams to ensure seamless integration and deployment of custom images.- Document processes and provide training as needed to ensure team members can effectively use and manage the custom containers.Requirements:- Bachelor\u2019s or master\u2019s degree in computer science, Engineering, or a related field.- Proven experience in managing and developing custom containers, especially with Python and R.- Strong expertise with Amazon SageMaker and its various components.- Experience with containerization technologies such as Docker.- Knowledge of dependency management and package resolution for Python and R.- Understanding of security best practices and vulnerability mitigation for container images.- Familiarity with CI/CD pipelines and tools (e.g., Jenkins, GitLab CI/CD).- Strong programming skills in Python and/or R.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration skills.Preferred Qualifications:- Experience with AWS Cloud and its services like S3, ECR, EKS etc.Knowledge of infrastructure as code (IaC) using tools like Terraform or AWS CloudFormation.Understanding of machine learning model lifecycle management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "233_AWS DevOps": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: AWS DevOpsProvide a stable cloud environment with proven experience in cloud engineering and a deep understanding of AWS and/or GCP services and offerings. Develop processes and pipelines with hands-on experience in CI/CD technologies - eg: Argo CD (preferred), and Argo Workflow (preferred), Chef or Puppet, Ansible, Jenkins. Gitlab etc. Create and provide support for monitoring, alerting, fault analysis, and other common reliability engineering concepts. Provide incident response and RCA using strong problem-solving skills and the ability to adapt to a fast-paced, evolving environment. Develop and deploy containerized applications with proven experience in Kubernetes and container orchestration.Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "234_Site Reliability Engineer-DevOps": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Site Reliability Engineer-DevOpsBachelor's degree in Computer Science, Information Technology, Information Systems, a related technical field, or equivalent practical experienceLogical, innovative, articulate, with the proven ability to develop and maintain relationships with technical and non-technical teams and individualsHighly proactive self-starting ethos, strong customer focus, strong work ethic, and exceptional problem-solving skillsProficiency in Google Cloud Platform.Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "235_Senior Azure DevOps Engineer": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Senior Azure DevOps EngineerHands on automation skills designing, building, and troubleshooting CI/CD pipelines and Infrastructure as Code of enterprise applicationsBuild and support highly available CICD tools for pipelines and related integrationsDesign, build, and implement reusable YAML pipelines in Azure DevOps while adhering to best practicesPerform analysis of DevOps practices, identify gaps and impediments for continuous integration and deliveryEngage with information Security and augment the CI/CD framework with DevSecOps and standardized security tools and controlsDevelop and apply quality controls and gates to the build, test, and deploy process while establishing Definition of Ready and Definition of Done to ensure quality of deliveryBe a member of Center of Excellence for DevOps practices and develop policies, standards, guidelines, governance for both CI/CD operations and for work of developersProvide guidance, coaching, training on build, test, and release management best practicesCollaborate on and provide input into the CICD roadmap, identify gaps and dependenciesOnboard agile product teams to the CICD framework and perform DevOps health assessment on pipelines, identify gaps and execute recommendationsWork directly with business leadership to understand data requirements; propose and develop solutions that enable effective decision-making and drives business objectivesPerform other duties as assigned Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "236_SRE Lead with Kubernetes (Local to CA)": "sanjeev,\nHMG America LLC\nsanjeev@hmgamerica.com\nReply to: sanjeev@hmgamerica.com\nSRE LeadLong Term ContractSFO, CA (Onsite) Qualifications: \u00b7 The ideal candidate will have a strong background in production monitoring, a deep understanding of development and operations, and a proven track record in managing and scaling distributed systems in a public, private, or hybrid cloud environments.\u00b7 Understanding of SRE principles, including monitoring, alerting, fault analysis, and other common reliability engineering concepts, with a keen eye for opportunities to eliminate toil by code and process improvements. \u00b7 Expertise in infrastructure as code (IAC), configuration management, build automation, source control, and CI/CD tools (e.g., Terraform, CloudFormation, Ansible, GitHub, Artifactory, Jenkins).\u00b7 Deep understanding of containerization and orchestration technologies (e.g., Docker, Kubernetes). \u00b7 Experience with monitoring and logging tools (e.g., Prometheus, Grafana, Dynatrace, Splunk) and incident response processes. \u00b7 Proficient in Java, .NET, Web UI/JavaScript Frameworks and scripting languages such as Python, Bash, and PowerShell. \u00b7 High-level understanding of the different layers of the Tech stack and how they come together to provide a service (e.g. network, compute, storage, OS (Linux, Windows), supporting services, application layer). Responsibilities: \u00b7 Key measures of success will include platform stability, effective integration and delivery, instrumentation, release quality, technical debt(toil) reduction, development of automation, risk/security compliance, and sustained advancement of the SRE practice. \u00b7 Design & implement scalable, automated, monitored, and well-documented systems to accelerate the development of the services running in the AWS and Azure cloud. \u00b7 Configure, tune, and fix multi-tiered systems to achieve optimal application performance, stability, and availability. \u00b7 Be part of an on-call rotation providing hands-on technical expertise during service-impacting events. \u00b7 Apply troubleshooting skills, debugging tools, and examine logs, telemetry, and other methods to verify assumptions and customer impact. Lead blameless postmortems for root cause and production resiliency.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "237_Devops Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nThis company provides a global payments software offering that has grown consistently over the last decade. Great company with vertical growth opportunity!This Jobot Job is hosted by: Alex DickinsonAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.Salary: $120,000 - $165,000 per yearA bit about us:This company is a modern software product firm that services clients all across the US. They have a very mature and modern IT infrastructure posture. Their current team is looking for a Devops engineer functioning in a .NET environment with cloud experience.Why join us?Competitive Base Salary - $120-155kQuarterly bonus plan401k with matchGym reimbursementWFH optionsAccelerated Career Growth!Job DetailsBuilding, configuration, deployment, and management of high volume, highly available .Net applicationsWindows Server/IIS deployment, configuration, and administrationKubernetes (AKS) deployment, configuration, and administrationAzure DevOpsMonolithic architectureTerraformOctopus DeployGitPowershell abilitiesSolid understanding of core concepts: DNS, HTTP/HTTPS, Load-Balancing, TCP/IP routing and switchingInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.Employers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "238_AWS DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nTitle : AWS DevOps EngineerLocation: Chicago, IL (Must able to work in 3 days in office and 2 days remote)Duration: 12 Months Contract with High Possibility of ExtensionsNOTE: W2 ONLYJob Responsibilities:Analyze, optimize, and implement cost optimization actions over AWS cloud using GitLab CI/CD pipeline services.Research, design, implement and manage automated tools/solutions to help identify cost trends and anomalies.Build proactive and reactive automation that optimizes AWS resources via the GitLab CI/CD pipeline environment.Create financial and usage dashboards.Top 3 soft skills:Collaboration & TeamworkAutonomy and Critical ThinkingAbility to assess necessary activities and prioritize for completion of assignments using excellent time management, prioritization skills and work ethic.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "239_Devops Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nThis company provides a global payments software offering that has grown consistently over the last decade. Great company with vertical growth opportunity!This Jobot Job is hosted by: Alex DickinsonAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.Salary: $120,000 - $165,000 per yearA bit about us:This company is a modern software product firm that services clients all across the US. They have a very mature and modern IT infrastructure posture. Their current team is looking for a Devops engineer functioning in a .NET environment with cloud experience.Why join us?Competitive Base Salary - $120-155kQuarterly bonus plan401k with matchGym reimbursementWFH optionsAccelerated Career Growth!Job DetailsBuilding, configuration, deployment, and management of high volume, highly available .Net applicationsWindows Server/IIS deployment, configuration, and administrationKubernetes (AKS) deployment, configuration, and administrationAzure DevOpsMonolithic architectureTerraformOctopus DeployGitPowershell abilitiesSolid understanding of core concepts: DNS, HTTP/HTTPS, Load-Balancing, TCP/IP routing and switchingInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.Employers have access to artificial intelligence language tools (\u201cAI\u201d) that help generate and enhance job descriptions and AI may have been used to create this description. The position description has been reviewed for accuracy and Dice believes it to correctly reflect the job opportunity.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "240_Devops Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nJob Title: Devops EngineerLocation: Plano, TX:Understanding customer requirements and project KPIs, 5+ yrs of experience in Devops and Application Support.Implementing various development, testing, automation tools, and IT infrastructureSetting up tools and required infrastructure.Defining and setting development, test, release, update, and support processes for DevOps operationHave the technical skill to review, verify, and validate the software code developed in the project.Troubleshooting techniques and fixing the code bugsMonitoring the processes during the entire lifecycle for its adherence and updating or creating new processes for improvement and minimizing the wastageEncouraging and building automated processes wherever possible.Identifying and deploying cybersecurity measures by continuously performing vulnerability assessment and risk managementIncidence management and root cause analysisCoordination and communication within the team and with customersSelecting and deploying appropriate CI/CD toolsStrive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline (CI/CD Pipeline)Mentoring and guiding the team membersMonitoring and measuring customer experience and KPIsManaging periodic reporting on the progress to the management and the customerBSc in Computer Science, Engineering or relevant fieldExperience as a DevOps Engineer or similar software engineering roleProficient with git and git workflowsGood knowledge of Ruby, Python, JavaWorking knowledge of databases like MongoDB and SQLProblem-solving attitudeCollaborative team spiritGood knowledge of AWS, Cloud Watch, Synthetic monitoring.AIT Global inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "241_Technical Project Manager(Development or Devops Background)": "AJ,\nTARDUS INC\naj@tardusinc.com\nReply to: aj@tardusinc.com\nHi,Role : TPM (Development or Devops Background Exp)location : Plano, TXDuration : On-GoingPlease share local profiles to AJ@tardusinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "242_Full time position ---  AWS Infra Architect --- Alameda CA (100% on-site from day 1)": "Mohit Jaiswal,\nIntelligenz IT\nmohit.jaiswal@intelligenzit.com\nReply to: mohit.jaiswal@intelligenzit.com\nHi,I hope you and your family are doing well.I have a good position for you with my client. This point of time I don't know whether you are looking for a new job or not. But just thought if I can share the details and then confirm from you about your interest level for the opportunity. If you are interested and an available consultant, send me your most updated resumes in word format and contact details.Job Title: AWS Infra ArchitectLocation: Alameda CA (100% on-site from day 1)Duration: Full time positionKey Responsibilities:Architect, design and implement unified data access for various personas like data engineer, data analyst, data steward by simplified access control mechanism which supports governance on AWS and non-AWS assets.Design, build & optimize CI/CD pipelines for data engineers to deploy and push code across multiple environments.Develop and maintain infrastructure as code (IaaC) with Terraform for reproducible and scalable deployments.Design & implementation of AWS cloud infrastructure using best practices and industry standards by working closely with internal & external stakeholders like Information Security, Cloud Infrastructure, Data Engineering teams, etc.Work closely with data governance teams in implementing data governance policies, periodic review, and optimization of repository from security & governance perspective.Continuous improvement of Cloud Data platform to enhance speed, agility, and cost efficiency. Qualifications:Expertise in reducing costs and increasing speed and efficiency in large scale data platform deployments.Proficiency in Terraform, Python Scripting along with GitHub Actions for deployment of infrastructure as code.Deep understanding of data governance policies, implementation best practices and hands-on experience in implementing Policies as CodeArchitecture, design & implementation of unified data access using AWS Lake formation, AWS Data Zone, IAM, OKTA and others.Experience in architecture & security of AWS EC2, EBS, S3, EKS, Athena, Redshift, RDS, Kafka, Glue, and other data management toolsExperience in in design and implementation of granular access to data without losing the speed & agility of delivering the access provisioning.Expertise in CI/CD process and deployment technologies using tools like GitHub, GitHub ActionsExpertise in logging, monitoring, reliability engineeringStrong knowledge of networking concepts and experience managing network-related issuesRegards,Mohit JaiswalIntelligenz ITWork : 646-502-7441Maillot: Mohit.Jaiswal@Intelligenzit.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "243_Hot Requirements : DevOps Architect  Remote Need 12+": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nDevOps ArchitectLPLRemoteTerraform Template creation Terraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSec Branching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how to deploy same authentication in Lambda and EKSServer less authentication servicesThanks and RegardsPrincy JainPrincy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "244_AWS Data Application Infrastructure Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: AWS Data Application Infrastructure Lead - OnsiteWork location:ATLANTA (US:30301), GAClient : CapgeminiDESCRIPTIONManage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience in Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "245_Senior Software Engineer - Azure, Python, devops, terraform, MySQL - Remote": "Shivam Pachauri,\nTek inspirations LLC\nshivam.pachauri@tekinspirations.com\nReply to: shivam.pachauri@tekinspirations.com\nJob Description -NOTE : Please share more than 13+ years of candidate on this roleSenior Software Engineer - Azure, Python, devops, terraform, MySQLStart date: ASAPDuration: 6 months Location: Fully remote contract We are seeking a candidate who can independently design and develop solutions, and who has substantial experience with MS Azure, Python, devops, terraform, MySQL and knowledge on networking and architecture.Primary responsibilities:Collaborating with stakeholders to understand requirements and translate them into technical solutionsWriting complex SQL queries for data retrieval, manipulation, and analysisImplementing data security measures such as user authentication, authorization, and securityWriting scripts to automate data extraction, transformation, and loading ( ETL) processesKeeping up to date with latest scripting languages (e.g., DAX, M, Python) to enhance capabilitiesAdhering to software development best practices and maintaining code quality standardsRequired Qualifications: 5+ years of experience in multiple back-end languages such as C#, Python, Java, or Node.js5+ years of experience with DevSecOps, CI/CD pipelines, and test automation tools5+ years of experience using a cloud platform such as Azure, AWS Familiarity with version control systems, particularly GitStrong understanding of database management systems such as relational (MS SQL)Familiarity with microservices architecture.Thanks & Regards,Shivam PachauriTechnical Recruiter TEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035E: shivam.pachauri@tekinspirations.com Linkedin: linkedin.com/in/shivam-pachauri-97b754296\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "246_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "247_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "248_AWS LEAD (Technical Lead)- San Antonio, TX- Onsite  Need Locals- 12-14 -20 + years of IT experience": "James Smith,\nYochana Solutions INC\nsmith@yochana.com\nReply to: smith@yochana.com\nAWS LEAD (Technical Lead)- San Antonio, TX- Onsite \u2013 Need Locals- 12-14+ years of IT experienceAWS LEAD (Technical Lead)- San Antonio, TX- Onsite \u2013 Need Locals- 15+ years of IT experienceAWS Architect (Enterprise Architect)- San Antonio, TX- Onsite \u2013 Need Locals- 15-20+ years of IT experience Job Title- AWS LEAD (Technical Lead)Location \u2013 San Antonio, TX- Onsite \u2013 Need LocalsDuration \u2013 1+year Mandatory Skill Set- AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS 12-14+ years of IT experience Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud.Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the details AWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration Job Title- AWS LEAD (Senior Technical LeadLocation \u2013 San Antonio, TX- Onsite \u2013 Need LocalsDuration \u2013 1+year Mandatory Skill Set- AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS15+ years of IT experience Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud.Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the details AWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration Job Title- AWS Architect (Enterprise Architect)Location \u2013 San Antonio, TX- Onsite \u2013 Need LocalsDuration \u2013 1+year Mandatory Skill Set- AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS , Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, AWS Cloud Watch, Kibana, Grafana, Prometheus 15-20+ years of IT experience Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud.Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the details AWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS Docker Containers, Kubernetes Prometheus, ELK, Grafana CI/CD API integration Thanks & Regards, James Smith- Team Lead smith@yochana.com Direct No: 949-201-1313 Yochana Solutions INCWindsor, Ontario- CanadaFarmington hills, MI-48335- USAUSA | CANADA I INDIA W: www.yochana.comUSA | CANADA | INDIA Note: This is not an unsolicited mail. If you are not interested in receiving our e-mails then please reply with subject line Remove\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "249_Hiring For Machine Learning Engineer with AWS Sagemaker for Hexaware": "Alok Kumar Verma,\nTeamware Solutions\nalokkumar.v@twsol.com\nReply to: alokkumar.v@twsol.com\nRole: Machine Learning Engineer with AWS SagemakerProject Location: Reston, VA( Once In a month)Employment Type : FTE /C2C Job Description:#1 - As a SageMaker V2 Migration and Custom Container Engineer, your primary responsibility will be to support the migration to SageMaker V2 and manage custom container environments. You will work closely with our Analytics teams to understand their specific needs for Python and R container images, resolve dependencies, address vulnerabilities, and ensure smooth deployment on SageMaker and Domino Data Lab.#2 - As an MLOps Implementation Engineer, your primary responsibility will be to understand and implement our pre-developed MLOps strategies across various tools, including Domino Data Lab and AWS SageMaker. You will play a crucial role in containerizing these models and developing automated processes to deploy them into AWS Batch, Amazon Elastic Container Registry (ECR), or Amazon Elastic Kubernetes Service (EKS). Additional info :Key Responsibilities:- Interact with Analytics teams to gather requirements for Python and R container images.- Resolve dependencies for the required Python and R packages.- Identify and address any security vulnerabilities in the container images.- Develop and maintain custom container images to ensure compatibility with SageMaker and Domino Data Lab.- Test and validate container images to ensure they meet performance and reliability standards.- Collaborate with cross-functional teams to ensure seamless integration and deployment of custom images.- Document processes and provide training as needed to ensure team members can effectively use and manage the custom containers.Requirements:- Bachelor\u2019s or master\u2019s degree in computer science, Engineering, or a related field.- Proven experience in managing and developing custom containers, especially with Python and R.- Strong expertise with Amazon SageMaker and its various components.- Experience with containerization technologies such as Docker.- Knowledge of dependency management and package resolution for Python and R.- Understanding of security best practices and vulnerability mitigation for container images.- Familiarity with CI/CD pipelines and tools (e.g., Jenkins, GitLab CI/CD).- Strong programming skills in Python and/or R.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration skills.Preferred Qualifications:- Experience with AWS Cloud and its services like S3, ECR, EKS etc.Knowledge of infrastructure as code (IaC) using tools like Terraform or AWS CloudFormation.Understanding of machine learning model lifecycle management.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "250_AWS Architect  ; San Antonio, TX onsite": "Arun kumar,\nyochana IT\narun@yochana.com\nReply to: arun@yochana.com\nAWS Architect (Enterprise Architect) San Antonio, TX onsite Contract JD AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS , Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, AWS Cloud Watch, Kibana, Grafana, Prometheus\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "251_Cloud - Source Code Consultant  !! REMOTE  !!": "Shailendra,\nTekpyramids\nshailendra@tekpyramids.com\nReply to: shailendra@tekpyramids.com\nCloud / Source Code ConsultantREMOTE6+ MonthsPhone/Skype Hire Any Visa is OK ! Requirements : \u2022 8-12 years of relevant experience within any of the following areas: Development, DevOps/DevSecOps, or Application Security.\u2022 Experience with Jenkins, GitHub Actions/Workflows and or other CI/CD platforms. Ability to build and automate security within CI/CD pipelines.\u2022 Developer experience and or understanding of development processes with modern programming/scripting languages. Best Regards ,Shailendra Technical Recruiter Phone: (248)-707-1965Email : shailendra@tekpyramids.com Fax : 248-856-9456 24175, Northwestern Hwy, Suite 120, Southfield, MI 48075 Click here to view our latest jobsThis electronic mail (including any attachments) may contain information that is privileged, confidential, and/or otherwise protected from disclosure to anyone other than its intended recipient(s). Any dissemination or use of this electronic mail or its contents (including any attachments) by persons other than the intended recipient(s) is strictly prohibited. If you have received this message in error, please notify us immediately by reply e-mail so that we may correct our internal records. Please then delete the original message (including any attachments) in its entirety. Thank you.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "252_Minimum 13+ yrs. of Exp. || Urgent Opening || GCP DevOps Lead-Architect || Remote": "Prashant Pal,\nVbeyond Corporation\nprashantp@vbeyond.com\nReply to: prashantp@vbeyond.com\nHello,Greetings of the day,This is Prashant from VBeyond Corp. We are a global recruitment company with specialization in of hiring of (IT) professionals. One of our clients is looking for GCP DevOps Lead/Architect at Remote.Minimum 13+ yrs. of exp must have and please share the profiles with the visa status and valid LinkedIn url.Job Description Position: GCP DevOps Lead/Architect Location: RemoteDuration: ContractDescriptionAs a Lead/Architect DevOps Engineer, you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.Responsibilities: -Design and build cloud infrastructure and platform for cloud-native systems.Implement DevOps practices such as infrastructure as code, continuous integration, and automated deployment.Monitor and manage cloud performance and cost optimization.Knowledge on Python, Google Cloud Services, Terraform, VPC-SC Ensure cloud architecture meets security compliance policies.Collaborate with team members to improve the company\u2019s cloud use.Participate in all phases of software development from inception to implementation and support.Collaborate with cross-functional teams to define, design, and ship new features.Continuously discover, evaluate, and implement new technologies to maximize development efficiency.Work on bug fixing and improving application performance.Educational Qualifications: -Engineering Degree \u2013 BE/ME/BTech/MTech/BSc/MSc.Technical certification in multiple technologies is desirable.Skills: -Mandatory skills\u00b7 Proven experience as a Cloud Engineer or similar role in cloud computing.\u00b7 Experience with Google Cloud Platform services.\u00b7 Knowledge of scripting languages such as Python and Bash.\u00b7 Familiarity with DevOps tools like Jenkins, Ansible, Docker, Kubernetes.\u00b7 Understanding of networking and security concepts. Good to have skills: -GCP certification is a plus.ThanksPrashanthttps://www.linkedin.com/in/abhimanyudwivedi/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "253_SR LEAD AWS CLOUD ENGINEER LOCAL TO TX": "Maseera Khan,\nAbsolute IT\nmaseera@absoluting.com\nReply to: maseera@absoluting.com\nJob DescriptionSr Lead AWS Cloud EngineerDuration: 6 Mon+Location: DALLAS, TX \u2013 MUST BE LOCAL \u2013 NO RELOCATION CANDIDATES \u2013 Hybrid role with onsite days as needed. NEED VISA DL LINKEDINMUST ANSWER:Minimum 8 years of experience in AWS platform with proven focus on well Architected Framework and best practices in architecture and design? Project scope, deliverables met, etc?4 years of experience in Infrastructure Automation using Terraform?Provide project examples where your evaluation and selection of AWS services optimized performance, cost, and security.Knowledge of programming languages: GO and Python?Are you local to DFW area? What City?Must be willing to work onsite days as needed.DO NOT EMAIL WITHOUT RESUME AND RESPONSES TO ABOVE\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "254_Opening for Azure Cloud Engineer": "Asha Kannan,\nTeamware Solutions\nasha.k@twsol.com\nReply to: asha.k@twsol.com\nHi Everyone,Hope you are doing well. Please find the JD below, Role: Azure Cloud Engineerlocation: Minneapolis, MN \u2013 OnsiteExperience: 9 YearsVisa: GC - EAD Description:Essential Skills:Experience in the below - AZURE Cloud, IAAS, PAAS, VM Migration, traffic manager, azure cloud, sql azure, networking, data lake, power shell Strong CommunicationHelp in AZURE configuration, document standards, security patching, testing and implementationCompetencies:Digital: Microsoft Azure, Digital: Amazon Web Service(AWS) Cloud Computing Thanks & Regards,ASHAasha.k@twsol.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "255_Senior Cloud Developer": "Kuldeep Sharma,\nVizonInc\nkuldeep@vizoninc.com\nReply to: kuldeep@vizoninc.com\nJob Description - Send me your top two ONLY candidates (- AWS Certified Professional preferred Senior Cloud Developer Qualifications:- 4+ years of experience in design and implementation of large enterprise public cloud environments - AWS Certified Professional preferred - 2+ years of hands-on experience integrating mission critical applications into AWS - Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.) - Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools - Fluency with one or more scripting/coding languages such as Java, Javascript, REST, JSON, Python, bash - Experience implementing and leveraging the logging and monitoring solutions - Experience in cloud native architectures and micro-services design - Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects - Experience in design of complex distributed systems environments - Demonstrated ability to think strategically about business, product, and technical challenges - Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail - Independent and self-motivated and very thorough worker WHAT ELSE? - Candidate must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.Responsibilities:- Provide technical leadership and design expertise for AWS Foundation integration- Define standards and patterns for when and how to efficiently leverage AWS services- Lead automation of AWS integration processes and tasks- Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners- Develop frameworks and tools to enable dev teams to consume authorized AWS services- Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins- Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS- Ensure Security-by-Design is a foundational component of every app deployed into AWS- Ensure enterprise monitoring standards are applied to each AWS app deployment- Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration- Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies)- Perform cost benefit analysis to determine best system architecture via software, hardware, internal cloud, externally hosted or other defined model- Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs- Partner with peer Infrastructure teams to provide enterprise class AWS integration- Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designsComments/Special InstructionsAWS , CFT , Ansible\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "256_Contract : immediate interviews on AWS LEAD (Technical Lead)   :: San Antonio, Texas (Onsite)": "Rushi,\nYochana\nrushi@yochana.com\nReply to: rushi@yochana.com\nGreetings! I am Rushi from YochanaIT Solutions Inc, we are one of the leading \"IT professional services provider in North America\". I am reaching out to check your availability. Please review the job description below and submit your updated resume as soon as possible feel free to reach me at 2485987513. Also, if someone else in your network is qualified for the position, then please feel free to forward this mail to them. Job Title : AWS LEAD (Technical Lead)Location : San Antonio, Texas (Onsite)Duration : Contract Job Description: Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus. Thanks & RegardsRushinga ReddyYochana Solutions Inc\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "257_Full time position --- Azure Devops Architect --- Omaha NE (100% on-site from day 1)": "Mohit Jaiswal,\nIntelligenz IT\nmohit.jaiswal@intelligenzit.com\nReply to: mohit.jaiswal@intelligenzit.com\nHi,I hope you and your family are doing well.I have a good position for you with my client. This point of time I don't know whether you are looking for a new job or not. But just thought if I can share the details and then confirm from you about your interest level for the opportunity. If you are interested and an available consultant, send me your most updated resumes in word format and contact details.Job Title: Azure Devops ArchitectLocation: Omaha NE (100% on-site from day 1)Duration: Full time positionOnly Greencard Holder & US CitizenStart Date ASAP....Educational Qualification*Bachelor\u2019s Degree or higher in Information Systems, Computer Science, or equivalent experience.JD/Description of Role*10-12 years of Proven experience in designing and implementing cloud infrastructure solutions on Microsoft Azure.Extensive experience with Azure DevOps for CI/CD and release management.The ideal candidate will play a crucial role in designing, implementing, and maintaining our cloud infrastructure while also managing and optimizing our development and deployment pipelines using Azure DevOps.Focuses on creating the foundational elements necessary for hosting applications, services, and data within the Azure environment.An understanding of application security and information security controlsA good understanding of large-scale distributed systems in practice, including multi-tier architectures, application security, monitoring and storage systems.Working knowledge of GitHub Actions, Azure DevOps, Jenkins (or other similar toolset)Role should be for hands-on Azure architect with deep expertise in Infrastructure.He/She should be able to wear multiple hats and be able to design scalable solutions on AzureSound understanding of cost/billing modules in Azure and be able to build scalable resource tagging and mapping solutions for enterprisesAble to design and build IaC solution to standardize patterns and provisioning of these patterns using Terraform and Azure DevOpsSolid understanding of containerization services in AzurePrimary (Must have skills)*Infrastructure Architecture:Design and implement scalable, secure, and highly available cloud infrastructure solutions on Microsoft Azure.Collaborate with cross-functional teams to gather requirements and translate them into robust and efficient architecture designs.Ensure compliance with industry best practices, security standards, and regulatory requirements.Azure DevOps:Set up and manage CI/CD pipelines using Azure DevOps for multiple projects and environments.Implement and maintain automation scripts for deployment, monitoring, and management of infrastructure and applications.Continuously optimize and improve the CI/CD processes for enhanced efficiency and reliability.Collaboration and Communication:Work closely with development, operations, and security teams to ensure seamless integration of infrastructure and deployment processes.Provide technical leadership and guidance to team members and stakeholders.Communicate complex technical concepts to non-technical stakeholders effectively.Monitoring and Troubleshooting:Implement monitoring solutions for infrastructure and applications, ensuring timely detection and resolution of issues.Conduct root cause analysis for incidents and implement preventive measures.Documentation:Create and maintain comprehensive documentation for architecture, configurations, and processes.Train team members on best practices and usage of implemented solutions.Secondary Skills (Good To have)*Relevant certifications such as Microsoft Certified: Azure Solutions Architect Expert, Microsoft Certified: DevOps Engineer Expert, etc.Soft skills/other skills (If any)Strong analytical and problem-solving skills.Excellent communication and interpersonal skills.Ability to work collaboratively in a team environment.Regards,Mohit JaiswalIntelligenz ITWork : 646-502-7441Maillot: Mohit.Jaiswal@Intelligenzit.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "258_Azure Cloud Migration Engineer  ---Infosys ---Chicago, IL ---Hybrid Onsite": "Azhar,\nSoftcom Systems Inc\nazhar@softcomsystems.com\nReply to: azhar@softcomsystems.com\nHello Professionals,This Azhar Mohammad from Softcom Systems INC.I have a below Job Opportunity if you have any Consultants Please let me know\u2026!Job Title: Azure Cloud Migration EngineerLocation: Chicago, IL - 60607 (Hybrid Onsite)Duration : 12 Months Job Details:This role will be responsible for architecting, implementing and supporting enterprise-wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis.Key Responsibilities:\u2022 Azure Cloud Migration:o Lead the planning, design, and execution of cloud migration projects to Azure.o Assess existing on-premises infrastructure, applications, and data for cloud readiness.o Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed.o Execute migration tasks, including data transfer, application reconfiguration, and validation.\u2022 Infrastructure as Code (IaC) with Terraform:o Design and implement infrastructure using Terraform for repeatable and consistent deployment.o Develop and maintain Terraform scripts and modules for provisioning Azure resources.o Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.\u2022 CI/CD Management:o Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab.o Automate build, test, and deployment processes to enhance efficiency and reliability.o Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.\u2022 Enterprise Tools:o Set up and manage enterprise testing tools.o Integrate testing tools with CI/CD pipelines to enable automated testing.o Develop and enforce testing best practices and standards to ensure high-quality deliverables.o Provide support activities for the queries, issues, access, installation and configuration for supported toolso In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.\u2022 Collaboration and Documentation:o Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives.o Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows.o Provide training and support to team members on cloud and DevOps best practices.Qualifications:\u2022 Bachelor\u2019s degree in computer science, Information Technology, or a related field.\u2022 Proven experience in Azure cloud migration projects.\u2022 Strong expertise in Terraform and infrastructure as code (IaC) principles.\u2022 Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.\u2022 Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.\u2022 Solid understanding of cloud computing concepts, networking, and security in Azure.\u2022 Excellent problem-solving skills and attention to detail.\u2022 Strong communication and collaboration abilities.Preferred Qualifications:\u2022 Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure Administrator).\u2022 Experience with other cloud platforms (AWS, Google Cloud).\u2022 Familiarity with containerization and orchestration tools like Docker and KubernetesPreferred tool experience:\u2022 Microsoft Azure DevOps, JIRA, BitBuket, Bamboo, ALM, Load Runner/Performance Center, JMeter, GitHub, Jenkins, Microfocus UFT/LeanFT, Selenium, Eclipse, ServiceNow, Perfecto, SeaLights, Gremlin, ToxiProxy, Chaos Monkey, Selenium Grid.Thanks & Regards:-Azhar MohammadSoftcom Systems Inc.Email: azhar@softcomsystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "259_Azure Infrastructure Engineer": "Upendar,\nCodebase Inc\nupendar.r@codebaseinc.com\nReply to: upendar.r@codebaseinc.com\nPosition : Azure Infrastructure EngineerLocation: Dallas, TX (Locals Only)Duration : 1 Year Job Description:We are seeking a highly experienced and skilled Azure Infrastructure L2 Senior Engineer to join our IT team. This role is crucial in ensuring the robust operation, maintenance, and optimization of our Azure cloud infrastructure. The ideal candidate will have extensive hands-on experience with Azure, strong technical skills, and the ability to provide advanced support and troubleshooting services. Proficiency in Terraform, ServiceNow, automation, and DevOps practices is essential. Excellent Communication Skills ARE MUST HAVEAzureNeed to write Terraform templatesWork with ServiceNOW, manage the tickets, escalate the tickets. (how to use ServiceNow ticketing tool)CI/CD monitoringLargely widows, some linuxMust have Patch management exp, (methods, change management, tickect management)Roles & Responsibilities:\u00b7 Advanced Infrastructure Monitoring: Proactively monitor Azure infrastructure to ensure optimal performance, availability, and security.\u00b7 Incident and Problem Management: Handle complex infrastructure-related incidents and problems, performing advanced troubleshooting and root cause analysis.\u00b7 ServiceNow Ticket Handling: Manage and resolve high-priority tickets using the ServiceNow platform, ensuring adherence to SLAs.\u00b7 Maintenance and Optimization: Perform advanced maintenance tasks, including system updates, patch management, performance tuning, and optimization.\u00b7 Automation and Scripting: Develop, implement, and maintain automation scripts and processes using tools such as PowerShell, Azure CLI, and Terraform.\u00b7 DevOps Practices: Collaborate with DevOps teams to integrate infrastructure management with CI/CD pipelines, promoting a seamless development and deployment process.\u00b7 Infrastructure as Code (IaC): Utilize Terraform to manage infrastructure as code, ensuring consistency, scalability, and reliability.\u00b7 Documentation and Knowledge Sharing: Maintain comprehensive documentation of configurations, processes, and procedures. Mentor and train junior engineers.\u00b7 Security and Compliance: Ensure the infrastructure complies with security policies, best practices, and regulatory requirements.\u00b7 Collaboration and Leadership: Work closely with other IT teams, providing guidance and leadership to ensure effective and efficient infrastructure management.\u00b7 Continuous Improvement: Identify and implement improvements to processes, tools, and technologies to enhance the overall performance and reliability of the Azure infrastructure.Required Mandatory experience:\u00b7 Azure Administration: Minimum of 8years of hands-on experience managing and supporting Azure environments.\u00b7 Terraform Expertise: Extensive experience with Terraform for managing infrastructure as code, including creating, modifying, and managing infrastructure resources.\u00b7 ServiceNow Proficiency: Proficiency in using ServiceNow for advanced ticket handling, including creating, updating, resolving high-priority tickets, and managing workflows.\u00b7 Automation Skills: Strong experience in automation using PowerShell, Azure CLI, and other relevant tools.\u00b7 DevOps Practices: Proven experience in integrating infrastructure management with CI/CD pipelines and collaborating with DevOps teams.Troubleshooting and Problem Resolution: Demonstrated ability to diagnose and resolve complex technical issues efficiently\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "260_Sr. AWS Cloud Engineer_Atlanta, GA(Onsite)": "Jernisha,\nWilco Source\njernishaj@wilcosource.com\nReply to: jernishaj@wilcosource.com\nLocal to Atlanta, GA Candidates onlyInperson Interview Must4+ years of experience in designing and implementing large enterprise public cloud environments. AWS Certified Professional preferred.2+ years of hands-on experience integrating mission-critical applications into AWS.Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.).Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM, or similar tools.Fluency in one or more scripting/coding languages such as Java, JavaScript, REST, JSON, Python, bash.Experience implementing and leveraging logging and monitoring solutions.Experience with cloud-native architectures and microservices design.Understanding of the shared responsibility model in AWS.Proven track record in major enterprise-level projects.Experience designing complex distributed systems environments.Strategic thinking about business, product, and technical challenges.Excellent communication and presentation skills at various organizational levels.Strong organizational and analytical skills with attention to detail.Independent, self-motivated, and thorough worker.Demonstrated teamwork skills, innovation, creativity, and strong problem-solving abilities.Job Responsibilities:Provide technical leadership and design expertise for AWS Foundation integration.Define standards and patterns for efficient AWS service utilization.Lead automation of AWS integration processes and tasks.Develop and maintain Public Cloud Usage Statistics and Reporting dashboards.Create frameworks and tools to enable development teams to use authorized AWS services.Collaborate with the DevTools team to develop and deploy a CI/CD pipeline using tools such as GitLab, Maven, and Jenkins.Lead automation development using Ansible or CloudFormation to accelerate application delivery into AWS.Ensure Security-by-Design is integral to every app deployed into AWS.Apply enterprise monitoring standards to each AWS app deployment.Manage Intranet FAQs, user guides, knowledgebase, standards, and build documents for AWS Foundations integration.Identify and implement new concepts, ideas, techniques, and technologies, including best practices, guidelines, and design patterns.Perform cost-benefit analysis to determine the best system architecture (software, hardware, internal cloud, externally hosted, etc.).Mentor engineering and development staff on public cloud integration design and hygiene.Partner with peer infrastructure teams to provide enterprise-class AWS integration.Design and maintain standard templates, reference architectures, and design patterns to aid Cloud Engineers in creating standards-based designs.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "261_AWS Architect (Enterprise Architect)": "sandeep,\nyochana\nsandeepb@yochana.com\nReply to: sandeepb@yochana.com\nHi This is Sandeep from Yochana IT Solutions Inc, I hope you are doing good.We have urgent requirement with one of our client for AWS Architect (Enterprise Architect). Kindly go through the requirement below and let me know your interest.Job Title: AWS Architect (Enterprise Architect)Location: San Antonio, TX (onsite)Employment type: ContractRole & Responsibilities Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus. Thanks & Regards,Bitla Sandeep\u2013 Resource SpecialistYochana IT Solutions Inc. E : Sandeepb@Yochana.com F : 248 876 4228A : 23000 Commerce Drive, Farmington Hills, MI 48335 www.yochana.com/ Please follow us on: USA | CANADA | INDIA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "262_urgent new requirement for  AWS Technical Lead  9800 Fredericksburg Rd, San Antonio, TX 78288  12 months": "rajesh,\ninfoways\nrajesh@info-ways.com\nReply to: rajesh@info-ways.com\nHi, Hope you are doing great. We have an urgent requirement for the below position with our IT Client HCL.Kindly reply back to this email or call me to discuss further. position : AWS Technical Leadlocation : 9800 Fredericksburg Rd, San Antonio, TX 78288duration : 12 months Mandatory required skills \" AWS Sagemaker, AWS Neptune, GCP Vertex AI Job Description: The DevOps Engineer's primary responsibility is to ensure the successful implementation and management of AI/ML services/components for USAA Enterprise. This includes AWS Sagemaker, AWS Neptune, GCP Vertex AI. High level job responsibilities:-Will work with the engineering team and the infrastructure team in order to achieve all the milestones needed for the scope of work.Work with implementation engineers to ensure platform components are working as intended for day to day use.Work with infrastructure engineers/team to ensure platform components are implemented in a manner befitting USAA's policy and guardrails.Monitor and improve AI/ML components in AWS/GCP to ensure the continued quality delivery of functions in the AI/ML space.Candidate needs to be able to independently work and be self-driven in to successfully enable and implement the AWS Sagemaker, Neptune and GCPDirectly working with a fast paced team and stakeholders ranging from engineers all the way to the EMG group.This candidate will need to assess the scope of work and be able to come up with action items/plans with minimal.Best RegardsRajeshInfoWaysPhone#: 609-858-0846Email: Visit us at: www.info-ways.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "263_Urgent: AWS Data Engineer:: Scottsdale AZ (Onsite)": "Mahender Singh,\nDigitaldhara\nmahender@digitaldhara.com\nReply to: mahender@digitaldhara.com\nAWS Data EngineerLocation : Scottsdale AZ (Day 1 onsite)Hire type : Contractor. Must have :IAM , AWS , Glue , S3 Redshift , Kinesis , Python/Java , Scala RDS SQL ServerAWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Needs someone who can understand and work on AWS echo system - Glue , Kinensis, DynamoDB etc. ) and the access permissions and especially terraform infra as code.It is combination of skills AWS DE + some hands-on knowledge in AWS infra. Position Summary:We are seeking a highly skilled AWS Data Engineer with extensive experience in AWS technologies to join our team as a contractor.The ideal candidate will have a strong background in designing, building, and maintaining data pipelines, and must be capable of contributing immediately to our ongoing projects.Key Responsibilities:- Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage.- Implement data ingestion processes to handle streaming and batch data.- Ensure data quality and integrity through robust ETL processes.- Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS.- Optimize and tune data processing workflows for performance and cost efficiency.- Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability.- Document data architecture, processes, and workflows.Qualifications:- Bachelor's or master\u2019s degree in computer science, Engineering, or a related field.- Minimum of 5 years of experience in data engineering, with a focus on AWS technologies.- Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server.- Strong proficiency in SQL and experience with database design and optimization.- Expertise in ETL/ELT processes and tools.- Familiarity with data warehousing concepts and best practices.- Experience with data modeling and schema design.- Proficiency in programming languages such as Python, Java, or Scala.- Knowledge of data governance and security best practices in a cloud environment.- Excellent problem-solving skills and the ability to work independently with minimal supervision.- Strong communication and collaboration skills.Qualifications:- AWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.- Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL Server- Familiarity with big data technologies like Apache Spark or Hadoop.- Experience with reporting and visualization tools like Tableau- Knowledge of DevOps practices and tools for CI/CD such as Jira and Harness.mahender@digitaldhara.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "264_AWS LEAD - San Antonio, TX ( Onsite )": "Gopi Chand,\nYochana IT\ngopi@yochana.com\nReply to: gopi@yochana.com\nHello, I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider with 14 plus years of experience delivering value to leading companies across the U.S and Canada. I'm currently staffing for AWS LEAD - San Antonio, TX ( Onsite ). Below you will find the job description, if you are qualified and interested please send me your Updated Word Document Resume. My apologies if this position is not an ideal fit, we'll keep you in mind for other suitable positions and referrals would be appreciated. Thank you in advance. Role: AWS LEADLocation: San Antonio, TX ( Onsite )Vendor / Client: Direct Vendor Duration: Contract Job Summary:Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Thanks and regardsGopi ChandTechnical Team Lead Yochana IT Solutions Inc23000 Commerce Dr., Farmington hills, MI-48335gopi@Yochana.com || www.yochana.comLet\u2019s Challenge The Status Quo\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "265_AWS Architect OR AWS Lead Location: San Antonio, TX": "SAM,\nAventure Systems LLC\nsam@aventuresys.com\nReply to: sam@aventuresys.com\nJob Title: AWS Architect (Enterprise Architect_)Location: San Antonio, TX Rate :70Mandatory Skillset Overview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands-on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Job Title: AWS LEAD (SENIOR TECHNICAL LEAD)Location: San Antonio, TX Rate :60Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:\u00b7 Participate in producing conceptual, solution and component-level architectures and associated artifacts. \u00b7 Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. \u00b7 Develop common components for shared business capabilities with Standards and best practices. \u00b7 Develop the transition architecture using services offered by AWS and private cloud.\u00b7 Migrate legacy applications to AWS and private cloud. \u00b7 Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7 Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7 Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7 Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7 Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7 Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7 Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7 Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. \u00b7 Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. \u00b7 Ability to design patterns for moving legacy applications to AWS and private cloud. \u00b7 Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) \u00b7 Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) \u00b7 Experience with SAFe framework or other similar agile frameworks. \u00b7 Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. \u00b7 Guidewire experience is big plus. Job Title: AWS LEAD (TECHNICAL LEAD)Location: San Antonio, TXRate : 60 Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:\u00b7 Participate in producing conceptual, solution and component-level architectures and associated artifacts. \u00b7 Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. \u00b7 Develop common components for shared business capabilities with Standards and best practices. \u00b7 Develop the transition architecture using services offered by AWS and private cloud.\u00b7 Migrate legacy applications to AWS and private cloud. \u00b7 Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7 Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7 Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7 Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7 Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7 Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7 Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7 Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. \u00b7 Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. \u00b7 Ability to design patterns for moving legacy applications to AWS and private cloud. \u00b7 Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) \u00b7 Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) \u00b7 Experience with SAFe framework or other similar agile frameworks. \u00b7 Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. \u00b7 Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "266_Senior AWS Cloud Engineer Atlanta, Georgia only local or usc or Gc": "Badal kanojia,\nStellentit\nbadal@stellentit.com\nReply to: badal@stellentit.com\nSenior AWS Cloud EngineerAtlanta, GeorgiaPhone + SkypeLocals onlyOnsite interview may be requiredJob description:Qualifications:- Experience in design and implementation of large enterprise public cloud environments - AWS Certified Professional - Hands-on experience integrating mission critical applications into AWS - Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.) - Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools - Fluency with one or more scripting/coding languages such as Java, Javascript, REST, JSON, Python, bash - Experience implementing and leveraging the logging and monitoring solutions - Experience in cloud native architectures and micro-services design - Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects - Experience in design of complex distributed systems environments - Demonstrated ability to think strategically about business, product, and technical challenges - Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail Responsibilities:- Provide technical leadership and design expertise for AWS Foundation integration- Define standards and patterns for when and how to efficiently leverage AWS services- Lead automation of AWS integration processes and tasks- Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners- Develop frameworks and tools to enable dev teams to consume authorized AWS services- Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins- Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS- Ensure Security-by-Design is a foundational component of every app deployed into AWS- Ensure enterprise monitoring standards are applied to each AWS app deployment- Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration- Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies)- Perform cost benefit analysis to determine best system architecture via software, hardware, internal cloud, externally hosted or other defined model- Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs- Partner with peer Infrastructure teams to provide enterprise class AWS integration- Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designs.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "267_AWS Cloud Engineer  Atlanta- GA-Hybrid": "Gulshan,\nStellent IT\ngulshan@stellentit.com\nReply to: gulshan@stellentit.com\nAWS Cloud EngineerAtlanta, GA-HybridPhone+Skype(F2F May Be)6+MonthJob Description Qualifications:- Experience in design and implementation of large enterprise public cloud environments - AWS Certified Professional - Hands-on experience integrating mission critical applications into AWS - Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.) - Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools - Fluency with one or more scripting/coding languages such as Java, Javascript, REST, JSON, Python, bash - Experience implementing and leveraging the logging and monitoring solutions - Experience in cloud native architectures and micro-services design - Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects - Experience in design of complex distributed systems environments - Demonstrated ability to think strategically about business, product, and technical challenges - Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail - Independent and self-motivated and very thorough worker WHAT ELSE? - Candidates must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.Responsibilities:- Provide technical leadership and design expertise for AWS Foundation integration- Define standards and patterns for when and how to efficiently leverage AWS services- Lead automation of AWS integration processes and tasks- Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners- Develop frameworks and tools to enable dev teams to consume authorized AWS services- Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins- Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS- Ensure Security-by-Design is a foundational component of every app deployed into AWS- Ensure enterprise monitoring standards are applied to each AWS app deployment- Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration- Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies)- Perform cost benefit analysis to determine best system architecture via software, hardware, internal cloud, externally hosted or other defined model- Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs- Partner with peer Infrastructure teams to provide enterprise class AWS integration- Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designs\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "268_Looking for Azure Cloud Engineer need 10+yrs need locals to NY and Princeton NJ": "Austin varma,\nEminencets\naustin@eminencets.com\nReply to: austin@eminencets.com\nRole: Cloud DevOps EngineerDuration: Long TermLocation: Princeton NJ or NYC office (1166 Avenue ofAmericas, New York)Hybrid: 3 days WFORoles and Responsibilites: Azure Cloud Engineer (4-5 Years Azure experience, Worked on Azure APIM/Front Door/App Gateway/Load Balancer/Logic Apps/Function Apps/Storage/Key Vaults/ARM templates/Docker/Kubernetes/GH Actions/Azure DevOPs etc.) Expertise in deploying .Net based apps, Docker/Kubernetes/GH Actions/Azure DevOps). Thanks, and regards.Austin VarmaRecruiter Eminence Technology Solutions LLCEmail: Austin@eminencets.comWebsite: www.eminencets.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "269_Mobility - AWS AppSync Or TypeScript Developer Lead": "Santhoshi,\nHAN IT Staffing\nsanthoshi@hanstaffing.com\nReply to: santhoshi@hanstaffing.com\nRole: Mobility_AWS AppSync / TypeScript Developer Lead - OnsiteWork Location: ATLANTA, GA (Office Location)Client : CapgeminiRoles and Responsibilities:Responsibilities:\u2022 Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.\u2022 Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.\u2022 Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.\u2022 Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.\u2022 Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.\u2022 Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.\u2022 Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.\u2022 Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.\u2022 Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.\u2022 Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.\u2022 Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.\u2022 Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.Requirements:\u2022 Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.\u2022 Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.\u2022 Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.\u2022 Experience coding in Swift is a plus.\u2022 Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.\u2022 Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.\u2022 Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.\u2022 Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.\u2022 Familiarity with Charles Proxy or similar tools for network analysis and debugging.\u2022 Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.\u2022 Experience with CI/CD pipelines and DevOps practices is a plus.\u2022 Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.\u2022 Strong documentation skills to maintain clear and concise technical documentation.??TypeScript, AWS AppSync, AWS Lambda, Node.JS.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "270_Cloud Engineer": "sandeep,\nyochana\nsandeepb@yochana.com\nReply to: sandeepb@yochana.com\nHi This is Sandeep from Yochana IT Solutions Inc, I hope you are doing good.We have urgent requirement with one of our client for Cloud Engineer. Kindly go through the requirement below and let me know your interest.Job Title: Cloud EngineerLocation: Seatle, Washington - OnsiteEmployment type: ContractRole & ResponsibilitiesKey Responsibilities\u2022 Oversee the network onboarding process for new users and systems into Cloud environment\u2022 Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.\u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.\u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.\u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.\u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.\u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements.\u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.\u2022 Provide technical support and troubleshooting for Cloud network-related issues\u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications\u2022 Experience with network onboarding and provisioning in cloud environments.\u2022 Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).\u2022 In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.\u2022 Familiarity with security regulations, standards, and compliance requirements for cloud network environments.\u2022 Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.\u2022 Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.\u2022 Strong communication and documentation skills for collaborating with cross-functional teams.\u2022 Experience with automation tools (e.g., Python, Terraform) for network configuration and management\u2022 Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred. Thanks & Regards,Bitla Sandeep\u2013 Resource SpecialistYochana IT Solutions Inc. E : Sandeepb@Yochana.com F : 248 876 4228A : 23000 Commerce Drive, Farmington Hills, MI 48335 www.yochana.com/ Please follow us on: USA | CANADA | INDIA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "271_SRE Lead with Kubernetes (Local to CA)": "sanjeev,\nHMG America LLC\nsanjeev@hmgamerica.com\nReply to: sanjeev@hmgamerica.com\nSRE LeadLong Term ContractSFO, CA (Onsite) Qualifications: \u00b7 The ideal candidate will have a strong background in production monitoring, a deep understanding of development and operations, and a proven track record in managing and scaling distributed systems in a public, private, or hybrid cloud environments.\u00b7 Understanding of SRE principles, including monitoring, alerting, fault analysis, and other common reliability engineering concepts, with a keen eye for opportunities to eliminate toil by code and process improvements. \u00b7 Expertise in infrastructure as code (IAC), configuration management, build automation, source control, and CI/CD tools (e.g., Terraform, CloudFormation, Ansible, GitHub, Artifactory, Jenkins).\u00b7 Deep understanding of containerization and orchestration technologies (e.g., Docker, Kubernetes). \u00b7 Experience with monitoring and logging tools (e.g., Prometheus, Grafana, Dynatrace, Splunk) and incident response processes. \u00b7 Proficient in Java, .NET, Web UI/JavaScript Frameworks and scripting languages such as Python, Bash, and PowerShell. \u00b7 High-level understanding of the different layers of the Tech stack and how they come together to provide a service (e.g. network, compute, storage, OS (Linux, Windows), supporting services, application layer). Responsibilities: \u00b7 Key measures of success will include platform stability, effective integration and delivery, instrumentation, release quality, technical debt(toil) reduction, development of automation, risk/security compliance, and sustained advancement of the SRE practice. \u00b7 Design & implement scalable, automated, monitored, and well-documented systems to accelerate the development of the services running in the AWS and Azure cloud. \u00b7 Configure, tune, and fix multi-tiered systems to achieve optimal application performance, stability, and availability. \u00b7 Be part of an on-call rotation providing hands-on technical expertise during service-impacting events. \u00b7 Apply troubleshooting skills, debugging tools, and examine logs, telemetry, and other methods to verify assumptions and customer impact. Lead blameless postmortems for root cause and production resiliency.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "272_AWS Solution Engineer - Little Rock, AR -  LOCALS Only - 14+ years of experience": "Sai,\no2f\nsai@o2finc.com\nReply to: sai@o2finc.com\nThe DFA AWS Cloud Solutions Engineer is responsible for analyzing user requirements and procedures to migrate, modify, and support existing systems, and review system-wide capabilities and workflows to efficiently manage and secure these very large systems for the AIRS Service Center. Roles and ResponsibilitiesProvide primary operational support and engineering for the public cloud platform, and debug and optimize systems and automate routine tasks.Plan, design, and implementation of workload migrations from on premises to AWS Cloud.Configure, monitor and assist with the maintenance and support of the infrastructure environment within the AWS environment.Configure, tune, and maintain AWS services like CloudWatch, CloudTrail, GuardDuty, Security Hub, Systems Manager, Network Firewall, WAF, Security Group, Tagging, Network ACL\u2019s and Routing Tables.Setup and maintain Gateway Load Balancer and Internet Gateway.Setup and maintain VPC\u2019s in different Availability Zones, including WFCS and SQL installations and HAG.Setup and maintain backup solutions using AWS native services. This includes EC2 instances, FSX Servers, various S3 storage types, AMI, and Infrastructure as Code Scripts like Cloud Formation and Terraform.Configure, automate, and monitor AWS CloudWatch alarms and notifications for system(s) health checks.Design, develop and maintain Executive Level dashboards for system(s) utilization, health, and cost.Setup and maintain security using tools like AWS Guard, AWS OU, IAM, CrowdStrike, and Tenable.Environment hardening to meet compliance requirements and configuration rules for Pub 1075, PII, and NIST frameworks.Collaborate with other IT resources and vendors to resolve application and/or infrastructure related issues to the existing AWS environment.Maintain technical documentation of the existing AWS environment related to storage, backups, & patching.Participate in the selection of new technologies while maintaining departmental information technology and security standards.Effectively communicate with Management, co-workers, AMS, and business partners.Perform other duties as assigned. SkillRequired / DesiredAmountof Experience Hands-on experience migrating/implementing significant applications on AWS Cloud platformsRequired4Years AWS web environment design and build experience, which includes AWS services like EC2, ELB, Systems Manager, FSX, SES, SNS and S3Required4Years Experience using AWS security solutions such as WAF, Network Firewall, Security Group, Tagging, Network ACL\u2019s, Routing Tables, Gateway Load BalancerRequired4Years Experience using Internet Gateway, AWS Guard, SecurityHub and GuardDutyRequired4Years Experience with high volume, mission critical applications and their interdependencies with other applications and databasesRequired4Years Infrastructure to Code scripting using Cloud Formation, or other languages, and AMIRequired4Years Monitoring, maintenance, and support of the infrastructure within AWS environmentsRequired4Years Experience using monitoring solutions like CloudWatch and CloudTrailRequired2Years Experience configuring CrowdStrike and TenableRequired2Years Experience and knowledge of AWS network-level logging configuration and managementRequired4Years Configuration and maintenance of AWS dashboardsRequired4Years Experience with all facets of IT including Networking, DB Systems, Security, DevOps, Backup, DR and modern development methodologiesRequired4Years Experience designing and implementing common shared services across enterprise applicationsRequired4Years Strong analytical skills and ability to resolve complex business/IT problemsRequired4Years Microsoft Windows Server System Administrator experienceRequired GenTax architecture and application maintenance experienceDesired Familiarity with industry security standards and best practices (PUB1075, FIPS, CIS, NIST, PII)Highly desired Strong communication skills, both oral and writtenHighly desired Availability to provide 24 hour supportHighly desired Experience using Incident Management software solutions like JiraHighly desired AWS Solutions Architect certificationHighly desired\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "273_Urgent Position !! AWS Solution Architect Required in Minneapolis, MN (On-Site)": "Suman Gupta,\nKK Software Associates\nsuman.g@kksoftwareassociates.com\nReply to: suman.g@kksoftwareassociates.com\nHello Folks,Hope you are doing great.Kindly go through the job description if you are interested, please share your updated resume at Suman.g@kksoftwareassociates.com or you can call me at (614) 699-5977 Job Position: AWS Solution ArchitectLocation: Minneapolis, MN (On-Site)Duration: Long TermsRequired Qualifications:Bachelor\u2019s degree or foreign equivalent required. Will also consider three year of relevant work experience in lieu of every year of education.In depth knowledge of key AWS services - EC2, S3, RDS, ELB, VPC, Elasti Cache, CloudFormation, CloudWatch, Route 53, CloudFront, SNS, IAM, ECS, EKS, Direct Connect, Service Catalog, Systems Manager, Code Deploy, Code Pipeline etc.Experience in AWS Foundation, Discovery and assessment and migration of Infrastructure.Experience with Developing alternate technology solutions blueprint (cloud-based computing, storage, etc.) to build case for legacy technology modernization.Develop recommendations for operating model development to implement transformation initiatives.Hands on experience on cloud strategy and suitability assessment and definition.Proven knowledge of evaluating the AWS hosting consumption charges and cost optimization.Ability to quickly establish credibility and trustworthiness within key stakeholders.Experience in Managing delivery in Global Delivery Model with onsite/offshore resources.U.S. citizens and those authorized to work in the U.S. are encouraged to apply. We are unable to sponsor currently.Preferred Qualifications:Strong facilitation skills and a clear ability to build strong relationships with business partners at all levels.Excellent verbal, written and presentation skillsResults driven, and able to collaborate with management and colleagues to share the responsibilities for achieving an end-to-end solution for customers.Provide technical perspectives to other architecture functions to ensure that solutions effectively leverage infrastructure capabilities and services and integrate with them.Excellent initiative, organization, and customer service skills.Experience and desire to work in a Global delivery environment.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "274_Senior Software Engineer - Must Have: (Serverless - AWS TypeScript React expertise - Palo Alto CA  (Healthcare Domain)": "John,\nVYsystems\njohn@vysystems.com\nReply to: john@vysystems.com\nst Have: (Serverless/AWS/TypeScript/React expertise)AWS Devops BackgroundRequired Qualifications:Key TechnologiesStrong foundation in TypeScript and React for frontend developmentWell-versed designing and integrating GraphQLStrong experience with AWS Cloud Development Kit (CDK)Ability to leverage Serverless Framework, CloudFormation, and TerraformProficient in designing applications for AWS cloud environmentsLambda (TypeScript/Node), SQS, EventBridge, CloudFront, API Gateway, DynamoDB, RDS (MySQL), and CognitoSuccess in building large-scale web projects across the full development stackUnderstanding of event-driven design and architecture principlesExperience with data-driven innovation and analysis techniquesExcellent troubleshooting and debugging skillsCore competencies in full-stack software engineering best practices and tools, including:Infrastructure as Code (IaC) toolsTesting and validation frameworks like JestContinuous Integration and Continuous Deployment (CI/CD) pipelines using CircleCIAgile issue tracking software such as JiraVersion Control systems like GitHubObservability and real-time telemetry solutions such as Datadog, Prometheus/GrafanaFamiliarity with DERN (DynamoDB, Express, React, Node.js) development stacks Preferred Qualifications:Demonstrated ability to take ownership of contributions and ensure timely deliveryAbility to document requirements, architectural designs, and analysis findings effectivelyFamiliarity with OAuth2, OIDC/SAML integrations for Authorization and AuthenticationExperience with test-driven development methodologies to ensure code quality and reliability Key Responsibilities:Collaborate closely with technical and business team stakeholders using tools such as Zoom, Slack, GitHub, Miro, and GSuite to drive project successEnhance operational excellence and the quality of deliverables through continuous improvement initiativesOptimize existing technology stacks for scalability, observability, and reliability to meet evolving business needsContribute to the design, implementation, and delivery of current roadmap priorities and new product offeringsRapidly acquire proficiency in technologies across distributed systems to contribute effectively to project objectives\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "275_C2H || We have multiple AWS role in San Antonio, TX": "Amit,\nTechStar Group\namit.kumar@techstargroup.com\nReply to: amit.kumar@techstargroup.com\nRole: AWS LEAD (Technical Lead)Job Description:Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.Role: AWS LEAD (Senior Technical Lead)Job Description:Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASLooking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.Role: AWS Architect (Enterprise Architect)Location: San Antonio, TX Job Description:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross-domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.Regards,Amit Kumar\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "276_Azure Data Engineer, Onsite Day1, Weehawken NJ": "Sunita Sharma,\nTech Source Inc\nsunita@tsourceinc.net\nReply to: sunita@tsourceinc.net\nAzure Data AnalystLocation:-Weehawken NJOnsite Role from Day1, KRA'sManage and analyze large datasets using Azure data services to inform business decisions. Build and maintain data pipeline architecture within Azure. Assemble complex data sets that meet functional business requirements. Identify, design, and implement internal process improvements.responsible for collecting, analyzing, and visualizing data from various sources using Azure services. They work with large amounts of data to identify trends, patterns, and insights that can help businesses make informed decisions.dissect complex data sets, identify patterns, and derive insights that can drive business decisions.analyze and interpret data within the Microsoft Azure cloud ecosystem\u2022Responsible for designing, building, and maintaining scalable data pipelines and infrastructure\u2022Hands on with Microsoft Azure Databricks, Microsoft Azure Data Factory, Microsoft Azure SQL, Microsoft Azure Cloud Services, Azure Data Lake ADLS Azure Data Lake Storage\u2022Day to day activities will include collaborating with cross functional teams, analyzing data requirements, and implementing data solutions using your expertise in Data Engineering\u2022Design and develop scalable data pipelines and infrastructure using Data Engineering expertise\u2022Ensure data quality and integrity by implementing data governance and best practices\u2022Optimize data processing and storage for performance and cost efficiency\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "277_GCP Terraform Engineer - GCP Devops Local to AZ or ex Amex - PP no must": "Prashanth,\nBrillius\nprashanthn@brillius.com\nReply to: prashanthn@brillius.com\nJob Title: GCP Terraform Engineer (Kubernetes) Location: Phoenix, AZ Onsite/Hybrid roleContract C2C Job Description:We are seeking a talented GCP/GKE Engineer with strong experience in deploying and managing infrastructure on Google Cloud Platform, particularly using Google Kubernetes Engine (GKE). The ideal candidate should also have extensive hands-on experience with Terraform for infrastructure as code (IaC) automation.Responsibilities:\u00b7 Design, deploy, and manage applications and services on GCP using best practices for cloud infrastructure architecture.\u00b7 Implement and manage Kubernetes clusters using GKE, ensuring scalability, security, and reliability.\u00b7 Develop and maintain Terraform scripts/modules for infrastructure provisioning and configuration management.\u00b7 Collaborate with cross-functional teams (DevOps, developers, and architects) to design and implement cloud solutions.\u00b7 Monitor, troubleshoot, and optimize cloud infrastructure and applications to ensure high availability and performance.\u00b7 Implement CI/CD pipelines for automated deployment using tools like Jenkins, GitLab CI/CD, etc.\u00b7 Ensure compliance with security standards and best practices in cloud environments.\u00b7 Document infrastructure and processes, and participate in knowledge sharing sessions.Requirements:\u00b7 Bachelor\u2019s degree in Computer Science, Engineering, or a related field (or equivalent experience).\u00b7 Proven experience as a GCP/GKE Engineer with hands-on experience deploying and managing Kubernetes clusters in production environments.\u00b7 Strong proficiency with Terraform and infrastructure as code principles.\u00b7 Experience with Docker containers and container orchestration tools.\u00b7 Solid understanding of networking concepts, cloud security best practices, and monitoring tools.\u00b7 Familiarity with CI/CD pipelines and related tools.\u00b7 Excellent problem-solving skills and ability to work independently as well as part of a team.\u00b7 Strong communication skills and ability to collaborate effectively with stakeholders at all levels.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "278_Enterprise Azure Architect - Dallas TX - Onsite": "Ganesh,\nBisoftLLC\nganesh.t@bisoftllc.com\nReply to: ganesh.t@bisoftllc.com\nHi,Hope you are doing great,Hiring on Enterprise Azure Architect - Dallas TX - OnsiteWork : Dallas TX - Onsite Mandatory Skills Required:Need to have strong principles and familiarity with Iac. Mindset to create resources only through code/Automation and some pipeline. Security mindset - Good concepts on applying policies (Writing, Policy initiatives. Scope etc) aligning with sec benchmarks and guardrails. Basic understanding of networking constructs Strong foundation on Landing zone set up. Knowledge of best practices (Management group, subscription etc) Observability -Knowledge of Best practices on monitoring , logging etc. - knowledge on sending obs data to third party tool (Splunk, Event Hub etc) Azure Expertise - Experience on Permissions & RBAC, Best Practices on Monitoring and Logging, Landing Zone Implementation, Cloud Infrastructure Automation Extensive Terraform experience in Infrastructure Automation, state management, secret handling, and drift detection,\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "279_AWS Engineer : Cincinnati, OH :: Contract": "Jaffar,\nATVS\nshaik.jaffar@atvsllc.com\nReply to: shaik.jaffar@atvsllc.com\nAWS EngineerLocation: Cincinnati, OHMandatory Skills: AWS EKS,NLB,MQ,EFS and Basic AWS ServicesSkills: AWS sysops\"U.S. Citizens and Green cards are encouraged to apply for this position. We are unable to sponsor at this time.\"Planning and designing the cloud infrastructure with AWSMaintaining and deploying the cloud applicationsTroubleshooting and resolving issues with the cloud infrastructureProficient in deploying, managing, and scaling containerized applications using Amazon EKS. Experience in configuring Kubernetes clusters, managing workloads, and integrating with other AWS services for enhanced container orchestration and microservices architecture.Experienced in deploying, configuring, and managing Amazon RDS instances for various database engines (MySQL, PostgreSQL, SQL Server, etc.). Proficient in tasks such as backup and recovery, performance tuning, and high availability setup using multi-AZ deployments and Read Replicas.Experienced in setting up and configuring Amazon CloudWatch for monitoring and logging AWS resources and applications.Should have experience on creating custom metrics, setting up alarms, and using CloudWatch Logs for centralized logging and troubleshooting.Proficient in managing and administering Linux-based systems, including server configuration, user management, shell scripting, and performance tuning.Ensuring stable and secure Linux environments for running cloud-based applications and services.Designing and implementing secure network solutions that meet business requirements.Creating and configuring virtualized systems in the AWS environment.Performing infrastructure upgrades and updates to maximize system efficiency while minimizing downtime.Deploying applications in AWS using EC2 instances.Creating blueprints using CloudFormation templates for common workloads.Having experience on disaster recovery procedures is an added advantage.Implementing automation using scripting languages (e.g., Python, Perl) to manage AWS services.Building tools for deployment, monitoring, and troubleshooting of system resources in an AWS environment. Knowledge in Java/Python .NET that interact with AWS cloud services by leveraging the AWS APIs.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "280_AWS Solution Architect - Minneapolis": "Mahesh Kumar,\nKK Associates LLC\nshetty.m@kksoftwareassociates.com\nReply to: shetty.m@kksoftwareassociates.com\nHi, We have a job opportunity with one of our clients. If you have any matching profiles please get in touch with me Role Description:AWS Solution ArchitectCompetencies:Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years):4-6Essential Skills:AWS Solution ArchitectDesirable Skills:AWS Solution ArchitectCountry:United StatesBranch | City | Location:Minneapolis Downtown, MNMINNEAPOLISMinneapolisKeywords:AWS Solution Architect Email is the best way to reach me if I missed your callRegards,Mahesh KumarKK Associates LLC.8751 Collin McKinney Pkwy, # 1302, McKinney, TX 75070555 Metro Place North, Suite # 100, Dublin, OH 43017Direct: 925-298-0978Email: shetty.m@kksoftwareassociates.comWeb: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "281_AWS Database architect in San Antonio, TX": "Amit,\nTechStar Group\namit.kumar@techstargroup.com\nReply to: amit.kumar@techstargroup.com\nRole: AWS Database architect Location: San Antonio, TXType: Contract Job Description:A Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.ResponsibilitiesOwn and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. Provide data architectural solutions/designs to project execution teams for implementation. Provide architectural assessments, strategies, and roadmaps for data management. Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. Project and solution estimation and team structure definition Develop Proof-of-Concept projects to validate new architectures and solutions. Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. Mentor and provide architectural guidance to multiple teams building innovative applications. Drive common vision, practices and capabilities across teams. Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "282_Back-fill position --- Full time position--- DevOps Engineers--- Alameda CA (100% on-site from day 1)": "Mohit Jaiswal,\nIntelligenz IT\nmohit.jaiswal@intelligenzit.com\nReply to: mohit.jaiswal@intelligenzit.com\nHi,I hope you and your family are doing well.I have a good position for you with my client. This point of time I don't know whether you are looking for a new job or not. But just thought if I can share the details and then confirm from you about your interest level for the opportunity. If you are interested and an available consultant, send me your most updated resumes in word format and contact details.Job Title: DevOps Engineers Location: Alameda CA (100% on-site from day 1)Duration: Full time position This is Back-fill position. Only one round of customer interview Start Date ASAP... Engage with stakeholders to design cloud environments, Create Design documents, runbooks for new tools and services as applicable.Build designed cloud environments using Terraform, deploy designed environments to test and production, Monitor Deployment and update stake holders.Advanced level of terraform knowledge . Working with modules, use of terraform functions and a custom terraform backend. Knowledge of code quality tools such as TFSec, GitLeaks and Semgrep to identify hard coded keys in the code will be added advantageAdded advantage is use of Atlantis tool to deploy infrastructure through terraform pull requests.Work with Github Actions and OIDC to trigger terraform code to perform infra tasks such as - deploy applications, deploy S3 content,AWS Services :Advanced S3 usage - lifecycle policies, bucket policy, expiration policyLambda : E.g. to gather metrics from other AWS services e.g. S3, IAM etc..,OpenSearchAWS Shield AdvancedDeploy and configure ECS, ECRSSM Setup and ConfigurationSetting up and configuration of SagemakerSetting up and configuration of AWS WorkspacesAWS EC2, ASG , Create custom AMI with agents of tools such as SentinelOne, SCCMSES, SQS and SNS,AWS Control TowerAWS CognitoAWS AppFlowSystems ManagerAWS Trusted AdvisorQuiksightRoute53 as domain hosting serviceSecrets manager to rotate access keys for services such as JIRA or RDSRegards,Mohit JaiswalIntelligenz ITWork : 646-502-7441Maillot: Mohit.Jaiswal@Intelligenzit.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "283_Only W2 Devops Developer  only Citizen": "Ram,\nData Erp System\nramya.k@dataerpsystems.com\nReply to: ramya.k@dataerpsystems.com\nOnly W2.Need Devops engineer100% Remote Mandatory, someone worked on Salesforce application in the devops environment.Min 5yrs exp.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "284_Role : AWS Cloud Technical Architect ---Location: San Antonio, TX-(Onsite only)": "Moorthy,\nIDC\nmoorthy.a@idctechnologies.com\nReply to: moorthy.a@idctechnologies.com\nHiPlease share profiles Email : moorthy.a@idctechnologies.comRole : AWS Cloud Technical ArchitectLocation: San Antonio, TX-(Onsite only) Work Location (with City, State & Zip Code) 9800 Fredericksburg Rd, San Antonio, TX 78288 Client interview required for selection (Yes/No)YesOpen to accept H1 transfer candidates for FTE role (Yes/No)Yes Job Title/Role AWS Cloud Technical ArchitectExperience level required (Years)10 to 15Mandatory required skills Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL TechnologyPreferred/Desired skillsCloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL TechnologyDetailed Job DescriptionJob OverviewA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.ResponsibilitiesOwn and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. Provide data architectural solutions/designs to project execution teams for implementation. Provide architectural assessments, strategies, and roadmaps for data management. Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. Project and solution estimation and team structure definition Develop Proof-of-Concept projects to validate new architectures and solutions. Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. Mentor and provide architectural guidance to multiple teams building innovative applications. Drive common vision, practices and capabilities across teams. Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change Moorthy. A Lead Technical RecruiterDirect : (408) 457-1319. moorthy.a@idctechnologies.comwww.idctechnologies.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "285_CLOUD ENGINEER": "Atiya,\nNetonics inc\natiya.s@netonicsinc.com\nReply to: atiya.s@netonicsinc.com\nRole: Cloud EngineerNO OPT CPTLOCAL TO TX ONLYLocation: Plano TX ( onsite) Mandate Skills:AWS, Terraform JD8-9 years of hands-on experience in AWS cloud engineering with a strong focus on Terraform.Proven experience with Infrastructure as Code (IaC) using Terraform, including writing, testing, and deploying complex Terraform scripts.Experience in any programming language, preferably GO/Python\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "286_AWS Solution Architect - Minneapolis, MN (Onsite)": "Yash,\nItech US\nyash@itechus.net\nReply to: yash@itechus.net\nHelloHope you are doing well. Role: AWS Solution ArchitectLocation: Minneapolis, MN (Onsite) Job Description Role Description: Looking for AWS Solution Architect Best regards, YashITech US, Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "287_Immediate need Devops AppDynamics  Onsite from Day 1 in Austin, TX Rate: 52 on W2 Max": "Shobhana Kulhade,\nSydata Inc\nshobhana@sydatainc.com\nReply to: shobhana@sydatainc.com\nImmediate need Devops AppDynamics \u2013 Onsite from Day 1 in Austin, TX Rate: $52 on W2 Max Devops AppDynamics Consultant \u2013 Onsite from Day 1Austin, TXRate: $52 on W2 MaxInfosys / Apple No C2C Working experience inObservability in general and anything associated with that.Good with unix, troubleshooting and problem solving.AppDynamics or similar tools experience is a must.Must be able to trail blaze projects on their own Thanks & Regards:Shobhana Kulhade \u2013 Sr. IT RecruiterSydata IncEmail: shobhana@sydatainc.com 6494 Weathers Place Suite#100 San Diego, California, 92121Website: www.sydatainc.com Notice: This email contains confidential or proprietary information which may be legally privileged. It is intended only for the named recipient (s). If an addressing or transmission error has misdirected the email, please notify the author by replying to this message by \"REMOVE\"\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "288_ETL Test Lead Azure at Remote": "Dinesh Kumar V,\nVbeyond Corporation\ndineshv@vbeyond.com\nReply to: dineshv@vbeyond.com\nRole : ETL Test Lead with Azure(ADF, Databrick, Azure Devops)Mode : ContractLocation : RemoteAs an ETL Test Lead you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards .Responsibilities:Experience of ETL testing, specifically utilizing ADF ,Databricks and Azure, to test data integration workflows ensuring seamless data flow across diverse systems. Knowledge on Azure based application Ability to review test scenarios and test cases Develop and execute comprehensive test plans, test cases, and test scripts for interoperability projects. Ability to test UI based application also added advantage.Collaborate with development, DevOps, and QA teams to ensure seamless integration and deployment processes.Conduct functional, regression, integration, and performance testing on various systems and applications. Identify, document, and track defects, and work with developers to ensure timely resolution.Ensure testing processes align with industry standards and best practices.Participate in Agile ceremonies, including sprint planning, daily stand-ups, and retrospectives.Utilize automated testing tools and frameworks to enhance test coverage and efficiency.Continuously improve test processes and methodologies to support DevOps practices.Stay updated on the latest industry trends and technologies to ensure the adoption of best practice Ensure data exchange processes comply with industry standards and regulations\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "289_Devops Engineer": "Ragul Saravanan,\nKK Software Associates\nragul.s@kksoftwareassociates.com\nReply to: ragul.s@kksoftwareassociates.com\nHi Everyone Hope you are doing wellExclusive PositionKindly share resumes to ragul.s@kksoftwareassociates.comRole : Devops EngineerLocation : MinneapolisJD: Kuberneter, Docker, AWS, KafkaExperience : 8-10Mandatory Skills : Kuberneter, Docker, AWS, Kafka Regards, Ragul Saravanan |Technical Recruiter KK Software Associates (Global) Pvt Ltd.Contact: +1 (469) 343-1682\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "290_AWS Cloud Security Engineer": "Tejveer Singh,\nVeridian Tech Solutions\ns.tejveer@veridiants.com\nReply to: s.tejveer@veridiants.com\nRole:- Cloud Security Architect Location-Reston VA Day 1 onsite in hybrid set up.NOTE:- Need strong Cloud (AWS only) security and architecture experience.Job Description \u2013The Cloud Security Architect (CSA) will leverage broad technical knowledge of cloud security best practices of key public cloud offerings of providers such AWS, Azure, and GCP to establish secure design patterns, to architect integrations among cloud and/or on-premises infrastructures. This individual must be able to assist in ensuring the security and compliance of the cloud environment based on enterprise cloud security policies, standards, and procedures. The CSA will ensure that solutions operating on the cloud comply with enterprise security requirements in both off-premises and hybrid environment models. The position will work with Enterprise Architects and Application Dev teams to come up with Security Architecture for applications and enterprise tech capabilities migrating to Cloud. Must-Haves: Required qualifications to be successful in this role: 10 years of total IT experience with the following must haves: \u2022 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect \u2022 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u2022 Excellent written and communication skills to report, document and communicate security architecture \u2022 Excellent coordination skills and must be detail oriented Nice-to-Haves: \u2022 Cloud agnostic security architecture experience a plus \u2022 1-2 years of experience in working with NIST assessments of business applications \u2022 Container Security experience to protect container workloads during build and run-time \u2022 API Security architecture experience with industry standard API Gateways \u2022 Security engineering/administration background leveraging SIEM, Network firewalls, host-based security, and security configuration \u2022 One or more industry standard security certification such as CISSP, CCSP or relevant GIAC certifications (ANY) \u2022 One or more Cloud Service Providers Security Specialty Certifications such as AWS Security Specialty or Azure AZ-500 Certification \u2022 The group of skills related to Security including designing and evaluating security systems, identifying security threats, securing computers, assessing vulnerability, etc. \u2022 The group of skills related to Relationship Management including managing and engaging stakeholders, customers, and vendors, building relationship networks, contracting, etc. \u2022 Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand \u2022 The group of skills related to Risk Assessment and Management including evaluating and designing controls, conducting impact assessments, identifying control gaps, remediating risk, etc. \u2022 Experience identifying and determining levels of risk to an organization's networks and systems using cybersecurity techniques \u2022 Working with people with different functional expertise respectfully and cooperatively to work toward a common goal \u2022 Skilled in cloud technologies and cloud computing \u2022 The group of skills related to Influencing including negotiating, persuading others, facilitating meetings, and resolving conflict Key Areas of Responsibility: \u2022 Partner with Enterprise/Portfolio Architecture team and Business Units development squads to collaboratively develop security architectures/designs leveraging approved patterns that ensure applications migrating from on-premise to Cloud, achieving high standards of security practices and compliance. \u2022 Drive the development and adoption of cloud security standards, best practices, and technologies within Enterprise IT infrastructure \u2022 Liaise on security-related issues with internal business stakeholders, InfoSec, Enterprise Architecture, and application development squads \u2022 Work to develop, enhance and document security architecture, security policies, patterns, procedures, guidelines and standards required to design cloud-based solutions \u2022 Educate application, portfolio and solution architects on secure solution design and industry best security practices \u2022 Work on assessments of compliance and standards including and not limited to NIST, FedRAMP, FIPS, etc. \u2022 Support threat modeling and update application security architecture as needed. \u2022 Support application development squads with Security implementations and issues\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "291_SR.Python Developer with AWS experience:: Day 1 Onsite :: Plano TX": "kavyasree,\nIT America\nkavyasree@itamerica.com\nReply to: kavyasree@itamerica.com\nDay 1 OnsiteRole:Sr.Python Developer with Strong AWSLocation: Plano TXDay 1 OnsiteRoles & Responsibilities :AWS servicesDevelop python ETL data pipelines leverage AWS data servicesGlue Kinesis to build scalable data processing applicationsDesign develop test deploy and maintain microservices REST APIs and event-driven architectures onEstablish CI CD pipelines infrastructure as code workflows using Github Git Actions and TerraformInstrument and monitor API gateways lambda functions understand and optimize performance\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "292_Urgent Hiring For DevOps Engineer :::: Austin, Texas": "Kayla Cooper,\nAdame Services\nkayla@adameservices.com\nReply to: kayla@adameservices.com\nHi Partner,We are hiring for the below job role. Please have a look and let me know if you have any suitable candidates. Role-Devops EngineerLocation-Austin area,Texas Responsibilities include (but are not limited to):CI/CD Modernization and automation of the entire pipeline from code check-in to deployment utilizing industry best practices such as Infrastructure as Code (IaC), Configuration as Code (GitOps), and Blue-Green and Canary Deployment Strategies.Implement \"Shift Left\" security approach by integrating security tools and automating security checks and compliance into the CI/CD pipeline.Monitoring and Observability to provide comprehensive monitoring, logging, and alerting for the CI/CD pipeline. Minimum Candidate Characteristics:7 years of DevOps/Software Engineering.DevSecOps automation experience.Java and/or .NET programming experience.Bash, Python, PowerShell scripting and automation experience.Jenkins and CI/CD pipelining experience.Experience coaching/training/mentoring. Exceptional Candidate Characteristics:4 years of Cybersecurity CI/CD pipelining experience.Legacy application experience.At least 3 years of Public sector experience. Thanks | RegardsKayla Cooper |Technical RecruiterC: +13473777344https://www.linkedin.com/in/khushboo-kundra-5b8a47251/www.adameservices.comAdame Services 1309 Coffeen Avenue STE 1200, Sheridan, Wyoming, 82801 | USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "293_AWS LEAD TECHNICAL LEAD || Onsite-San Antonio, TX  || JD": "akshay,\nintellicept\nakushwah@intellicept.com\nReply to: akushwah@intellicept.com\nHi Hope you are doing well Job Role: AWS LEAD (TECHNICAL LEAD)Location: San Antonio, TX Job Description: Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:\u00b7 Participate in producing conceptual, solution and component-level architectures and associated artifacts.\u00b7 Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.\u00b7 Develop common components for shared business capabilities with Standards and best practices. \u00b7 Develop the transition architecture using services offered by AWS and private cloud.\u00b7 Migrate legacy applications to AWS and private cloud. \u00b7 Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7 Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7 Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7 Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7 Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7 Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7 Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7 Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.\u00b7 Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.\u00b7 Ability to design patterns for moving legacy applications to AWS and private cloud.\u00b7 Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)\u00b7 Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)\u00b7 Experience with SAFe framework or other similar agile frameworks.\u00b7 Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.\u00b7 Guidewire experience is big plus. Akshay Kushwah ( He/Him)US IT Recruiterakushwah@intellicept.comIntellicept Inc - A Division of McKinsol Consulting SAP S4 HANA | SAP S/4 Fashion | SAP Staffing| Non-SAP Roles (Intellicept Inc.)Note: We respect your Online Privacy. This is not an unsolicited mail. Under Bill s.1618 Title III passed by the 105th U.S. Congress this mail cannot be considered Spam as long as we include Contact information and a method to be removed from our mailing list. If you are not interested in receiving our e-mails then please reply with a \"remove\" in the subject line and mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience caused to you.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "294_Devops Engineer - Alpharetta,GA-Berkeley Heights,NJ only Locals": "Rakesh,\nBlue Ocean Ventures\nrakesh.d@blue-oceanventures.com\nReply to: rakesh.d@blue-oceanventures.com\nHi,Role: Devops EngineerLinux side :\u00b7 Networking, subnets, DNS, DHCP, route , gateway, load balancer\u00b7 File system \u2013 local, NFS etc\u00b7 Scripting \u2013 shell, python etc\u00b7 Basic OS commands \u00b7 Experience with VMware vSphere and VMs OpenShift Container Platform (OCP) \u00b7 Building OCP\u00b7 Ansible automation\u00b7 Strong Kubernetes skills\u00b7 CKA is a plus.Location : Alpharetta,GA/Berkeley Heights,NJ\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "295_Sr. Cloud Engineer at Atlanta ,GA": "Dheeraj srivastava,\nDatum Software\ndheeraj@datumsoftware.com\nReply to: dheeraj@datumsoftware.com\nCurrently, we have an opening for Sr. Cloud Engineer with our Client in Atlanta, GA || Hybrid. I appreciate your time and look forward to hearing from you. Job Details: Job Title: Sr. Cloud EngineerDuration: Long-Term ContractLocation: Atlanta, GA || Hybrid Responsibilities:Provide technical leadership and design expertise for AWS Foundation integration.Define standards and patterns for when and how to efficiently leverage AWS services.Lead automation of AWS integration processes and tasks.Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners.Develop frameworks and tools to enable dev teams to consume authorized AWS services.Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins.Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS.Ensure Security-by-Design is a foundational component of every app deployed into AWS.Ensure enterprise monitoring standards are applied to each AWS app deployment.Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration.Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies).Perform cost benefit analysis to determine the best system architecture via software, hardware, internal cloud, externally hosted or another defined model.Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs.Partner with peer Infrastructure teams to provide enterprise class AWS integration.Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designs. Qualifications:4+ years of experience in design and implementation of large enterprise public cloud environments.2+ years of hands-on experience integrating mission critical applications into AWS.Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.)Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools.Fluency with one or more scripting/coding languages such as Java, JavaScript, REST, JSON, Python, Bash.Experience implementing and leveraging logging and monitoring solutions - Experience in cloud native architectures and micro-services design.Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects.Experience in design of complex distributed systems environments.Demonstrated ability to think strategically about business, product, and technical challenges.Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail.Independent and self-motivated and very thorough worker. PreferredCandidate must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "296_AWS Architect (Enterprise Architect) at San Antonio, TX (Onsite) for long term Contract": "Jayasurya tanuku,\nYochana IT\njayasurya@yochana.com\nReply to: jayasurya@yochana.com\n\u201cHello, Hope you are doing Well! I'm currently staffing for an AWS Architect (Enterprise Architect) at San Antonio, TX (Onsite). Below you will find the job description, if you are qualified and interested please send me your Updated Word Document Resume to Jayasurya@yochana.com. My apologies if this position is not an ideal fit, we'll keep you in mind for other suitable positions and referrals would be appreciated. Thank you in advance.\u201d Role: AWS Architect (Enterprise Architect)Location: San Antonio, TX (Onsite)Duration: Long term contract on C2C/W2 Mandatory Skills:AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS , Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, AWS Cloud Watch, Kibana, Grafana, Prometheus Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Thanks & Regards,Jayasurya,Resource Specialist,Yochana IT Solutions INC,Farmington Hills, MI,Jayasurya@yochana.com LinkedIn: https://www.linkedin.com/in/jayasurya-t-81a094158/ Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program.Note: If you are not interested in receiving our e-mails then please reply with subject line \u201cRemove\u201d.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "297_AWS Lead | San Antonio, TX (Onsite) | Only for GC &amp; USC": "Ambika,\nTechnosphere Inc.\nambika@technosphere.com\nReply to: ambika@technosphere.com\nJob DescriptionMandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the detailsAWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "298_DevOps Engineer - Contract Role": "Raghu Prasad,\nBlue Ocean Ventures\nraghu.prasad@blue-oceanventures.com\nReply to: raghu.prasad@blue-oceanventures.com\nHi Role: DevOps EngineerLocation: Alpharetta,GA/Berkeley Heights,NJ/Omaha,Nebraska (Local Only 100% OnsiteLinux side :\u00b7 Networking, subnets, DNS, DHCP, route , gateway, load balancer\u00b7 File system \u2013 local, NFS etc\u00b7 Scripting \u2013 shell, python etc\u00b7 Basic OS commands \u00b7 Experience with VMware vSphere and VMs OpenShift Container Platform (OCP) \u00b7 Building OCP\u00b7 Ansible automation\u00b7 Strong Kubernetes skills\u00b7 CKA is a plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "299_AWS Cloud Technical Architect": "Nasir,\nincorporaninc.com\nnasir@incorporaninc.com\nReply to: nasir@incorporaninc.com\nHello Professionals,Position: AWS Cloud Technical ArchitectLocation: San Antonio, TXDuration: 6 Months PlusJob OverviewA Technical Architect is expected to be functionally knowledgeable in multiple Cloud EDW, Snowflake, Informatica, DataStage, DBT, Big Data and NoSQL Technology areas and hands-on experience in data management. As a leader in our Data Solutions, you will provide best-fit architectural solutions for one or more projects; assist in defining scope and sizing of work; and anchor Proof of Concept developments. You will provide solution architecture for the business problem, platform integration with third party services, designing and developing complex features for clients' business needs. You will collaborate with some of the best talent in the industry to create and implement innovative high quality solutions, participate in Sales and various pursuits focused on our clients' business needs.You will also contribute in a variety of roles in thought leadership, mentorship, systems analysis, architecture, design, configuration, testing, debugging, and documentation. You will challenge your leading edge solutions, consultative and business skills through the diversity of work in multiple industry domains. This role is considered part of the Business Unit Senior Leadership team and may mentor junior architects and other delivery team members.ResponsibilitiesOwn and aggressively drive forward specific areas of Cloud EDW technology architecture. This includes data management architectures involving batch, micro-batch, and real-time streaming of data in both cloud and on-premise solutions Architect market leading Snowflake on AWS using Solutions. This includes ETL (Informatica or DataStage) and EDW design. Provide data architectural solutions/designs to project execution teams for implementation. Provide architectural assessments, strategies, and roadmaps for data management. Lead projects within architecture. Work with Product Owner/Business Analysts to understand functional requirements and interact with other cross-functional teams to architect, design, develop, test, and release features. Project and solution estimation and team structure definition Develop Proof-of-Concept projects to validate new architectures and solutions. Support multiple Agile Scrum teams with planning, scoping and creation of technical solutions for the new product capabilities, through to continuous delivery to production. Liaise with offshore team and clients for resolving technical dependencies, issues, and risks. Mentor and provide architectural guidance to multiple teams building innovative applications. Drive common vision, practices and capabilities across teams. Engage with business stakeholders to understand required capabilities, integrating business knowledge with technical solutions Engage with Technical Architects and technical staff to determine the most appropriate technical strategy and designs to meet business needs Demonstrate broad solutions technical leadership, impacting significant technical direction, exerting influence outside of the immediate team and driving change Thanks & Regards, Nasir MahmoodUS IT Recruiter Incorporan Inc.* Nasir@incorporaninc.com / LinkedIn: https://www.linkedin.com/in/nasir-mahmood-25321118a/239 New Road,Building \u2018B\u2019, Suite 206Parsippany, NJ 07054.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "300_Urgent Need: Azure Architect with EventHub, Snowflake and Cosmos DB, 100% Remote": "SAPNA,\nITECS\nsapna@itecsus.com\nReply to: sapna@itecsus.com\nUrgent need from Infocepts. Only US Citizens. Phone + Video interview.100% Remote from US location. Primary Skills: Technologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBT Job Title: Azure Architect with EventHub, Snowflake and Cosmos DBWork Location: Onshore in USA \u2013 100% Remote Purpose of the Position: Purpose of this role is to design and build data architecture for a migration project. Major responsibilities under this role would be to create high level and low level design for data integration framework, translate the business requirements into technical prototypes, help developers in developing the data integration pipelines, perform peer reviews of the code, own the technical delivery of the DI part from inception to the deployment. Also own the technical consultation for existing or new clients during initial phase of any initiative and help in proposals and pre-sales initiatives. Must Have Technical Experience:At least 12 years of IT experience; Minimum 5 years of experience on AzureMust have minimum 5+ years of experience in implementing and managing Azure Cloud components.Expertise in Python and SQL best practicesExperience in data and analytics domainKnowledge of integrating Azure Active Directory with other servicesIn-depth experience of key Azure services for data integration, BI and processing services including but not limited to Azure HDInsight, Azure Databricks, Azure Portal, Log Analytics, Azure Data Factory, , Azure Search, Azure Functions, Azure Stream Analytics, Managed Airflow and Azure Event Hub.Experience in Azure cosmos DB over PostgreSQL, mongo dB and live streaming with Azure EventHub.Experience on Azure functions with Python language.Experience in migration of on-prem applications and data analytics workload to Azure cloud. Experience in designing Azure cloud architecture and operationalizing solutions around monitoring, alerting, logging etc.Working knowledge on Architecture design, Modelling, prototyping, and benchmarking of DataStore/ EDW solutions.Knowledge of variety of advanced architecture, tools, and concepts across all layers of modern distributed technology stack covering the big data ecosystem.Knowledge of Azure storage services such as Azure storage account, Azure Cosmos DB, Azure Time Series Insights, Redis Cache, SQL Databases, Table StorageExperience with programming languages such as PythonExperience in enabling DevOps automation for Azure with appropriate security and privacy considerations.Knowledge of configuring and provisioning networks and infrastructure in cloudGood to have knowledge of Big Data ecosystems such as Hadoop / Hive, HDFS, Spark, YARN and others.Good to have Snowflake, Scala, and Java experienceGood to have Technical Experience:Pre-sales experience will be an added advantage.Any BI experienceExperience in any other cloud technologies like AWS, ALI, GCP.Snowflake, DBT, Scala and JavaTechnologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBTQualifications:Bachelor\u2019s degree in computer science, engineering, or related field (master\u2019s degree is a plus)Demonstrated continued learning through one or more technical certifications or related methods. ---------------------------------------------------------------------------------------------------------------------------Please follow the submission format table provided and ensure all required information is included with each submission with resume. Incomplete submissions will be disqualified. Candidate Legal Name (As per Documents) Summary Year of experience in each skillset: Email Address Phone Number Work Authorization Current Location For non-local, Relocation (Yes/No) Pay Rate Current working status Reason for change LinkedIn Profile Availability for Interview (Please provide 03 time slots) Availability for Joining the project\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "301_Urgent Hire :: Contract :: AWS Solution Architect :: Minneapolis Downtown, MN": "Amit Kushwaha,\nKKSA\nkushwaha@kksoftwareassociates.com\nReply to: kushwaha@kksoftwareassociates.com\nRequirement ID: 9589124Role Description: AWS Solution ArchitectLocation :: Minneapolis Downtown, MNContract Competencies: Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years): 4-6YEssential Skills: AWS Solution ArchitectDesirable Skills: AWS Solution Architect Best Regards,Amit Kushwaha KK Associates LLC. 8751 Collin McKinney Pkwy, # 1302, McKinney, TX 75070555 Metro Place North, Suite # 100, Dublin, OH 43017Direct: +1 (469) 343-1783 | Fax: (614) 413-3428Email: kushwaha@kksoftwareassociates.comWeb: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "302_Need 12+ Years experience - ETL Test Lead with Azure at Remote (Only ETL Tester candidtaes)": "Anita N,\nVbeyond Corporation\nanitan@vbeyond.com\nReply to: anitan@vbeyond.com\nHi,Greetings from Vbeyond Corporation,I hope you are doing well; I am Anita N from VBeyond Corporation. We are a Global Recruitment company with a specialization in hiring IT professionals. One of our clients has Multiple Job Openings for ETL Test Lead with Azure (ADF, Databrick, Azure Devops) at RemoteIf you are interested, kindly send me your updated resume, contact details and a convenient time to talk to you. If you feel this requirement is not suitable for you, kindly share the same with your circle or Reply REMOVE. Role:- ETL Test Lead with Azure (ADF, Databrick, Azure Devops)Location: RemoteType:- Contract As an ETL Test Lead you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards.Responsibilities: Experience of ETL testing, specifically utilizing ADF ,Databricks and Azure, to test data integration workflows ensuring seamless data flow across diverse systems.Knowledge on Azure based application Ability to review test scenarios and test cases Develop and execute comprehensive test plans, test cases, and test scripts for interoperability projects. Ability to test UI based application also added advantage.Collaborate with development, DevOps, and QA teams to ensure seamless integration and deployment processes.Conduct functional, regression, integration, and performance testing on various systems and applications. Identify, document, and track defects, and work with developers to ensure timely resolution.Ensure testing processes align with industry standards and best practices.Participate in Agile ceremonies, including sprint planning, daily stand-ups, and retrospectives.Utilize automated testing tools and frameworks to enhance test coverage and efficiency.Continuously improve test processes and methodologies to support DevOps practices.Stay updated on the latest industry trends and technologies to ensure the adoption of best practice Ensure data exchange processes comply with industry standards and regulations Thanks and Regards,Anita NVBeyond Corporation || PARTNERING FOR GROWTHHillsborough, New Jersey, USAEmail ID: AnitaN@vbeyond.comContact : (908) 589 7315, Ext. 712Website: www.vbeyond.comLinkedIn: linkedin.com/in/anita-n-13b847198\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "303_Terraform Enterprise Architect or Lead with GCP and Terraform Certification  is Must- Austin, TX- onsite": "James Smith,\nYochana Solutions INC\nsmith@yochana.com\nReply to: smith@yochana.com\nJob Title- Terraform Enterprise Architect or Lead with GCP and Terraform experience Location \u2013 Austin, TX- onsite Duration \u2013 1+year Terraform and GCP Certification is Must 12 to 15 years of Minimum experience 5+ Years of experience in GCP 5+ Years of experience in Terraform Enterprise Architect or Terraform Architect Azure/AWS or other cloud providers experience Docker and other virtualization technologies, Scripting language (bash, PowerShell etc) Expert-level proficiency in designing and implementing cloud infrastructure solutions on the google Cloud Platform (GCP) Utilizing Terraform for infrastructure as code (IaC). Extensive hands-on experience with Terraform, including module development, configuration management, and state management, to automate and streamline infrastructure provisioning and management processes. Deep understanding of GCP&#39;s core services and offerings, such as Compute Engine, Cloud Storage, Cloud SQL, VPCs, and IAM, and how to leverage Terraform to provision and manage these resources effectively. Demonstrated ability to utilize Terraform to orchestrate complex, multi-tiered GCP architectures, incorporating best practices for security, scalability, and cost optimization. Skilled in Version 1.2 troubleshooting Terraform configurations, identifying and resolving infrastructure-related issues, and optimizing Terraform code for improved performance and efficiency. Thanks & Regards James SmithTeam Lead smith@yochana.com Yochana Solutions INCWindsor, Ontario- Canada Farmington hills, MI-48335- USAUSA | CANADA I INDIADirect No: 949-201-1313 W: www.yochana.comPlease follow us on :USA | CANADA | INDIA Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program. Note: This is not an unsolicited mail. If you are not interested in receiving our e-mails then please reply with subject line Remove\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "304_Senior AWS API Developer || Atlanta GA": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nJob Description -Job Title: Senior AWS API DeveloperLocation: Atlanta, GA Contract: 12 Months Responsibilities:\u2022 Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.\u2022 Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.\u2022 Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.\u2022 Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.\u2022 Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.\u2022 Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.\u2022 Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.\u2022 Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.\u2022 Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.\u2022 Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.\u2022 Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.\u2022 Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "305_Devops Engineer Any Visa is fine": "Suresh,\nTech Rakers\nsureshk@techrakers.com\nReply to: sureshk@techrakers.com\nTitle: Devops EngineerLocation: Richmond, VADuration: 6+ MonthsRequired:Need a devops engineer with Good Communication Skills. Thanks and regards,Suresh Kumar,sureshk@techrakers.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "306_Azure cloud Architec": "Arshee syed,\nJKC\narshee@jkcitsol.com\nReply to: arshee@jkcitsol.com\nJob Title : Azure Cloud ArchitectLocation : RemoteThe Azure Cloud Architect taking care for develop solutions, implement projects, and set up monitoring systems for companies using Microsoft Azure. You will develop cloud-based computing service that focuses on crafting, testing, deploying, and managing application and service software. look for someone who is completely on Infra and have lot of terraform experience.Need Strong Architect for customer blueprinting and implementation need.We are seeking a skilled and experienced Azure Cloud Architect to join our consulting team and work closely with our clients. As an Azure Cloud Architect, you will be responsible for creating blueprints, developing comprehensive plans, and implementing new applications in the Azure Cloud environment. The ideal candidate must have a strong proficiency with Terraform, extensive experience in cloud infrastructure, and a deep understanding of Azure cloud services. Experience in the insurance domain and DevSecOps practices is a plus.Key Responsibilities:Blueprint Creation:Design and develop architectural blueprints that align with client requirements and best practices.Ensure blueprints adhere to security, scalability, and performance standards.Conduct architectural reviews and provide recommendations for improvements.Planning and Strategy:Collaborate with stakeholders to gather and analyze requirements.Develop detailed plans and strategies for cloud deployment and migration.Define cloud architecture roadmaps and ensure alignment with business objectives.Implementation of New Applications:Lead the deployment and implementation of new applications in the Azure Cloud environment.Utilize Terraform for infrastructure as code (IaC) to automate and streamline deployments.Ensure seamless integration of applications with existing systems and services.Cloud Infrastructure and Services:Manage and optimize cloud infrastructure, including networking, Active Directory, and security.Ensure robust and secure cloud operations and data center management.Implement and maintain platform engineering solutions to support development and operations.Observability and Monitoring:Implement observability solutions to monitor and manage cloud infrastructure and applications.Ensure effective monitoring, logging, and alerting mechanisms are in place.Troubleshoot and resolve performance issues and incidents promptly.Technical Leadership:Provide technical guidance and mentorship to development and operations teams.Stay updated with the latest Azure cloud technologies and industry trends.Conduct training sessions and workshops for team members and clients.DevSecOps Practices:Integrate security practices within the DevOps pipeline.Ensure continuous security monitoring and compliance throughout the software development lifecycle.Automate security controls and configurations using DevSecOps tools and methodologies.Must-Have Qualifications:Proficiency in Terraform:Extensive experience with Terraform for infrastructure as code.Ability to write, test, and maintain Terraform scripts for Azure deployments.Azure Cloud Expertise:Deep understanding of Azure cloud services, including computing, storage, networking, and databases.Experience with Azure DevOps, Azure Resource Manager (ARM) templates, and other Azure management tools.Cloud Infrastructure and Operations:Proven experience in managing cloud infrastructure, data centers, and cloud services.Expertise in networking, Active Directory, and security within cloud environments.Architectural Skills:Proven experience in designing and implementing complex cloud architectures.Strong problem-solving skills and the ability to troubleshoot complex issues.Communication and Collaboration:Excellent communication skills, both written and verbal.Ability to work collaboratively with cross-functional teams and clients.Strong project management skills and attention to detail.Preferred Qualifications:Relevant certifications such as Microsoft Certified: Azure Solutions Architect Expert or Terraform Associate Certification.Experience in the insurance domain.Experience with other cloud platforms (e.g., Google Cloud).Knowledge of containerization technologies (e.g., Docker, Kubernetes).Experience with IAC and automation and configuration management tools (e.g., Terraform, ARM, Biceps, Ansible,)Familiarity with DevSecOps practices and toolsSyed ArsheePH : +1 646-980-4762Email : arshee@jkcitsol.comLinkedin : https://www.linkedin.com/in/syed-a-8903312aa/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "307_AWS Data Application Infrastructure Lead - Urgent Role": "Shivam Pachauri,\nTek inspirations LLC\nshivam.pachauri@tekinspirations.com\nReply to: shivam.pachauri@tekinspirations.com\nJob Description -Job Title: AWS Data Application Infrastructure Lead Location: Atlanta , GA Contract : 6 Months Must be 14+ Years of Experience\u2026Relocation is OpenDESCRIPTIONManage AWS Infrastructure for provisioningManage and Streamline Deployments of Pipelines to ProductionManage FinOps for Productionalized applicationsPOC and recommendations for product usage and processesMust Have Experience in Big 4 Consulting CompaniesStrong Communication Skills and Client Relationship managementThanks & Regards,Shivam PachauriTechnical Recruiter TEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035E: shivam.pachauri@tekinspirations.com Linkedin: linkedin.com/in/shivam-pachauri-97b754296\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "308_Hiring Now: AWS Cloud Engineer : MA (Hybrid)": "Sandhyarani,\nVyze inc\nsandhyarani@vyzeinc.com\nReply to: sandhyarani@vyzeinc.com\nJob Description -AWS Cloud EngineerMust be from trusted 3rd partywe only get 2 subs total so this will close quickly Location: 200 Arlington St. Chelsea MA (Must be within 2 hour if driving distance, No Generated bills)Duration: 12 MonthEnd Client: Executive Office of for Admin and FinanceVisa: anyInterview: 2 roundsPlease send submits over with LinkedIn, contact details, proof of I.D. & address. Please ensure their education details on are their resume Job descriptionMUST HAVEAWS CERTIFICATIONProven experience in migrating on-premises databases to AWS RDS.Strong knowledge of AWS RDS, including setup, configuration, and maintenance.Hands-on experience with Oracle, SQL Server, and PostgreSQL databasesAWS service Bachelor's degree in Computer Science, Information Technology, or a related field.Proven experience in migrating on-premises databases to AWS RDS.Strong knowledge of AWS RDS, including setup, configuration, and maintenance.Hands-on experience with Oracle, SQL Server, and PostgreSQL databases.Experience with database migration tools and techniques (e.g., AWS Database Migration Service, Schema Conversion Tool).Familiarity with AWS services and infrastructure, including IAM, EC2, S3, VPC, and CloudWatch.Excellent problem-solving skills and attention to detail.Strong communication and interpersonal skills, with the ability to work effectively with cross-functional teams.Certification in AWS (e.g., AWS Certified Database \u00e2\u20ac\u201c Specialty, AWS Certified Solutions Architect) is a plus. Preferred Skills:Experience with database performance tuning and optimization.Knowledge of database security best practices.Understanding of data compliance and governance requirements.Ability to script and automate tasks using languages such as Python, Bash, or PowerShell.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "309_Opening for Cloud Infrastructure Engineer - Devops Infrastructure Engineer": "Asha Kannan,\nTeamware Solutions\nasha.k@twsol.com\nReply to: asha.k@twsol.com\nHi Everyone,Hope you are doing well. Please find the JD below, Role: Cloud Infrastructure Engineer/Devops Infrastructure Engineerlocation: Jersey City, NJ \u2013 OnsiteExperience: 9 YearsVisa: H1BMust Have: Blockchain Description:Blockchain Infra Ops Engineer At least 5 years of prior Sys Admin, DevOps, or otherwise relevant industry experience Intermediate knowledge in developing scalable & secure cloud networking architecture Intermediate knowledge of AWS or comparable cloud providers Capable of constructing and orchestrating Docker-based infrastructure Very comfortable with Linux systems Willingness to learn and master new toolingStrong opinions about how to build a sensible and safe cloud environmentExperience with backing up, provisioning & disaster recovery production databasesExperience with Kubernetes and working with micro-service architectureKnowledge in GitCompetencies:Digital : DevOps, Blockchain Thanks & Regards,ASHAasha.k@twsol.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "310_Urgent Hire :: Contract :: AWS Solution Architect :: Minneapolis Downtown, MN": "Amit Kushwaha,\nKKSA\nkushwaha@kksoftwareassociates.com\nReply to: kushwaha@kksoftwareassociates.com\nRequirement ID: 9589124Role Description: AWS Solution ArchitectLocation :: Minneapolis Downtown, MNContract Competencies: Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years): 4-6YEssential Skills: AWS Solution ArchitectDesirable Skills: AWS Solution Architect Best Regards,Amit Kushwaha KK Associates LLC. 8751 Collin McKinney Pkwy, # 1302, McKinney, TX 75070555 Metro Place North, Suite # 100, Dublin, OH 43017Direct: +1 (469) 343-1783 | Fax: (614) 413-3428Email: kushwaha@kksoftwareassociates.comWeb: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "311_Mobility AWS AppSync - TypeScript Developer Lead -ATLANTA, GA (onsite)": "Barla Santosh,\nGac Sol\nsbarla@gacsol.com\nReply to: sbarla@gacsol.com\nTitle: Mobility AWS AppSync/TypeScript Developer LeadLocation: ATLANTA, GA (onsite) Roles and Responsibilities:Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions.Requirements:Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.Experience coding in Swift is a plus.Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.Familiarity with Charles Proxy or similar tools for network analysis and debugging.Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.Experience with CI/CD pipelines and DevOps practices is a plus.Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.Strong documentation skills to maintain clear and concise technical documentation.??TypeScript, AWS AppSync, AWS Lambda, Node.JS. Thanks,Barla SantoshTechnical RecruiterE: Sbarla@gacsol.com www.gacsol.com\u2018Experts in Digitalization and Engineering - Enterprise 4.0\u2019\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "312_AWS Solution Architect": "Sushil. N,\nkksoftwareassociates\nsushilsingh.n@kksoftwareassociates.com\nReply to: sushilsingh.n@kksoftwareassociates.com\nRole name:EngineerRole Description:AWS Solution ArchitectCompetencies:Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years):4-6Essential Skills:AWS Solution ArchitectDesirable Skills:AWS Solution ArchitectCountry:United StatesBranch | City | Location:MinneapolisKeywords:AWS Solution ArchitectThanks and Regards, Sushil. N |IT Recruiter| KK Associates LLC.Contact: +1 614-756-0911 555 Metro Place North, Suite 100 Dublin, OH 43017 | 8751 Collin-McKinney Pkwy, #1302, McKinney, TX 75070Email Id: sushilsingh.n@kksoftwareassociates.comLinked In: https://www.linkedin.com/in/ningthoujam-sushil-721430204/URL: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "313_Oracle IBM Cloud Engineer - JERSEY CITY NJ - 0nsite": "Barla Santosh,\nGac Sol\nsbarla@gacsol.com\nReply to: sbarla@gacsol.com\nTitle: Oracle IBM Cloud EngineerLocation: JERSEY CITY NJJOB DESCRIPTION: As an Oracle OCI/OIC and IBM Cloud Engineer, you will be responsible for designing, implementing, and maintaining cloud infrastructure and services on Oracle Cloud Infrastructure (OCI), Oracle Integration Cloud (OIC), and IBM Cloud. You will work closely with our development, operations, and security teams to ensure that our cloud environment is secure, scalable, and efficient.Responsibilities include but are not limited to:Design and implement Oracle OCI/OIC and IBM Cloud solutions to meet business requirements.Configure and deploy cloud resources such as virtual machines, databases, storage, and networking.Monitor and optimize cloud infrastructure and services for performance and cost.Troubleshoot and resolve issues related to cloud infrastructure and services.Collaborate with development teams to ensure that applications are cloud ready.Implement and maintain cloud security best practices.Ensure that cloud infrastructure and deployments adhere to best practices for security, compliance, and scalability.Monitor and analyze cloud infrastructure performance metrics to identify areas for improvement and optimization.Provide technical guidance and support to other team members. Work Experience and Qualifications:Bachelor\u2019s degree in computer science, Engineering, or related field.3+ years of experience with Oracle OCI/OIC and IBM Cloud.Strong understanding of cloud computing principles and technologies.Experience with designing and implementing cloud solutions.Knowledge of Oracle OCI/OIC and IBM Cloud services and products.Experience with cloud automation and scripting (e.g., Terraform, Ansible, Python).Excellent troubleshooting and problem-solving skills.Strong communication and collaboration skills.Preferred Qualifications:Oracle OCI/OIC and IBM Cloud certifications.Experience with other cloud platforms (e.g., AWS, Azure, Google Cloud).Experience with DevOps practices and tools.Thanks,Barla SantoshTechnical RecruiterE: Sbarla@gacsol.com www.gacsol.com\u2018Experts in Digitalization and Engineering - Enterprise 4.0\u2019\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "314_AWS Data engineer || Minneapolis MN ||| only local": "Akash,\nsmart it frame\nakash@smartitframe.com\nReply to: akash@smartitframe.com\nHi,Greetings from Smart IT Frame, Hope you are doing well!!!Smart IT Frame specializes in enabling you with your most critical line of resources. Whether it\u2019s for permanent staffing, contract staffing, contract-to-hire or executive search, we understand the importance of delivering the most suitable talent; on time and within budget. With our Core focus in emerging technologies, we have provided global technology workforce solutions in North America, Canada & India. We take pride in delivering specialized talent, superior performance, and seamless execution to meet the challenging business needs of customers worldwide. Role: AWS Data engineerLocation: MNExp : 10 YearsMandatory Skills: Expert in AWS Services like lambda, S3, ECR, Sagemaker, AWS Glue/EMR, AWS Redshift.Good in SQL (SQL Server , Snowflake , Postgres)Good knowledge in pythonGood knowledge in Docker/Podman, KubernetesExperience with unix/linux, Including basic commands and shell Scripting. ---Warm Regards,AkashSmart IT Frame LLCakash@smartitframe.comwww.smartitframe.comhttps://www.linkedin.com/in/akash-s-332905212/-----WBENC------\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "315_Need:Cloud Engineer-Gov Cloud-Seatle, Washington - Onsite": "Supriya,\nYochana\nsupriya@yochana.com\nReply to: supriya@yochana.com\nHello,Hope you are doing great!!!This is Sai Supriya from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your time.Title : Cloud Engineer-Gov CloudLocation : Seatle, Washington - OnsiteLong term ContractKey ResponsibilitiesOversee the network onboarding process for new users and systems into Cloud environmentProvision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.Conduct regular network assessments and audits to ensure compliance with internal and external requirements.Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.Provide technical support and troubleshooting for Cloud network-related issuesStay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms QualificationsExperience with network onboarding and provisioning in cloud environments.Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.Familiarity with security regulations, standards, and compliance requirements for cloud network environments.Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.Strong communication and documentation skills for collaborating with cross-functional teams.Experience with automation tools (e.g., Python, Terraform) for network configuration and managementRelevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred. Thanks & Regards, Sai Supriya- Resource Specialist Yochana IT Solutions Inc.Email: Supriya@yochana.com |Phone: 248-537-0766 (Direct) |Address : 23000 Commerce Drive, Farmington Hills, MI 48335 |www.yochana.com/ Please follow us on: USA | CANADA | INDIA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "316_DevOps Engineer Cupertino, California hybrid Local Only H1B Transfer or US citizen only": "Muskan,\nInfoTech Spectrum\nmuskan.sharma@infotechspectrum.com\nReply to: muskan.sharma@infotechspectrum.com\nRole Description:Job Title: DevOps Engineer (Ony US CITIZEN or H1b Transfers) 14 + years experienceJob Location: Cupertino, California Hybrid \u2013 3 days onsite (Local Only)Job Duration: 6+ MonthsOny US CITIZEN or H1b TransfersExperience: 14+ years Job Description:\u00b7 Looking for a Devops Engineer with 14+ years of experience who are local to the job location or willing to relocate to the job location.\u00b7 The DevOps position would be primarily centered on migrating applications and databases from an internal data center to GCP. \u00b7 Senior level experience with web application development would be very beneficial. Thank YouBest RegardsMuskan SharmaInfoTech Spectrum Inc2060, Walsh Ave, #120, Santa Clara, CA 95050Email : muskan.sharma@infotechspectrum.com Web: www.infotechspectrum.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "317_Urgent Require Azure Developer || Onsite || Minneapolis Downtown, MN.": "SANJANA,\nIBRIDGETECHSOFT\nsanjana.t@ibridgetechsoft.com\nReply to: sanjana.t@ibridgetechsoft.com\nAzure DeveloperOnsite ( Minneapolis Downtown, MN)Contract Job Description Role Description: Help in AZURE configuration, document standards, security patching, testing and implementationCompetencies: Digital : Microsoft Azure, Digital : Amazon Web Service(AWS) Cloud ComputingExperience (Years): 6-8Essential Skills: Experience in the below - AZURE Cloud, IAAS, PAAS, VM Migration, traffic manager, azure cloud, sql azure, networking, data lake, power shellStrong CommunicationDesirable Skills: Experience in life and annuities industry Thanks and Regards,SanjanaTechnical Recruiter \ud83d\udcde: +1 904-587-1235 \ud83d\udce7: Sanjana.t@ibridgetechsoft.com465 Grant Logan DR, St Johns, FL\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "318_Onsite Role: Urgent need Looking for Sr DevOps Engineer in Linthicum, MD": "Sridhar,\nStellarIT Solutions\nsridhar@stellarit.com\nReply to: sridhar@stellarit.com\nOur Fortune 500 client is looking for a for Sr DevOps Engineer on their project based out Linthicum, MDJob Title: Sr DevOps EngineerLocation: Linthicum, MD-OnsiteDuration: Long term**************************************Need Local candidates ****************************Interview Mode : Face to Face & Day-1 OnsiteResponsibilities:Design, deploy, and manage AWS infrastructure using best practices for scalability, performance, and security.Collaborate with development teams to understand application requirements and provide guidance on optimizing cloud architecture.Implement automation solutions for infrastructure provisioning, configuration management, and deployment using tools such as AWS CloudFormation, Terraform, and Ansible.Configure and maintain monitoring, logging, and tracing solutions using OTEL (Open Telemetry) to ensure comprehensive observability of applications and infrastructure.Continuously optimize AWS resources to improve cost efficiency and performance.Implement and maintain CI/CD pipelines to automate software delivery and deployment processes.Perform regular system and security audits, and implement remediation measures as necessary.Troubleshoot and resolve issues related to AWS infrastructure and application deployments.Stay up-to-date with industry trends and best practices in AWS cloud services and DevOps methodologies.Requirements:Bachelor\u2019s degree in computer science, Engineering, or related field.Proven experience as a DevOps Engineer or similar role, with a focus on AWS cloud infrastructure.Strong understanding of AWS services such as EC2, S3, RDS, Lambda, IAM, and VPC.Hands-on experience with infrastructure as code (IaC) tools like AWS CloudFormation, Terraform, or Ansible.Proficiency in scripting languages such as Python, Shell, or Bash.Experience with containerization technologies like Docker and container orchestration platforms such as Kubernetes.Familiarity with OTEL (OpenTelemetry) and its components for monitoring, logging, and distributed tracing.Knowledge of CI/CD pipelines and related tools such as Jenkins, GitLab CI, or CircleCI.Solid understanding of networking concepts, security principles, and best practices in cloud environmentsExcellent problem-solving and communication skills, with the ability to work effectively in a collaborative team environment. Preferred Qualifications:AWS certification (e.g., AWS Certified DevOps Engineer, AWS Certified Solutions Architect).Experience with serverless computing and AWS Lambda.Familiarity with other cloud platforms such as Azure or Google Cloud Platform (GCP).Knowledge of microservices architecture and related technologies like service mesh (e.g., Istio).Experience with log management and analysis tools such as ELK stack (Elasticsearch, Logstash, Kibana) or Splunk.Please send your updated word format resume along with your best contact details to sridhar@stellarit.com Stellar IT Solutions is a Global IT Solution provider headquartered in Rockville, MD with operations in the US and India. Stellar IT Solutions has over 15 years of IT and consulting experience to give cost effective solutions to many Fortune 500 companies.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "319_Need Cloud Security Architect -- Reston, VA": "sharon,\nNorthpole\nsharon@npoleinc.com\nReply to: sharon@npoleinc.com\nHi, Role \u2013 Cloud Security ArchitectLocation \u2013 Reston, VAType \u2013 Hybrid (2 days from Office within a week) ContractJob Description \u2013The Cloud Security Architect (CSA) will leverage broad technical knowledge of cloud security best practices of key public cloud offerings of providers such AWS, Azure, and GCP to establish secure design patterns, to architect integrations among cloud and/or on-premises infrastructures. This individual must be able to assist in ensuring the security and compliance of the cloud environment based on enterprise cloud security policies, standards, and procedures. The CSA will ensure that solutions operating on the cloud comply with enterprise security requirements in both off-premises and hybrid environment models. The position will work with Enterprise Architects and Application Dev teams to come up with Security Architecture for applications and enterprise tech capabilities migrating to Cloud.Must-Haves: Required qualifications to be successful in this role: 7-10 years of total IT experience with the following must haves: \u2022 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect \u2022 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u2022 Excellent written and communication skills to report, document and communicate security architecture \u2022 Excellent coordination skills and must be detail oriented Nice-to-Haves: \u2022 Cloud agnostic security architecture experience a plus \u2022 1-2 years of experience in working with NIST assessments of business applications \u2022 Container Security experience to protect container workloads during build and run-time \u2022 API Security architecture experience with industry standard API Gateways \u2022 Security engineering/administration background leveraging SIEM, Network firewalls, host-based security, and security configuration \u2022 One or more industry standard security certification such as CISSP, CCSP or relevant GIAC certifications \u2022 One or more Cloud Service Providers Security Specialty Certifications such as AWS Security Specialty or Azure AZ-500 Certification \u2022 The group of skills related to Security including designing and evaluating security systems, identifying security threats, securing computers, assessing vulnerability, etc. \u2022 The group of skills related to Relationship Management including managing and engaging stakeholders, customers, and vendors, building relationship networks, contracting, etc. \u2022 Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand \u2022 The group of skills related to Risk Assessment and Management including evaluating and designing controls, conducting impact assessments, identifying control gaps, remediating risk, etc. \u2022 Experience identifying and determining levels of risk to an organization's networks and systems using cybersecurity techniques \u2022 Working with people with different functional expertise respectfully and cooperatively to work toward a common goal \u2022 Skilled in cloud technologies and cloud computing \u2022 The group of skills related to Influencing including negotiating, persuading others, facilitating meetings, and resolving conflict Key Areas of Responsibility: \u2022 Partner with Enterprise/Portfolio Architecture team and Business Units development squads to collaboratively develop security architectures/designs leveraging approved patterns that ensure applications migrating from on-premise to Cloud, achieving high standards of security practices and compliance. \u2022 Drive the development and adoption of cloud security standards, best practices, and technologies within Enterprise IT infrastructure \u2022 Liaise on security-related issues with internal business stakeholders, InfoSec, Enterprise Architecture, and application development squads \u2022 Work to develop, enhance and document security architecture, security policies, patterns, procedures, guidelines and standards required to design cloud-based solutions \u2022 Educate application, portfolio and solution architects on secure solution design and industry best security practices \u2022 Work on assessments of compliance and standards including and not limited to NIST, FedRAMP, FIPS, etc. \u2022 Support threat modeling and update application security architecture as needed. \u2022 Support application development squads with Security implementations and issues Regards,SharonSenior US IT Recruiter (O): 512.999.7446Sharon@npoleinc.com | www.npoleinc.com 3600 Brushy Creek Rd, #12, Cedar Park, TX,78613A Certified Minority-Owned Business(MBE)/Women-Owned Business (WBE) Enterprise\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "320_ETL Test Lead Azure at Remote": "Dinesh Kumar V,\nVbeyond Corporation\ndineshv@vbeyond.com\nReply to: dineshv@vbeyond.com\nRole : ETL Test Lead with Azure(ADF, Databrick, Azure Devops)Mode : ContractLocation : RemoteAs an ETL Test Lead you will be a part of an Agile team to build healthcare applications and implement new features while adhering to the best coding development standards .Responsibilities:Experience of ETL testing, specifically utilizing ADF ,Databricks and Azure, to test data integration workflows ensuring seamless data flow across diverse systems. Knowledge on Azure based application Ability to review test scenarios and test cases Develop and execute comprehensive test plans, test cases, and test scripts for interoperability projects. Ability to test UI based application also added advantage.Collaborate with development, DevOps, and QA teams to ensure seamless integration and deployment processes.Conduct functional, regression, integration, and performance testing on various systems and applications. Identify, document, and track defects, and work with developers to ensure timely resolution.Ensure testing processes align with industry standards and best practices.Participate in Agile ceremonies, including sprint planning, daily stand-ups, and retrospectives.Utilize automated testing tools and frameworks to enhance test coverage and efficiency.Continuously improve test processes and methodologies to support DevOps practices.Stay updated on the latest industry trends and technologies to ensure the adoption of best practice Ensure data exchange processes comply with industry standards and regulations\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "321_AWS Architect   Onsite-San Antonio, TX": "Asim,\nIntellicept\naahamed@intellicept.com\nReply to: aahamed@intellicept.com\nJob Title: AWS Architect Position Type: CTHLocation: San Antonio, TX Mandatory SkillsetOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:\u00b7 Participate in producing conceptual, solution and component-level architectures and associated artifacts.\u00b7 Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.\u00b7 Develop common components for shared business capabilities with Standards and best practices. \u00b7 Develop the transition architecture using services offered by AWS and private cloud.\u00b7 Migrate legacy applications to AWS and private cloud. \u00b7 Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7 Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7 Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7 Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7 Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7 Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7 Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7 Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.\u00b7 Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.\u00b7 Ability to design patterns for moving legacy applications to AWS and private cloud.\u00b7 Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)\u00b7 Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)\u00b7 Experience with SAFe framework or other similar agile frameworks.\u00b7 Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.\u00b7 Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "322_Immediate Hiring for AWS Expert + Python (Architect) - Austin (Remote)": "kaviarasan,\nExaways corp\nkavi@exaways.com\nReply to: kavi@exaways.com\nHi, I am Kaviarasan from Exaways Corporation!Exaways Corporation. (https://www.exaways.com/), is an Equal Opportunity Employer, is an IT consulting firm headquartered in Fremont, CA servicing clients nationwide.We provide services in IT, Telecom, Finance & Accounting and Engineering categories.If you are interested, kindly send me your updated resume, contact details and a convenient time to talk to you Role: AWS Expert + Python (Architect)Location: Austin (Remote) Job description:Terraform Template creationTerraform module write upAWS Resources including Gateway, Lambda, Authentication patternHelm templatePython or Shell scripting, Bash scriptingPython developmentGithub, Github action includes Scans like Sonar, AppSecBranching strategyGitOpsPython API developmentExperience in DevOps Architecture design and strategy.how deploy same authentication in Lambda and EKSServer less authentication servicesThanks,KaviarasanExaways Corporationkavi@exaways.comLinkedIn :linkedin.com/in/kavi-arasan-707a69209\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "323_Onsite Contract Position for Azure Cloud FinOps Architect in Issaquah, WA": "Gaurav Sharma,\nUSG Inc.\ngaurav.ks@usgrpinc.com\nReply to: gaurav.ks@usgrpinc.com\nHi, Hope you are doing great.Please go through the job description given below and if you are interested do share an updated word copy of your resume and best time to reach you over the phone. Position: Azure Cloud FinOps ArchitectLocations: Issaquah, WADuration: Contract Job Description:Looking for a skilled Azure Cloud FinOps Architect to join our team. Managing and optimizing cloud financial operations, ensuring cost efficiency and transparency in Azure cloud expenditures. This role involves working closely with finance, IT, and business teams to align cloud spending with organizational goals and drive financial accountability. Key Responsibilities:Cloud Cost Management:- Implement tagging across the full estate.- Monitor and analyze Azure cloud spending, identifying trends, anomalies, and opportunities for cost savings.- Implement and maintain cost allocation and chargeback models to accurately distribute cloud costs across departments or projects.- Utilize Azure Cost Management + Billing tools to track and manage cloud expenses.- Implement proactive alerts, budgets, thresholds for subscriptions.Optimization and Efficiency:- Develop and enforce cost optimization strategies, such as rightsizing resources, leveraging reserved instances, and eliminating unused assets.- Recommend and implement best practices for cost-effective use of Azure services.- Create optimization candidates to lower Azure cloud costs.- Conduct regular reviews and audits of cloud resource utilization and spend.Financial Reporting and Analysis:- Generate and present detailed financial reports on cloud spending, budget forecasts, and cost optimization efforts.- Provide insights and recommendations to stakeholders based on financial data analysis.- Collaborate with finance teams to ensure accurate budgeting and forecasting of cloud expenses.Governance and Compliance:- Establish and maintain cloud cost governance frameworks and policies.- Ensure compliance with organizational and regulatory financial policies.- Monitor adherence to cost management guidelines and practices.Collaboration and Support:- Work closely with IT, DevOps, and business teams to align cloud usage with financial goals.- Provide training and guidance to team members on FinOps best practices and tools.- Participate in cross-functional meetings to discuss and address cloud financial management issues.Continuous Improvement:- Stay updated with the latest trends and best practices in cloud financial management (FinOps).- Propose and implement improvements to enhance the efficiency and effectiveness of cloud financial operations.- Evaluate and integrate new tools and technologies to improve cost management processes. Qualifications:- Experience: 14+ Years of IT experience- 3+ years of experience in cloud financial management, preferably with a focus on Azure. Thanks with regards,Gaurav Sharma | Sr. Technical RecruiterEmail ID: gaurav.ks@usgrpinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "324_AWS Cloud Expert - Remote - Need US citizen Candidates": "Sofia,\niAgami LLC\nsofia@iagami.com\nReply to: sofia@iagami.com\nHi,Please review the positions below and let me know if any consultants are available on your bench.Please share your consultant resume to sofia@iagami.comAWS Cloud Expert - RemoteWe need candidates with SSBI clearance or US citizen . \u00b7Manage the EUC Identity Services components supporting VMC on GovCloud services, including:\u00b7Deploy new software deliveries to AWS environments using CI/CD infrastructure\u00b7Update service images and platform components versions to maintain software currency\u00b7Maintain secure configuration baseline in accordance with FedRAMP controls\u00b7Monitor and maintain system availability\u00b7Troubleshoot issues related to interaction between EUC Identity Services and VMC on GovCloud services\u00b7Maintain Jenkins CI/CD infrastructure for deploying and maintaining the EUC Identity Services\u00b7Participate in change advisory board processes for applying production changes\u00b7Implement changes required to meet new/modified FedRAMP controls\u00b7Participate in on-call rotations to support the serviceSkills preferred:\u00b7AWS\u00b7Cloudformation\u00b7ElasticBeanstalk\u00b7EC2\u00b7Application Load Balancer\u00b7Security Groups\u00b7VPC\u00b7CloudWatch\u00b7ElastiCache for Redis\u00b7Aurora MySQL\u00b7OpenSearch\u00b7Amazon Linux 2\u00b7Kinesis\u00b7Lambda\u00b7MSK\u00b7Docker\u00b7Jenkins\u00b7Nginx\u00b7FluentbitBest Regards,Sofia Sofia@iagami.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "325_DevOps Engineer - Alpharetta,GA-Berkeley Heights,NJ-Omaha,Nebraska - Onsite": "Abhilash s h,\nBlue ocean ventures\nabhilash.sh@blue-oceanventures.com\nReply to: abhilash.sh@blue-oceanventures.com\nRole: DevOps EngineerLocation:Alpharetta,GA/Berkeley Heights,NJ/Omaha,NebraskaJOB DESCRIPTION:Linux side :\u00b7 Networking, subnets, DNS, DHCP, route , gateway, load balancer\u00b7 File system \u2013 local, NFS etc\u00b7 Scripting \u2013 shell, python etc\u00b7 Basic OS commands \u00b7 Experience with VMware vSphere and VMs OpenShift Container Platform (OCP) \u00b7 Building OCP\u00b7 Ansible automation\u00b7 Strong Kubernetes skills\u00b7 CKA is a plus. Regards, Abhilash SHhttps://www.linkedin.com/in/abhilash-s-h-ab07a5231/Blue Ocean Ventures5555 Glenridge Connector Suite 200 Atlanta, GA 30342Will To Serve Will To Win Will To Lead\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "326_Sr Cloud Security Architect-Bellevue WA,Frisco, TX, Kansas city, KS and Alpharetta, GA(Only Locals)": "Manjusha,\nSparinfosys\nmanjusha.murugummala@sparinfosys.com\nReply to: manjusha.murugummala@sparinfosys.com\nHi,Hope you\u2019re doing well,Please find the below JD and let me know your interest. Job Role: Sr Cloud Security ArchitectJob Location: Bellevue WA (Onsite) and Frisco, TX, Kansas city, KS and Alpharetta, GAJob duration: Long term contract Must Have:Experience with onboarding and provisioning in cloud environmentsProficiency in scripting and automation tools (e.g., Python, Terraform)Knowledge of cloud security services, tools, and best practicesUnderstanding of security regulations, standards (e.g., FedRAMP, NIST), compliance requirements for cloud environmentsHands-on experience with cloud security services (IAM, CloudTrail, Config, GuardDuty, Security Hub)Cloud security certifications (e.g., AWS Certified Security - Specialty, Azure Security Engineer Associate)Problem-solving and analytical skills for identifying and mitigating security risks QualificationsExperience with onboarding and provisioning in cloud environments.Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.Proficiency in scripting and automation tools (e.g., Python, Terraform).Strong communication and documentation skills for collaborating with cross-functional teams. Thanks & Have a Great Day!Manjusha\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "327_AWS Architect (Enterprise Architect) -- San Antonio, TX -- Onsite": "Pinki Kumari,\nIntellicept Corporation\npinki@intellicept.com\nReply to: pinki@intellicept.com\nJob Title: AWS Architect (Enterprise Architect)Position Type: CTHLocation: San Antonio, TX Mandatory SkillsetOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.Thanks & RegardsPinki KumariTalent AdvisorEmail: pinki@intellicept.comCell:- (848) 999-4581Intellicept Corporation\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "328_Azure DATA ENGINEER": "saisekhar,\nmavinsys\nsaisekhar.n@mavinsys.com\nReply to: saisekhar.n@mavinsys.com\nJOB Title: Azure DATA ENGINEERLocation: RemoteClient- HumanaDuration: Long TermInterviews : 2 roundsClient Interview - NoStart Date : August 2024Responsibilities:Technical Leadership: Lead a team of data engineers in designing, developing, and maintaining scalable data solutions on Azure cloud platform.Architecture Design: Design robust data architectures and solutions utilizing Azure services such as Azure Data Factory, Azure Databricks, Azure SQL Database, and Azure Blob Storage.Data Pipelines: Oversee the development and implementation of ETL processes and data pipelines to ensure efficient data extraction, transformation, and loading from various sources.Data Modelling: Guide the team in designing and optimizing data models for performance and scalability within data warehousing solutions (e.g., Azure SQL Data Warehouse, Snowflake).Quality Assurance: Ensure data quality and integrity throughout all stages of data pipelines and workflows.Technical Guidance: Provide technical mentorship, guidance, and support to team members, fostering a collaborative and innovative team environment.Stakeholder Management: Collaborate closely with stakeholders, including data scientists, analysts, and business leaders, to understand requirements and deliver solutions that meet business needs.Documentation and Best Practices: Establish and enforce best practices for development, documentation, and maintenance of data solutions and processes.Qualifications:Technical Skills: Expertise in Azure cloud services (Azure Data Factory, Azure Databricks, Azure SQL Database, Azure Blob Storage, etc.) and Python programming for data manipulation and ETL processes.Leadership Experience: Proven experience leading technical teams in designing and implementing complex data solutions.Data Warehousing: Deep understanding of data warehousing principles and experience with data modelling, schema design, and optimization.Problem-Solving Abilities: Strong analytical and problem-solving skills to troubleshoot complex data issues and optimize performance.Communication Skills: Excellent verbal and written communication skills to effectively communicate technical concepts to both technical and non-technical stakeholders.Education and Experience: Bachelor's degree in Computer Science, Engineering, or a related field. Previous experience as a technical lead or senior data engineer in a cloud-based environment is required.Certifications: Certifications in Azure (e.g., Microsoft Certified: Azure Data Engineer Associate) or related technologies are desirable.Must Haves: Azure Data Factory (ADF), Azure Databricks, Azure SQL Database, Azure Blob Storage, Python, PySpark, Kafka.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "329_Cloud developer || Dallas- Onsite || Contract": "Nishant Kumar,\nVeridian Tech Solution\nnishant.k@veridiants.com\nReply to: nishant.k@veridiants.com\nJob Title: Cloud developerLocation: Dallas, TX OnsiteHire Type : ContractJD-.NET 3.0,CI CD, Micro Services, Azure, Azure Cloud, Services, Microservices, Azure Functions, Security, Performance \"Cloud developer will be responsible for development of Application and service, reusable patterns and code base that enable team to deliver sustainable, maintainable application and features. Primary Responsibilities: \u2022 Develop highly-scalable applications for the Azure cloud by building App Services, Microservices, Azure functions, and patterns for public cloud that enable security and privacy at scale \u2022 Define and implement technical solutions to meet business needs through Agile process \u2022 Foster high-performance, collaborative technical work resulting in high-quality output \u2022 Gather and analyze data to aide in informed decision-making while providing detailed, realistic estimates \u2022 Interact skillfully with business stakeholders and third-party technical organizations \u2022 Proactively automate infrastructure, application, and services to enable an automated delivery through the CICD pipelines to the cloud\"\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "330_Please share local candidates-AWS Technical Lead  San Antonio TX(onsite)-Job Description": "Pallavi,\nYochana\npallavi@yochana.com\nReply to: pallavi@yochana.com\nHello,I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider with 14+ years of experience delivering value to leading companies across the U.S.A. and Canada,I'm currently staffing for AWS Technical Lead \u2013 San Antonio, TX(onsite), Below you will find the job description, if you are qualified and interested, please send me your Updated Word Document Resume.I'm very sorry if this position is not an ideal fit, We'll keep you in mind for other suitable positions and referrals would be appreciated.Thank you so much for your attention and participation.Job Role: AWS Technical LeadJob Location: San Antonio, TX(onsite)Project Duration: 6-12 MonthsJob Description: Mandatory required skills : AWS Sagemaker, AWS Neptune, GCP Vertex AIThe DevOps Engineer's primary responsibility is to ensure the successful implementation and management of AI/ML services/components for USAA Enterprise. This includes AWS Sagemaker, AWS Neptune, GCP Vertex AI. High level job responsibilities:-Will work with the engineering team and the infrastructure team in order to achieve all the milestones needed for the scope of work.Work with implementation engineers to ensure platform components are working as intended for day to day use.Work with infrastructure engineers/team to ensure platform components are implemented in a manner befitting USAA's policy and guardrails.Monitor and improve AI/ML components in AWS/GCP to ensure the continued quality delivery of functions in the AI/ML space.Candidate needs to be able to independently work and be self-driven in to successfully enable and implement the AWS Sagemaker, Neptune and GCPDirectly working with a fast paced team and stakeholders ranging from engineers all the way to the EMG group.This candidate will need to assess the scope of work and be able to come up with action items/plans with minimal. Regards, Pallavi BoolaRESOURCE SPECIALIST Contact- 2482373189Yochana IT Solutions Inc. 23000 Commerce Dr, Farmington hills, MI-48335 pallavi@yochana.com || www.yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "331_Urgent Hiring || Cloud Architect|| Any Visa Except H1 || Madison, WI 53706": "Pankaj Tiwary,\nSibitalent Corp\nptiwary@sibitalent.com\nReply to: ptiwary@sibitalent.com\nHi,Hope you are doing well!Please find the below mentioned JD and send me relevant profile. Title \u2013 Cloud Architect Location \u2013 Hybrid in 816 State Street Madison, WI 53706Contract/Duration \u2013 10 Months with extensions possibleVisa \u2013 Any Visa Except H1Mode of Interview \u2013 Phone/SkypeJob description - Must be Wisconsin resident or willing to relocate at the candidate's expense prior to onboarding. Responsibilities: System Architecture and Integrations:Architect and oversee integrations between various systems (e.g. e-commerce, membership/CRM, email marketing, point of sales, DAMS, and other web platforms).Interface with users, project sponsors, and all other stakeholders in order to determine their evolving needsDevelop APIs, middleware, and data pipelines to enable seamless data flow.Ensure data consistency and integrity across integrated systems.Implement, support, and mature a Digital Asset Management System (DAMS) and related integrations to establish a single source of truth for digital assets and metadata.Creating architectural documentation ranging from a very high level conceptual design to a more detailed specification. Includes conceptual and logical data models, mappings, context diagrams, process architecture models and data flows.Maintain system architecture that integrates key systems including web CMS, DAM, and collections management systemsIn partnership with internal and external design and development teams, implement and maintain a state-of-the-art website, integrated with DAMS.Ensure relevant operational and collections-oriented data readily available to report on via TableauIdentify and activate a Digital Preservation solution for Collections data and state records.Implement a scalable storage architecture that can accommodate collections growth and preservation.Plan, design, and request server infrastructure that aligns with project requirements and future scalability.Partitioning large systems into successive layers of subsystems and components each of which can be assigned to a single team member accountable for maintaining and improving.Ensure system performance or reliability is not sacrificed by reductions in cost, shifting timelines, or otherwise Cloud Infrastructure:Design, implement, and optimize cloud-based solutions (e.g., AWS, Azure, Google Cloud) to meet business needs.Ensure high availability, security, and scalability of cloud services.Foster team culture that encourages utilization of automation and streamlined development practices like CI/CD, DevOps practices. Collaborate with developers and engineers to automate deployment and monitoring processes. Identity and Single Sign-On (SSO):Define and implement identity management and SSO strategies for internal (Entra ID) and external (Okta) users.Work closely with security teams to ensure secure authentication and authorization meeting enterprise standards as well as WHS business requirements.Integrate SSO solutions with existing applications. E-commerce and Point of Sale (POS):Collaborate with business stakeholders to understand e-commerce and brick and mortar retail requirements.Design and optimize commerce platforms, including payment gateways, analytics, financial reporting, and inventory management.Architect POS solutions for retail or online sales channels. Knowledge, Skills & AbilitiesProven experience as a Technical Architect or similar role.Strong knowledge of cloud platforms (AWS, Azure, GCP) and associated services.Familiarity with DAM systems, integration patterns, and SSO protocols.Excellent communication and collaboration skills.Experience as a Windows System Administrator in an enterprise environment.Strong knowledge of Windows Server operating systems and Active Directory.Experience with M365 products, including SharePoint, Teams, and Power Platform.Strong understanding of network and security principles, including firewall configurations and access controls.Excellent problem-solving and troubleshooting skills.Ability to work independently and lead a team, with outstanding communication and collaboration skills.Strong organizational and project management skills.Knowledge of Project management methodologies with effective results focus within an information systems environmentExpertise in C#, .NET Core, and the .NET ecosystem as a wholeKnowledge of object oriented design principles such as SOLID or GRASPFamiliarity with multiple types of software architecture (Clean architecture, DDD, TDD, Vertical slice architecture)Comfortable working in legacy codebasesComfortable with version control and automated testing and deployment frameworks (CI/CD)Knowledge of industry best practices and a commitment to implementing them.Knowledge of Cloud platforms such as AWS and Azure.Scripting and automation skills (e.g., PowerShell, Python).Familiarity with security principles, including encryption, authentication, and authorization.Ability to conduct research into systems issues and products as required.Ability to communicate ideas in both technical and user-friendly language.Ability to effectively prioritize and execute tasks in a high-pressure environment.Ability to quickly master new technologies as needed.Experience providing guidance and leadership to novice systems engineersExcellent written and oral communication skillsStrong analytical and creative problem-solving abilities Top skills and years of experience: 5+ years of experience in the following:- Experience with multiple Cloud infrastructure in AWS & Azure- Experience providing technical direction/leadership to a team of developers and engineers- Complex web, data, and middleware software architecture and development Nice to have skills:- Experience implementing multiple SSO technologies (Entra ID, Okta) with SaaS and on-prem systems - Experience with Digital Asset Management Systems- Strategic planning\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "332_Azure Devops - ONLY Local candidates required - Chicago ,IL": "AJAY,\nKK Software Associates\najay.e@kksoftwareassociates.com\nReply to: ajay.e@kksoftwareassociates.com\nRole name:DeveloperRole Description:#### Key Responsibilities: - Design and develop microservices using Spring Cloud. - Implement and maintain containerized applications using Docker and Kubernetes. - Deploy and manage applications on Microsoft Azure. - Collaborate with cross-functional teams to define, design, and ship new features. - Ensure the performance, quality, and responsiveness of applications. - Identify and correct bottlenecks and fix bugs. - Help maintain code quality, organization, and automatization. - #### Requirements: - Bachelor\u2019s degree in Computer Science, Engineering, or related field. - Proven experience as a Spring Cloud Developer. - Strong proficiency in Java and Spring Framework (Spring Boot, Spring Cloud). - Hands-on experience with containerization technologies like Docker and orchestration tools like Kubernetes. - Extensive experience with Microsoft Azure services (e.g., Azure Kubernetes Service, Azure DevOps, Azure Functions, etc.). - Familiarity with CI/CD pipelines and tools. - Knowledge of RESTful API design and implementation. - Understanding of distributed systems and microservices architecture. - Experience with configuration management tools such as Spring Cloud Config. - Excellent problem-solving skills and attention to detail. - Strong communication and collaboration skills. #### Preferred Qualifications: - Strong proficiency in Java 8 + and Spring Framework 3. x + (Spring Boot, Spring Cloud).- Experience with other cloud providers (AWS, Google Cloud). - Knowledge of monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack). - Familiarity with Agile/Scrum methodologies. - Certification in Microsoft Azure or other relevant technologies.Competencies:Digital : Microsoft Azure, Digital : Kubernetes Branch | City | Location:TCS - Chicago(Downtown), ILCHICAGOChicago, IL\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "333_AWS Data Engineer (12+ Years)--Local to CA": "Criz Salvin,\nNorthITe\ncriz@northite.com\nReply to: criz@northite.com\nPosition: AWS Data EngineerLocation: Los Angeles, CA (Hybrid)Duration: Long Term This role work: Hybrid. In office Tues/Wed/ThursResource has to be Local: Yes Job Description:\u201cMust-have\u201d skills1. AWS Data Integration2. Data Warehousing3. Data Modelling Years of experience required for each skillAt least 5 years Job Duties:\u2022 Define the direction and use of the AWS services such as AWS Glue, AWS Lambda, Amazon S3, Amazon Redshift\u2022 Provide expertise in AWS security best practices, including VPC, IAM, and data encryption.\u2022 Provide expertise on the approach to data modeling, including conceptual, logical, and physical data models within AWS\u2022 Develop data integration and transformation of data using tools such as AWS Glue, Python/Pyspark, SNP Glue\u2022 Optimization data integrations and debugging of issues in the data flow\u2022 Contribute and author deliverables which data integration strategy and the design, build and testing of data integrations \"Nice-to-have\u201d skills:1. Python/PySpark2. SAP ISU3. SQL\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "334_Azure Cloud Policy Architect _ Detroit, MI": "Jessica,\nSiriinfo\njessica@siriinfo.com\nReply to: jessica@siriinfo.com\nGreetings, We have the below requirement with our Client. Kindly go through the JD below and let me know your interest Role name: Product ArchitectDetroit, MIDuration : Contract Role Description: \u2022 Strong communication, interpersonal and presentation skills\u2022 Ability to mentor Engineering and Operations staffExcellent organizational and time management skillsCompetencies: Digital : Cloud Security ManagementExperience (Years): 6-8Essential Skills: \u2022 Strong knowledge of the Microsoft Cloud Platforms features, functionalities and best use cases\u2022 Experience in managing Azure Policies and initiatives in Azure\u2022 Thorough understanding to Azure Policy Remediation and Azure Role-Based-Access-Control (RBAC)\u2022 Thorough understanding of Defender for Cloud\u2022 Comprehensive understanding of networking, cloud computing, and enterprise infrastructure.\u2022 Design, implement, and maintain Azure DevOps pipelines, build processes, CI/CD processes with Custom policies.\u2022 Create and maintain Azure DevOps dashboards.\u2022 Design and implement Azure security policies.\u2022 Hands-on experience in Microsoft Azure Cloud Services including Defender for Cloud, Cloud Security Posture Management (CSPM), Azure security tools/platforms such as Azure Entra ID, Sentinel, Key Vault etc.\u2022 5+ years managing security policies and initiatives in Azure.\u2022 Working knowledge of Zero Trust, threat management, SOC monitoring (SIEM / SOAR), and Extended Detection & Response (XDR) + SIEM, Cloud Security, Identity, RBAC, and ServiceNow.\u2022 Working knowledge of current NIST 800-53 for Azure, PCI, GDPR and Azure CIS Benchmark compliance\u2022 Relevant certifications such as CCSP, CISSP, CISM or Azure security certifications are highly desirable. Desirable Skills: \u2022 Single point of contact for all Azure Cloud Policy issues. \u2022 Function as a trusted advisor (and facilitator) to customer and team\u2022 Develops standards, policies and procedures best practices documentation and architectural design\u2022 Translate security and technical requirements into business requirements, and communicate security risks to different audiences ranging from business leaders to engineers\u2022 Collaborates with stakeholders, recognizes challenges, and offers solutions\u2022 Collaborate and Guide team members to design and develop end-to-end processes aligning with the defined architecture\u2022 Implementation of necessary security controls aligned with M365 E5 and Microsoft Azure Security workloads while ensuring operational health\u2022 Conduct periodic audit/review of Azure Policies\u2022 Configuring alert rules for policy violation in Defender for Cloud, MS Sentinel and ServiceNow\u2022 Conduct architecture reviews and security impact assessments\u2022 Gathers customer insights (e.g., feedback around technical preferences, environments, business needs) and leverages these insights and existing plans to map solutions with customer business outcomes\u2022 Optimizing operational procedures for improved security response\u2022 Optimize Azure Cloud Security configurations\u2022 Drive Overall governance for misconfigurations remediation with cloud teams\u2022 Create remedial strategy for overall Cloud Security Posture Management\u2022 Interface with internal and external auditors for gathering cloud security logs/evidences\u2022 Identify risks and work with client and engineering team to build a risk mitigation plan\u2022 Empowering Development and Operations team to operate within a control framework Best Regards,Jessica|Sr Techical Recruiter| Email: jessica@siriinfo.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "335_Azure DevOps Developer or Engineer | 10+ yrs must": "Rajesh,\nAgile Enterprise Solutions\nrajesh_potlapelli@aesincus.com\nReply to: rajesh_potlapelli@aesincus.com\nAzure DevOps Developer/EngineerLocation: Glastonbury, CT or Charlotte (NC) Need EST OR Nearst state profiles$58-60/hr on C2C All inc Max ----Take this answers in email---------Do you have experience in azure devops for developing the applications not infrastructure?have u created YAML pipeline? (MUST)Do u have coding experience in powershell script?Do u have experience in MS build tool?Relevant certifications (e.g., DevOps, Agile, or Project Management) are advantageous. Min 6+ yrs of hands on experience in managing complex release cycles and coordinating cross-functional teams. Strong understanding of CI/CD principles, including hands-on experience with tools such as Azure DevOps, Jenkins, GitLab CI/CD, MSBUILD, Sonar or others. Hands-on expertise with version control systems (GIT) and branching strategies. Ability to work closely with development teams, understanding their requirements and aligning pipeline processes. Effective communication skills to bridge the gap between development, operations, and other stakeholders. Familiarity with infrastructure as code (IaC) tools like Terraform, Azure Bicep, and scripting using PowerShell. Solid grasp of Agile methodologies and their application in release management. Demonstrated ability to coordinate cross-functional work teams toward task completion. Proactive identification and resolution of pipeline issues. Handling deployment failures, rollbacks, and incident management. Monitoring pipeline performance and addressing bottlenecks. Effective leadership and analytical skills to drive successful outcomes. Advanced written and verbal communication skills are essential. Attention to detail, with a focus on maintaining high-quality software releases\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "336_Sr. AWS Data Integration Engineer Sr. AWS Data Integration Engineer Hybrid Local Only": "Muskan,\nInfoTech Spectrum\nmuskan.sharma@infotechspectrum.com\nReply to: muskan.sharma@infotechspectrum.com\nHi Job Title Sr. AWS Data Integration EngineerLocation 555 W 5th St #2700, Los Angeles, CALIFORNIA 90013 - 3 Days Onsite Tuesday / Wednesday and Thursday (Local Only with California local DL copy)Job Type Long Term Contract / 28 Months Contract Job Description / Skills Required Must have Required Skills: - AWS Data IntegrationData WarehousingData ModellingJob Role and Responsibilities: - Define the direction and use of the AWS services such as AWS Glue, AWS Lambda, Amazon S3, Amazon RedshiftProvide expertise in AWS security best practices, including VPC, IAM, and data encryption.Provide expertise on the approach to data modeling, including conceptual, logical, and physical data models within AWSDevelop data integration and transformation of data using tools such as AWS Glue, Python/Pyspark, SNP GlueOptimization data integrations and debugging of issues in the data flowContribute and author deliverables which data integration strategy and the design, build and testing of data integrationsNice-to-have\u201d skillsPython/PySparkSAP ISUSQLGas or Utility domain Thank YouBest RegardsMuskan SharmaInfoTech Spectrum Inc2060, Walsh Ave, #120, Santa Clara, CA 95050Email : muskan.sharma@infotechspectrum.com Web: www.infotechspectrum.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "337_10yrs AWS Eng with 8yrs Linux exp - Austin, TX (100% Remote) -Please share only Local profiles. Client will not consider non locals- LinkedIn is Must": "Shujath,\nPanzer\nshujath.ali@panzersolutions.com\nReply to: shujath.ali@panzersolutions.com\nDear Consultant,How are you doing well!My name is Shujath Ali and I am working as a Lead Talent Acquisition at Panzer Solutions LLC.We deal with US IT and non IT recruitment services. We have the below opening with our preferred client,let me know if you are available & interested in this position. If interested please share a copy of your resume and best time, number to reach.Job Title: AWS Eng with LinuxLocation: Austin, TX (100% Remote)Duration: 1+ Months with a possibility of extension.Please share only Local profiles. Client will not consider non localsMust have 10 years of AWS & 8 years of Linux ExperienceMust have active LinkedIn profile.Understands business objectives and problems, identifies alternative solutions, performs studies and cost/benefit analysis of alternatives. Analyzes user requirements, procedures, and problems to automate processing or to improve existing computer system: Confers with personnel of organizational units involved to analyze current operational procedures, identify problems, and learn specific input and output requirements, such as forms of data input, how data is to be; summarized, and formats for reports. Writes detailed description of user needs, program functions, and steps required to develop or modify computer program. Reviews computer system capabilities, specifications, and scheduling limitations to determine if requested program or program change is possible within existing system.We are looking for candidates who have hands on experience and display good knowledge of the subject matter. Interviews for selected candidates will be technical and detailed, drilling into specific details.Candidate will be responsible for the operations of SHARP secure cloud infrastructure, platforms, and software. Help develop patterns for automated provisioning, management, scalability and security. Ensure that all cloud solutions follow internally defined security and compliance standards and controls. Respond to and resolve application, and security cloud issues. Responsible for the overall vigor of the platform including production issues, troubleshooting, monitoring system capacity using various software, working with other technical teams and stakeholder.Job responsibilities include\u2022 Monitor and maintain multiple environments for increase observability\u2022 Develop tools to continuously improve maintainability, monitoring, security and scalability of cloud deployments.\u2022 Operations support and enhancement of the SHARP platform This position will be reprocured for fiscal year 2025 II. CANDIDATE SKILLS AND QUALIFICATIONS Minimum Requirements:Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.YearsRequired/PreferredExperience10RequiredExperience in establishing proactive controls to monitor the health of various AWS services to improve reliability of tools/applications.10RequiredExperience in supporting, maintaining, monitoring, and securing various AWS IaaS and PaaS services.8RequiredExperience in improving the security of various services based on zero trust model.8RequiredDemonstrated experience with Linux administration and troubleshooting network issues.1PreferredAWS SysOps Administrator Certification.1PreferredEffectively manage multiple responsibilities, prioritize conflicting assignments, and switch quickly between assignments, as required.1PreferredExcellent oral and written communication skills.=====================================================================================================Warm Regards,Shujath Ali | Lead - Talent AcquisitionPanzer Solutions LLC50 Washington Street, 9th Floor,SONO Corporate Center - Norwalk CT 06854Fax: 203-286-1457Email: shujath.ali@panzersolutions.comNotice of Confidentiality:The information contained herein is intended only for the confidential use of the recipient. If the reader of this message is neither the intended recipient, nor the person responsible for delivering it to the intended recipient, you are hereby notified that you have received this communication in error, and that any review, dissemination, distribution, or copying of this communication is strictly prohibited. If you receive this in error, please notify the sender immediately by telephone, and destroy this e-mail message OR reply with the subject \"REMOVE\" or \"Unsubscribe\" such that your email would be taken out of our distribution list. You can also forward your \"REMOVE\" or \"Unsubscribe\" emails to info@panzersolutions.com.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "338_DevOps Engineer": "Ashish sharma,\nAdventatech\nasharma@adventatech.com\nReply to: asharma@adventatech.com\nDevOps Engineer Austin TX, (Hybrid ) Must be localSC/ GC CPA will require the Worker to work on the following initiatives and will perform advanced tasks such as:CI/CD Modernization and automation of the entire pipeline from code check-in to deployment utilizing industry best practices such as Infrastructure as Code (IaC), Configuration as Code (GitOps), and Blue-Green and Canary Deployment Strategies.Implement \"Shift Left\" security approach by integrating security tools and automating security checks and compliance into the CI/CD pipeline.Monitoring and Observability to provide comprehensive monitoring, logging, and alerting for the CI/CD pipeline.Participating in all phases of SDLC.Performing extensive code reviews and analysis.Writing reports on code analysis to determine if industry standards and secure coding best practices are being followed; provide analysis to address found short comings.Providing guidance and knowledge sharing to existing development staff.Minimum Requirements:Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.YearsRequired/PreferredExperience7RequiredProfessional experience in DevOps engineering, Software Development, or related field6RequiredExperience with programming languages such as Java and .NET5RequiredExperience with scripting languages such as Bash, Python, and PowerShell to automate repetitive tasks such as monitoring, deployments, and configuration management5RequiredExperience setting up and managing Jenkins servers, creating and maintaining CI/CD pipelines, integrating with other tools (e.g., Git, Maven, SonarQube), writing Groovy scripts for pipeline automation, and monitoring and optimizing Jenkins performance.5RequiredExperience with Infrastructure as Code tools like Ansible, Terraform, or Chef5RequiredExperience with containerization and orchestration tools such as Docker and Kubernetes5RequiredExperience with automation of infrastructure provisioning and configuration management5RequiredExperience with Maven in building and managing Java projects, maintaining POM files, troubleshooting build issues, dependency management and versioning, and integrating with CI/CD pipelines5RequiredExperience with Artifactory set up, configuration, managing binary repositories, integrating with build tools (e.g., Maven and Jenkins), managing artifact lifecycle and versioning, and implementing security and access controls.5RequiredExperience with microservices architecture, design, development and containerization and orchestration5RequiredExperience with SQL and NoSQL databases5RequiredExperience designing, developing, testing, integrating, and implementing secure REST APIs5RequiredExperience with code reviews and in-depth code analysis5RequiredExperience with highly complex application security requirements5RequiredExperience with Git, Bitbucket, Subversion and version control systems4RequiredExperience with SonarQube set up, configuration, integrating with CI/CD pipelines, and analyzing code quality and security vulnerabilities4RequiredExperience with Jira and Confluence4RequiredExperience with Agile teams3RequiredExperience with coaching, training, mentoring and knowledge transfer4PreferredExperience in Cybersecurity and implementing and automating security best practices into CI/CD pipelines4PreferredExperience with security testing tools such as SAST, DAST, or IAST3PreferredExperience with cloud technologies and platforms such as AWS and Azure3PreferredExperience working with legacy applications/services3PreferredExperience in modern web technologies such as JavaScript, Node.js, React.js, Redux, HTML5, CSS33PreferredPublic sector experience (Federal, State or Local Government)2PreferredProficient with the Microsoft Office products, including Outlook, TEAMS, Microsoft Project, Word, Visio, Excel and PowerPoint\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "339_Sr Cloud Security Engineer :: Onsite - Dallas TX(Need DL from TX)": "NIGESH,\nERP MARK\nnigesh@erpmark.com\nReply to: nigesh@erpmark.com\nHi We have priority Job requirement from our Direct customer and here I am checking your interest/ Availability for the same. Please review. Role: Sr Cloud Security EngineerLocation: Onsite \u2013 Dallas TX Job Description:Oversee the network onboarding process for new users and systems into Cloud environment Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations. Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms. Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways. Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats. Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents. Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure. Conduct regular network assessments and audits to ensure compliance with internal and external requirements. Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards. Provide technical support and troubleshooting for Cloud network-related issues Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications Experience with network onboarding and provisioning in cloud environments. Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud. Familiarity with security regulations, standards, and compliance requirements for cloud network environments. Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers. Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud. Strong communication and documentation skills for collaborating with cross-functional teams. Experience with automation tools (e.g., Python, Terraform) for network configuration and management Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "340_10yrs AWS Eng with 8yrs Linux exp - Austin, TX (100% Remote) -Please share only Local profiles. Client will not consider non locals- LinkedIn is Must": "Shujath,\nPanzer\nshujath.ali@panzersolutions.com\nReply to: shujath.ali@panzersolutions.com\nDear Consultant,How are you doing well!My name is Shujath Ali and I am working as a Lead Talent Acquisition at Panzer Solutions LLC.We deal with US IT and non IT recruitment services. We have the below opening with our preferred client,let me know if you are available & interested in this position. If interested please share a copy of your resume and best time, number to reach.Job Title: AWS Eng with LinuxLocation: Austin, TX (100% Remote)Duration: 1+ Months with a possibility of extension.Please share only Local profiles. Client will not consider non localsMust have 10 years of AWS & 8 years of Linux ExperienceMust have active LinkedIn profile.Understands business objectives and problems, identifies alternative solutions, performs studies and cost/benefit analysis of alternatives. Analyzes user requirements, procedures, and problems to automate processing or to improve existing computer system: Confers with personnel of organizational units involved to analyze current operational procedures, identify problems, and learn specific input and output requirements, such as forms of data input, how data is to be; summarized, and formats for reports. Writes detailed description of user needs, program functions, and steps required to develop or modify computer program. Reviews computer system capabilities, specifications, and scheduling limitations to determine if requested program or program change is possible within existing system.We are looking for candidates who have hands on experience and display good knowledge of the subject matter. Interviews for selected candidates will be technical and detailed, drilling into specific details.Candidate will be responsible for the operations of SHARP secure cloud infrastructure, platforms, and software. Help develop patterns for automated provisioning, management, scalability and security. Ensure that all cloud solutions follow internally defined security and compliance standards and controls. Respond to and resolve application, and security cloud issues. Responsible for the overall vigor of the platform including production issues, troubleshooting, monitoring system capacity using various software, working with other technical teams and stakeholder.Job responsibilities include\u2022 Monitor and maintain multiple environments for increase observability\u2022 Develop tools to continuously improve maintainability, monitoring, security and scalability of cloud deployments.\u2022 Operations support and enhancement of the SHARP platform This position will be reprocured for fiscal year 2025 II. CANDIDATE SKILLS AND QUALIFICATIONS Minimum Requirements:Candidates that do not meet or exceed the minimum stated requirements (skills/experience) will be displayed to customers but may not be chosen for this opportunity.YearsRequired/PreferredExperience10RequiredExperience in establishing proactive controls to monitor the health of various AWS services to improve reliability of tools/applications.10RequiredExperience in supporting, maintaining, monitoring, and securing various AWS IaaS and PaaS services.8RequiredExperience in improving the security of various services based on zero trust model.8RequiredDemonstrated experience with Linux administration and troubleshooting network issues.1PreferredAWS SysOps Administrator Certification.1PreferredEffectively manage multiple responsibilities, prioritize conflicting assignments, and switch quickly between assignments, as required.1PreferredExcellent oral and written communication skills.=====================================================================================================Warm Regards,Shujath Ali | Lead - Talent AcquisitionPanzer Solutions LLC50 Washington Street, 9th Floor,SONO Corporate Center - Norwalk CT 06854Fax: 203-286-1457Email: shujath.ali@panzersolutions.comNotice of Confidentiality:The information contained herein is intended only for the confidential use of the recipient. If the reader of this message is neither the intended recipient, nor the person responsible for delivering it to the intended recipient, you are hereby notified that you have received this communication in error, and that any review, dissemination, distribution, or copying of this communication is strictly prohibited. If you receive this in error, please notify the sender immediately by telephone, and destroy this e-mail message OR reply with the subject \"REMOVE\" or \"Unsubscribe\" such that your email would be taken out of our distribution list. You can also forward your \"REMOVE\" or \"Unsubscribe\" emails to info@panzersolutions.com.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "341_Role : Active Directory (Azure Security Engineer)": "Bharath,\nSmartfolks\nbharath@smartfolksinc.com\nReply to: bharath@smartfolksinc.com\nRole : Active Directory (Azure Security Engineer) Location \u2013 Quincy, MA OR NJ (Day 1 onsite) Rate : $60/hr On C2C max . We need Only Onsite Note \u2013 This is L3 role. Pls submit senior candidates around 14 years of exp.Job Description \u2013 We are seeking a highly skilled Azure Security Engineer with expertise in security products, authentication, authorization, and identity and access management (IAM). As a key member of our security team, you will play a vital role in ensuring the secure and compliant implementation of Azure AD solutions, with a focus on IAM, MFA, and SSO.Responsibilities:1. Design, implement, and manage Azure Active Directory solutions to ensure secure and efficient authentication and authorization processes aligned with industry best practices2. Drive the onboarding of applications, app registration, enterprise application setup, and role-based access management (RBAC).3. Lead the implementation of Multi-Factor Authentication (MFA) and Single Sign-On (SSO) for enhanced security.4. Expertise in configuring and troubleshooting authentication protocols, including OAuth, OpenID Connect, and SAML for secure authentication and authorization5. Configure and manage conditional access policies to control access based on specific conditions, locations, and device compliance6. Collaborate with cross-functional teams to support and troubleshoot IAM-related issues, ensuring solutions are secure, compliant, and scalable.7. Understand and implement security best practices for Azure products, services, and solutions.8. Utilize Azure Sentinel for monitoring, creating alerts, and developing automation scripts for incident response.9. Provide production support, responding to and resolving security incidents in a timely manner.10. Establish and maintain identity governance frameworks, including privileged identity management (PIM) for elevated access11. Stay informed of Azure updates, security threats, and industry best practices to enhance our security posture.12. Collaborate with DevOps and development teams, demonstrating a basic understanding of tools and requirements.Qualifications:1. Proven experience in implementing security solutions on Azure, with a focus on IAM, MFA, and SSO.2. In-depth knowledge of Azure AD, Azure AD B2C, related authentication/authorization components and security protocols which including SAML, OAuth, and OpenID3. Strong scripting and automation skills (PowerShell, Azure CLI)4. Excellent understanding of cloud security principles5. Microsoft Certified: Azure Security Engineer Associate certification is a plus.6. Experience with Azure Sentinel for monitoring, alerting, and automation.7. Strong troubleshooting skills for identifying and resolving IAM-related issues.8. Ability to work in a dynamic environment and adapt to evolving security challenges.9. Excellent communication and collaboration skills for working with cross-functional teams.10. Commitment to maintaining a secure, compliant, and scalable IAM solutionThanks & RegardsKumar Sri Bharath\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "342_AWS Technical Lead San Antonio TX Onsite C2H": "Malavika,\nYochana\nmalavika@yochana.com\nReply to: malavika@yochana.com\nHi,This is Malavika from Yochana IT Solutions, I Recently found your resume in one of our Job portal and We are looking for \u201cAWS Lead (Onsite)\u201dwith one of our client, I have included the job information below, If you are interested, please share your updated resume.Job Title: AWS LEAD (TECHNICAL LEAD)Position Type: CTH Location: San Antonio, TX Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAASOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus. Required SkillsCandidate Self Rating(Scale 1-10)Work Experience (Years)Last Used (Year)End Customer Names/ImplementationAWS Certifications (Please confirm the details AWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration USAA Experience If Any - (Yes/No) - Eagle ID \u2013 Reporting Manager Project/Area of Work Vendor/Implementation Total Experience USAA (WHEN-WHEN) Thanks & Regards,Malavika,Resource Specialist,Yochana IT Solutions INC,Farmington Hills, MI,malavika@yochana.com Join the Referral Revolution of Yochana by Sharing, Earning, and Empowering! Ask us about our rewarding referral program.Note: If you are not interested in receiving our e-mails then please reply with subject line \u201cRemove\u201d.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "343_Senior Backend AWS and API Development Lead - Fully Remote": "Justin Davis,\nGAC Solutions\njustin@gacsol.com\nReply to: justin@gacsol.com\nHi, I hope you're doing well!Role: Senior Backend Engineer - AWS and API Development LeadWork Location: ATLANTA, GA (Onsite)6-12 monthsTypeScript, AWS AppSync, AWS Lambda, Node.JS. Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques. Responsibilities:Design, develop, and maintain APIs for mobile applications, using AWS AppSync (GraphQL) built on TypeScript and Node.JS.Utilize AWS services such as Lambda, AppSync, CloudFormation, CloudWatch, Kibana, API Gateway, and WAF to build robust and reliable APIs.Code GraphQL APIs for mobile and web applications, adhering to best practices and optimizing for performance and efficiency.Implement server-side logic using Node.js, Velocity, TypeScript, and other relevant technologies.Collaborate closely with front-end teams to understand requirements, provide technical guidance, and ensure seamless integration of APIs.Participate in Agile Kanban and Scrum methodologies, contributing to sprint planning, backlog grooming, and daily stand-ups.Conduct code reviews, provide constructive feedback, and ensure code quality, adherence to standards, and best practices.Monitor and troubleshoot API performance using CloudWatch, Kibana, Postman, Charles Proxy, and other relevant monitoring and analysis tools.Utilize Postman or similar tools to analyze and troubleshoot web services, API responses, and identify and resolve issues efficiently.Work closely with DevOps teams to deploy APIs and manage infrastructure using AWS services like CloudFormation.Stay updated with the latest advancements in AWS technologies and best practices, sharing knowledge with the team.Collaborate with other developers, architects, and stakeholders to define technical requirements and provide innovative solutions. Requirements:Solid experience developing mobile apps APIs using AWS technologies, including Lambda, AppSync, CloudFormation, CloudWatch, API Gateway, and WAF.Proficiency in coding in TypeScript to implement GraphQL APIs for mobile and web applications, with a strong understanding of GraphQL schema design and optimization techniques.Strong coding skills in TypeScript, Node.js, Velocity, and other relevant languages and frameworks.Experience coding in Swift is a plus.Familiarity with Agile Kanban and Scrum methodologies, and experience working in an Agile development environment.Excellent collaboration skills, with the ability to effectively communicate and work with front-end teams.Proactive and self-driven, with a passion for learning and staying updated with the latest technologies and best practices.Expert experience using Postman or similar tools to analyze and troubleshoot web services, API responses, and resolve issues efficiently.Familiarity with Charles Proxy or similar tools for network analysis and debugging.Strong problem-solving and analytical skills, with the ability to troubleshoot and resolve complex issues.Experience with CI/CD pipelines and DevOps practices is a plus.Good understanding of API security practices and implementation, including authentication, authorization, and WAF rules.Strong documentation skills to maintain clear and concise technical documentation.?? Thanks,Justin DavisSr Technical RecruiterE: justin@gacsol.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "344_Urgent Need: Azure Architect with EventHub, Snowflake and Cosmos DB, 100% Remote": "SAPNA,\nITECS\nsapna@itecsus.com\nReply to: sapna@itecsus.com\nUrgent need from Infocepts. Only US Citizens. Phone + Video interview.100% Remote from US location. Primary Skills: Technologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBT Job Title: Azure Architect with EventHub, Snowflake and Cosmos DBWork Location: Onshore in USA \u2013 100% Remote Purpose of the Position: Purpose of this role is to design and build data architecture for a migration project. Major responsibilities under this role would be to create high level and low level design for data integration framework, translate the business requirements into technical prototypes, help developers in developing the data integration pipelines, perform peer reviews of the code, own the technical delivery of the DI part from inception to the deployment. Also own the technical consultation for existing or new clients during initial phase of any initiative and help in proposals and pre-sales initiatives. Must Have Technical Experience:At least 12 years of IT experience; Minimum 5 years of experience on AzureMust have minimum 5+ years of experience in implementing and managing Azure Cloud components.Expertise in Python and SQL best practicesExperience in data and analytics domainKnowledge of integrating Azure Active Directory with other servicesIn-depth experience of key Azure services for data integration, BI and processing services including but not limited to Azure HDInsight, Azure Databricks, Azure Portal, Log Analytics, Azure Data Factory, , Azure Search, Azure Functions, Azure Stream Analytics, Managed Airflow and Azure Event Hub.Experience in Azure cosmos DB over PostgreSQL, mongo dB and live streaming with Azure EventHub.Experience on Azure functions with Python language.Experience in migration of on-prem applications and data analytics workload to Azure cloud. Experience in designing Azure cloud architecture and operationalizing solutions around monitoring, alerting, logging etc.Working knowledge on Architecture design, Modelling, prototyping, and benchmarking of DataStore/ EDW solutions.Knowledge of variety of advanced architecture, tools, and concepts across all layers of modern distributed technology stack covering the big data ecosystem.Knowledge of Azure storage services such as Azure storage account, Azure Cosmos DB, Azure Time Series Insights, Redis Cache, SQL Databases, Table StorageExperience with programming languages such as PythonExperience in enabling DevOps automation for Azure with appropriate security and privacy considerations.Knowledge of configuring and provisioning networks and infrastructure in cloudGood to have knowledge of Big Data ecosystems such as Hadoop / Hive, HDFS, Spark, YARN and others.Good to have Snowflake, Scala, and Java experienceGood to have Technical Experience:Pre-sales experience will be an added advantage.Any BI experienceExperience in any other cloud technologies like AWS, ALI, GCP.Snowflake, DBT, Scala and JavaTechnologies/Tools: Azure, EventHub, Snowflake, Cosmos DB, Managed Airflow, API Gateway & DBTQualifications:Bachelor\u2019s degree in computer science, engineering, or related field (master\u2019s degree is a plus)Demonstrated continued learning through one or more technical certifications or related methods. ---------------------------------------------------------------------------------------------------------------------------Please follow the submission format table provided and ensure all required information is included with each submission with resume. Incomplete submissions will be disqualified. Candidate Legal Name (As per Documents) Summary Year of experience in each skillset: Email Address Phone Number Work Authorization Current Location For non-local, Relocation (Yes/No) Pay Rate Current working status Reason for change LinkedIn Profile Availability for Interview (Please provide 03 time slots) Availability for Joining the project\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "345_Cloud Security Architect": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Cloud Security Architect\u00b7 Required qualifications to be successful in this role: 7-10 years of total IT experience with the following must haves:\u00b7 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect\u00b7 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u00b7 Excellent written and communication skills to report, document and communicate security architecture\u00b7 Excellent coordination skills and must be detail oriented Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "346_Share Only Illinois Candidate  :: Azure Solution Architect :: Chicago, IL  (Onsite)": "Brajesh Roy,\nScalable Systems\nbrajesh.roy@scalable-systems.com\nReply to: brajesh.roy@scalable-systems.com\nDear Greeting of the day! I Have an urgent job opening mention below, Please go through job description and let me know if you are comfortable or have any resume.Kindly revert me back with your updated resume as well. Job Title: Azure Solution ArchitectLocation: Chicago, IL (Need Only IL Candidate )Type: Long Term Contract Job Description:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools- In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation:- Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Thanks You,Brajesh RoyEmail: brajesh.roy@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "347_Share Only Illinois Candidate  :: Azure Cloud Developer  :: Chicago, IL  (Onsite)": "Brajesh Roy,\nScalable Systems\nbrajesh.roy@scalable-systems.com\nReply to: brajesh.roy@scalable-systems.com\nDear Greeting of the day! I Have an urgent job opening mention below, Please go through job description and let me know if you are comfortable or have any resume.Kindly revert me back with your updated resume as well. Job Title: Azure Cloud DeveloperLocation: Chicago, IL (Need Only IL Candidate )Type: Long Term Contract Job Description:Key Responsibilities: Design and develop microservices using Spring Cloud. Implement and maintain containerized applications using Docker and Kubernetes. Deploy and manage applications on Microsoft Azure. - Collaborate with cross-functional teams to define, design, and ship new features. Ensure the performance, quality, and responsiveness of applications. - Identify and correct bottlenecks and fix bugs. Help maintain code quality, organization, and automatization. Requirements: Bachelor\u2019s degree in Computer Science, Engineering, or related field. Proven experience as a Spring Cloud Developer. Strong proficiency in Java and Spring Framework (Spring Boot, Spring Cloud). Hands-on experience with containerization technologies like Docker and orchestration tools like Kubernetes. Extensive experience with Microsoft Azure services (e.g., Azure Kubernetes Service, Azure DevOps, Azure Functions, etc.). Familiarity with CI/CD pipelines and tools. Knowledge of RESTful API design and implementation. Understanding of distributed systems and microservices architecture. Experience with configuration management tools such as Spring Cloud Config. Excellent problem-solving skills and attention to detail. - Strong communication and collaboration skills. Preferred Qualifications: Strong proficiency in Java 8 + and Spring Framework 3. x + (Spring Boot, Spring Cloud).- Experience with other cloud providers (AWS, Google Cloud). Knowledge of monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack). Familiarity with Agile/Scrum methodologies. Certification in Microsoft Azure or other relevant technologies. Thanks You,Brajesh RoyEmail: brajesh.roy@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "348_AWS Cloud Architect Position at San Antonio, TX  - Onsite from day1 - CTH": "Pavan Kumar,\nYochana IT\npavank@yochana.com\nReply to: pavank@yochana.com\nMandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS , Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, AWS Cloud Watch, Kibana, Grafana, PrometheusOverview:Looking for 15+ years experienced talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "349_AI Cloud Engineer (GCP)": "LAL,\nTechgene\nlal.b@techgene.com\nReply to: lal.b@techgene.com\nNote: Linkedin is mandatory & NO H1BJob Title: AI Cloud Engineer (GCP)Location: Charlotte, NCNote: we have 8 positions.Responsibilities:We are seeking a skilled and passionate Cloud Engineer with strong expertise in Google Cloud Platform (GCP) AI services to join our team.You will play a crucial role in building, deploying, and managing our AI platform, enabling us to harness the power of AI for numerous use cases.This is a unique opportunity to be at the forefront of AI innovation in the banking industry. Responsibilities:Design, develop, and implement a model-agnostic, cloud-agnostic, and use-case-agnostic AI platform leveraging GCP services.Architect and build scalable, high-performance infrastructure for AI model training, deployment, and inference.Develop and maintain CI/CD pipelines for AI model development and deployment.Integrate and manage AI services like Vertex AI, BigQuery ML, and AI Platform Pipelines.Develop and implement monitoring and logging solutions for the AI platform.Collaborate with data scientists, engineers, and business stakeholders to define and implement AI solutions.Stay abreast of the latest advancements in AI and cloud technologies.Contribute to the development of best practices and standards for AI development and deployment within the organization. Required Skills and Experience:3+ years of experience in cloud engineering with a strong focus on Google Cloud Platform (GCP).Proven expertise in designing, implementing, and managing GCP AI services like Vertex AI, BigQuery ML, and AI Platform Pipelines.Solid understanding of AI/ML concepts and model lifecycle management.Experience with containerization technologies like Docker and Kubernetes.Strong programming skills in Python or Java.Experience with CI/CD pipelines and automation tools.Excellent communication and collaboration skills.Ability to work independently and as part of a team.Passion for learning and staying up-to-date with the latest technologies. Desired Skills:Experience with other cloud platforms like AWS or Azure.Experience with Big Data technologies like Hadoop, Spark, or Hive.Experience with DevOps practices and tools.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "350_Title: Sr Cloud Network Engineer : Onsite": "Amol,\nChabeztech\namol@chabeztech.com\nReply to: amol@chabeztech.com\nTitle: Sr Cloud Network Engineer : Onsite Location can work from Onsite from any of the locations \u2013 Hybrid 3 days Alpharetta GAFrisco TXKansas City KansasBellevue WA Below is the Job Description Key ResponsibilitiesOversee the network onboarding process for new users and systems into Cloud environmentProvision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.Conduct regular network assessments and audits to ensure compliance with internal and external requirements.Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.Provide technical support and troubleshooting for Cloud network-related issuesStay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms QualificationsExperience with network onboarding and provisioning in cloud environments.Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, MacieIn-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.Familiarity with security regulations, standards, and compliance requirements for cloud network environments.Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.Strong communication and documentation skills for collaborating with cross-functional teams.Experience with automation tools (e.g., Python, Terraform) for network configuration and managementRelevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "351_DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDESCRIPTION OF SERVICES.The Worker will perform highly advanced DevOps related work as part of a cross-functional team under the direction and guidance of the Shared Application Services manager. The Worker must have strong hands-on professional software development and/or IT operations experience building, testing, and deploying secure production applications and systems using continuous integration and continuous delivery/deployment (CI/CD) pipelines in a large-scale enterprise environment. The worker will be responsible for designing, building, and maintaining our CI/CD pipelines, monitoring applications and infrastructure for performance issues, and conducting regular assessments to ensure systems are performing optimally. The worker will also provide training on DevOps practices and stay updated on latest technologies. They should have expertise in a variety of DevOps tools, knowledge of DevOps automation, and strong experience with Linux administration, containerization technologies, and microservices architecture. The worker is expected to proactively address potential security risks and performance issues to ensure the security, stability, and efficiency of our CI/CD pipelines.The Worker to work on the following initiatives and will perform advanced tasks such as: CI/CD Modernization and automation of the entire pipeline from code check-in to deployment utilizing industry best practices such as Infrastructure as Code (IaC), Configuration as Code (GitOps), and Blue-Green and Canary Deployment Strategies. Implement \"Shift Left\" security approach by integrating security tools and automating security checks and compliance into the CI/CD pipeline. Monitoring and Observability to provide comprehensive monitoring, logging, and alerting for the CI/CD pipeline. Participating in all phases of SDLC. Performing extensive code reviews and analysis. Writing reports on code analysis to determine if industry standards and secure coding best practices are being followed; provide analysis to address found short comings. Providing guidance and knowledge sharing to existing development staff.Must be able to: develop and maintain CI/CD pipelines and automate building, testing, and deployment of software applications. collaborate effectively with software engineers, operations staff, security teams, testers, and various stakeholders to identify and implement \"shift left\" security measures and best practices in the CI/CD pipelines. design, test, and implement secure software development practices and standards in the CI/CD pipelines. continuously monitor applications and infrastructure for security vulnerabilities and performance issues and coordinate remediation efforts. troubleshoot and resolve issues in the CI/CD pipelines to minimize downtime and limit impact to developers and stakeholders participate in incident response and troubleshooting sessions. In coordination and alignment with Information Security Office, document and communicate security procedures and policies. stay abreast of new technologies, industry standards and best practices, and integrate them into the pipeline architecture and design where applicable. be productive working independently or in a team environment (both local and remote) with minimal supervision.Must possess: knowledge of DevSecOps methodologies, concepts, and practices knowledge of DevOps Automation comprehensive technical expertise in a variety of DevOps tools, including Ansible, Jenkins, Maven, Artifactory, SonarQube, Xray, Checkmarx, Jira, BitBucket, Subversion, Git/Version Control Software, or comparable technologies. familiarity with information security frameworks and standards such as NIST and OWASP Top 10. strong understanding of Linux administration and scripting languages (e.g., PowerShell, Bash, Python) experience with microservices architecture and cloud-native development. experience with containerization and orchestration technologies like Docker and Kubernetes. strong written, verbal, and interpersonal communication skills. strong problem-solving skills and ability to multi-task with readiness to put in extra effort when necessary the willingness to learn\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "352_Urgent role of DevOps Engineer at Plano, TX": "Akshat,\nTanishasystems.inc\nakshat@tanishasystems.com\nReply to: akshat@tanishasystems.com\nGreetings..!! My name is Akshat, and I am a Technical Recruiter at Tanisha Systems Inc. Tanisha Systems Inc is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients. Job Title : DevOps EngineerLocation : Plano, TX(Hybrid-3 days per week onsite)Type: ContractJob Summary :\u2022 we need DevOps with Java and Canary(monitoring tool) Qualification:BSc in Computer Science, Engineering or relevant fieldExperience as a DevOps Engineer or similar software engineering roleProblem-solving attitudeCollaborative team spiritGood knowledge of AWS, Cloud Watch. RegardsAkshat KumarAkshat@tanishasystems.comwww.tanishasystems.comlinkedin.com/in/akshat-kumar-7245821a299 Wood Ave South, Suite # 308, Iselin, New Jersey, NJ 08830, USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "353_Urgent :: Requirement for Devops Engineer,  Manassas, VA (Hybrid),  Face to Face Interview must for this role": "Akhand Singh,\nAdventa Tech Inc\nakhand@adventatech.com\nReply to: akhand@adventatech.com\nI hope you are doing great,Please let me know if you have any consultants for that requirement. Face to Face Interview must for this role Job Title: Devops EngineerClient Name/Domain: Location: Manassas, VA (Hybrid)Duration: 4 MonthsMode of Interview: In-PersonVisa : No H1b , CPT , OPTNote: LinkedIn id LinkedIn Id and JD should be aligningVisa and DL copyNeed candidates who are local to Virginia or Candidates should be able to relocateMust have experience in the skills mentionedJob Description:Key Responsibilities:Design, implement, and manage CI/CD pipelines using Jenkins to streamline development, testing, and deployment processes.Develop and maintain infrastructure as code using tools like Terraform, Ansible, or similar technologies.Deploy, manage, and scale applications using Kubernetes and other container orchestration platforms.Utilize cloud platform AWS, for infrastructure and application deployment.Automate deployment processes to ensure efficient and reliable releases.Write and maintain scripts in Python or Java for various automation tasks.Collaborate with development, QA, and operations teams to ensure seamless integration and delivery of solutions.Perform product testing to validate functionality, performance, and reliability of solutions.Troubleshoot and resolve issues related to application and infrastructure performance.Monitor system performance, identify bottlenecks, and propose optimizations.Ensure security and compliance standards are maintained throughout the development and deployment processes.Stay up-to-date with the latest industry trends and technologies to continually improve our DevOps practices. Qualifications:Proven experience as a DevOps Engineer or similar role.Strong background in CI/CD using Jenkins.Hands-on experience with infrastructure as code (IaC) tools such as Terraform or Ansible.Proficiency in container technologies like Docker and orchestration platforms like Kubernetes.Solid understanding of cloud platform AWS.Programming skills in Python or Java.Experience with deployment automation.Good understanding of Red Hat Enterprise Linux (RHEL).Familiarity with vCenter API.Experience in product testing and quality assurance practices.Strong problem-solving skills and attention to detail.Excellent communication and collaboration abilities.Ability to work independently and as part of a team in a fast-paced environment. SkillsAutomation and TestingContainer Technologies on LinuxDEV-OPSInfra As CodeLinux/RHEL OSPython /Java RegardsAhand Singh \u2706: 571 463 1138 |\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "354_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "355_Azure Cloud Security Architect || Midland, MI": "Shivam Kamboj,\nSiri Info Solutions\nshivam.kamboj@siriinfo.com\nReply to: shivam.kamboj@siriinfo.com\nGreetings, Hope you are doing wellPlease find the JD and share me your resume if you are interested in this position Azure Cloud Security ArchitectMidland, MI JD Strong knowledge of the Microsoft Cloud Platforms features, functionalities and best use cases\u2022 Experience in managing Azure Policies and initiatives in Azure\u2022 Thorough understanding to Azure Policy Remediation and Azure Role-Based-Access-Control (RBAC)\u2022 Thorough understanding of Defender for Cloud\u2022 Comprehensive understanding of networking, cloud computing, and enterprise infrastructure.\u2022 Design, implement, and maintain Azure DevOps pipelines, build processes, CI/CD processes with Custom policies.\u2022 Create and maintain Azure DevOps dashboards.\u2022 Design and implement Azure security policies.\u2022 Hands-on experience in Microsoft Azure Cloud Services including Defender for Cloud, Cloud Security Posture Management (CSPM), Azure security tools/platforms such as Azure Entra ID, Sentinel, Key Vault etc.\u2022 5+ years managing security policies and initiatives in Azure.\u2022 Working knowledge of Zero Trust, threat management, SOC monitoring (SIEM / SOAR), and Extended Detection & Response (XDR) + SIEM, Cloud Security, Identity, RBAC, and ServiceNow.\u2022 Working knowledge of current NIST 800-53 for Azure, PCI, GDPR and Azure CIS Benchmark compliance\u2022 Relevant certifications such as CCSP, CISSP, CISM or Azure security certifications are highly desirable. RegardsShivam KambojTechnical RecruiterSiri Info Solutions.Mail Id: shivam.kamboj@siriinfo.comDisclaimer: We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce. Note : Any resume submitted by Siriinfo is presented with the understanding that the candidate is being considered for your direct end-client (end-client is the company where the work will be performed). If there is any other company involved between the end-client and your company, please do not submit this resume without our written approval. If you submit the resume to another third party, Siriinfo reserves the right to work with the third party directly.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "356_Urgent :: Requirement for Cloud Engineer,  Waukegan or Willis Tower, IL.(Need Local)": "Akhand Singh,\nAdventa Tech Inc\nakhand@adventatech.com\nReply to: akhand@adventatech.com\nI hope you are doing great,Please let me know if you have any consultants for that requirement. Need Local candidate of IL with DLFinal round Interview is F2F Job Title: Cloud EngineerLocation: on-site in Waukegan or Willis Tower, IL.(Need Local)Visa: NO H1B / CPTDURATION: 6 MONTHS+MOI: Skype + F2FNOTE: Please Send Candidate LinkedIn. Job Description:Looking for a Cloud Engineer to join the Cloud Services Engineering practice. This role will be instrumental in deploying solutions in the public cloud space, AWS, or Azure. Experience provisioning public and private cloud infrastructure using Terraform. Ability to design and deliver cloud solutions securely via Infrastructure as Code. Experience with GitHub, Jenkins, and Harness.The Cloud Engineer should possess the following:\u2022 Ability to use a wide variety of open-source automation tools for provisioning and configuration management (Ansible, Terraform, Packer).\u2022 Ability to define pipelines in Jenkins or Harness for provisioning infrastructure.\u2022 Ability to code and script in support of infrastructure management.\u2022 Experience with Enterprise systems and IT operations.\u2022 Being comfortable with frequent, incremental code testing and deployment.\u2022 Understanding and focus on business outcomes.\u2022 Understanding and efficiency in collaboration, open communication and reaching across functional borders. Must Have:\u2022 Over 5 years of experience working with Infrastructure as Code\u2022 Experience with continuous integration tools such as Jenkins\u2022 Experience with AWS or Azure\u2022 Experience with Docker and Kubernetes Top Skills:\u2022 5+ years of Infrastructure as Code (IaC) experience.\u2022 Terraform; Must be able to provision and build out Infrastructure with Terraform.\u2022 Packer; Must be able to use Golden Images to Deploy OS\u2019s on prem.\u2022 Ansible; Use Automation with Ansible and create CI/CD Pipelines.\u2022 Experience designing and delivering cloud services in AWS or Azure.\u2022 Containerization experience with Docker and Kubernetes. Additional Skills & Qualifications:\u2022 This role is preferred to be on-site in Waukegan or Willis Tower, IL.\u2022 Schedule is 4 - 5 days/week in one of the above two locations. Unfeigned Regards, Akhand SinghAdventa Tech Inc.Cell: +1 571 463 1138E-Mail: akhand@adventatech.comWebsite: www.adventatech.com Disclaimer: This communication, along with any documents, files or attachments, is intended only for the use of the addressee and may contain confidential information. If you are not the intended recipient, you are hereby notified that any dissemination, distribution or copying of any information contained in or attached to this communication is strictly prohibited, To remove your email address permanently from future mailings, please send REMOVE to remove@adventatech.com.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "357_Azure Solution Architect (Need Only IL Candidate )": "Pradeep,\nScalable Systems\npradeep.sharma@scalable-systems.com\nReply to: pradeep.sharma@scalable-systems.com\nHi,Greetings of the day! I have an urgent requirement below, please go through JD and let me know if you are comfortable or have any profile. Kindly revert me back with your updated resume as well. Job Title: Azure Solution Architect (Need Only IL Candidate ) Location: Chicago, IL (Work from Office)Duration: Long Term Contract Job Description:This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration:- Lead the planning, design, and execution of cloud migration projects to Azure.- Assess existing on-premises infrastructure, applications, and data for cloud readiness.- Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed.- Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform:- Design and implement infrastructure using Terraform for repeatable and consistent deployment.- Develop and maintain Terraform scripts and modules for provisioning Azure resources.- Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab.- Automate build, test, and deployment processes to enhance efficiency and reliability.- Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools.- Integrate testing tools with CI/CD pipelines to enable automated testing.- Develop and enforce testing best practices and standards to ensure high-quality deliverables.- Provide support activities for the queries, issues, access, installation and configuration for supported tools- In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives.- Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows.- Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect Thanks & Regards, Pradeep SharmaEmail: pradeep.sharma@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "358_Azure Cloud Developer(Need Only IL Candidate )": "Pradeep,\nScalable Systems\npradeep.sharma@scalable-systems.com\nReply to: pradeep.sharma@scalable-systems.com\nHi,Greetings of the day! I have an urgent requirement below, please go through JD and let me know if you are comfortable or have any profile. Kindly revert me back with your updated resume as well. Job Title: Azure Cloud Developer (Need Only IL Candidate ) Location: Chicago, IL (Work from Office)Duration: Long Term Contract Job Description:- Design and develop microservices using Spring Cloud.- Implement and maintain containerized applications using Docker and Kubernetes.- Deploy and manage applications on Microsoft Azure.- Collaborate with cross-functional teams to define, design, and ship new features.- Ensure the performance, quality, and responsiveness of applications.- Identify and correct bottlenecks and fix bugs.- Help maintain code quality, organization, and automatization. Requirements:- Bachelor\u2019s degree in Computer Science, Engineering, or related field.- Proven experience as a Spring Cloud Developer.- Strong proficiency in Java and Spring Framework (Spring Boot, Spring Cloud).- Hands-on experience with containerization technologies like Docker and orchestration tools like Kubernetes.- Extensive experience with Microsoft Azure services (e.g., Azure Kubernetes Service, Azure DevOps, Azure Functions, etc.).- Familiarity with CI/CD pipelines and tools.- Knowledge of RESTful API design and implementation.- Understanding of distributed systems and microservices architecture.- Experience with configuration management tools such as Spring Cloud Config.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration skills. Preferred Qualifications:- Strong proficiency in Java 8 + and Spring Framework 3. x + (Spring Boot, Spring Cloud).- Experience with other cloud providers (AWS, Google Cloud).- Knowledge of monitoring and logging tools (e.g., Prometheus, Grafana, ELK Stack).- Familiarity with Agile/Scrum methodologies.- Certification in Microsoft Azure or other relevant technologies. Thanks & Regards, Pradeep SharmaEmail: pradeep.sharma@scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "359_AWS Data Engineer (12+ Years)--Local to CA--Visa Independent Candidates Only": "Criz Salvin,\nNorthITe\ncriz@northite.com\nReply to: criz@northite.com\nPosition: AWS Data EngineerLocation: Los Angeles, CA (Hybrid)Duration: Long TermJob Description:This role work: Hybrid. In office Tues/Wed/ThursResource has to be Local: Yes \u201cMust-have\u201d skills1. AWS Data Integration2. Data Warehousing3. Data Modelling Years of experience required for each skillAt least 5 years Job Duties:\u2022 Define the direction and use of the AWS services such as AWS Glue, AWS Lambda, Amazon S3, Amazon Redshift\u2022 Provide expertise in AWS security best practices, including VPC, IAM, and data encryption.\u2022 Provide expertise on the approach to data modeling, including conceptual, logical, and physical data models within AWS\u2022 Develop data integration and transformation of data using tools such as AWS Glue, Python/Pyspark, SNP Glue\u2022 Optimization data integrations and debugging of issues in the data flow\u2022 Contribute and author deliverables which data integration strategy and the design, build and testing of data integrations \"Nice-to-have\u201d skills:1. Python/PySpark2. SAP ISU3. SQL\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "360_Job Role: Cloud Architect || Raritan, NJ (Onsite)": "Mahendra,\nRao\nmahendrar@vbeyond.com\nReply to: mahendrar@vbeyond.com\nHi,Hope you are doing well,One of my premium clients is looking for, Job Role: Cloud ArchitectLocation: Raritan, NJ (Onsite)Project Type: Long Term Contact Requirements:Should have strong Google Cloud Platform (GCP) experience with all the different components implementation experience like Data Flow, Vertex AIStrong implementation experience in Micro Services ArchitectureExcellent knowledge of cloud computing technologies and current computing trends in GCP & AWSDevelop and maintain data management standards, policies, and guidelines.Assist in the development of overall project plans and timetables, analysis and identification of intermediate deliverables.Designing and implementing integration solutions that meet business needsProvide thought leadership; identify and analyze technical innovations and concepts supporting business objectives.Ability to write technical and design documents for proposed solutionShould have good understanding on the Cloud Infrastructure implementations and API integrations across the business programBe informed on current and emerging technological trends and communicate them with business to drive new opportunitiesFacilitate the establishment and execution of the roadmap and vision for information delivery and data management for analytics programs Provide advice, guidance, direction, and authorization to carry out major business program plansShould must be a Computer Science Technical Graduate (B.E / B.Tech) or Masters in Computer Sciences. Thanks & Regards,Mahendra Rao || Technical RecruiterVBeyond Corporation I PARTNERING FOR GROWTHHillsborough, New Jersey, USABottom of FormEmail : Mahendrar@VBeyond.comLinkedIn: linkedin.com/in/mahend-rao-966150156\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "361_AWS CLOUD NATIVE DEVELOPER - local to Georgia": "Sharad Kumar,\nPredica Inc.\nsharad@predicaz.com\nReply to: sharad@predicaz.com\nAWS CLOUD NATIVE DEVELOPER - local to Georgia SkillRequired / DesiredAmountof ExperienceProven experience as a software developer with a strong understanding of cloud computing principles and practicesRequired8YearsWork experience designing, developing, and deploying cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3Required5YearsWork experience Implementing serverless architectures using AWS Lambda functions with PythonRequired5YearsWork experience building and orchestrate workflows using AWS Step Functions and AWS State MachinesRequired5YearsWork experience designing, developing, and implementing SOAP-based web services using services technologiesRequired5YearsWork experience with XML, XSD, WSDL, and other related technologiesRequired5YearsAgile methodologies and SCRUM frameworkRequired3YearsExperience with CI/CD pipelines and DevOps practicesHighly desired Experience with containerization (e.g., Docker) and orchestration (e.g., Kubernetes)Highly desired AWS certifications (e.g., AWS Certified Developer)Highly desired Regards,sharath kumarsharad@predicaz.com Predica Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "362_C2C Opportunity for  AWS LEAD (DevOps exp) in San Antonio, TX": "Kiran,\nsapphiresoftwaresolutions\nkirankumar@sapphiresoftwaresolutions.com\nReply to: kirankumar@sapphiresoftwaresolutions.com\nJob Title: AWS LEAD (DevOps exp)Location: San Antonio, TX Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, Elatics SearchOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts.Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.Ability to design patterns for moving legacy applications to AWS and private cloud.Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)Experience with SAFe framework or other similar agile frameworks.Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "363_Aws Cloud Native Developer": "Archana Kalyankar,\nHanker Systems\narchanak@hankersystems.com\nReply to: archanak@hankersystems.com\nHello, I am a Recruiter at Hanker Systems. I am reaching out to you on an exciting job opportunity with one of direct client. Title: AWS DEVELOPER(AWS Cloud Native Developer)Location: Atlanta GA Need only locals In-person interview Required skills: Proven experience as a software developer with a strong understanding of cloud computing principles and practicesBachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Work experience designing, developing, and deploying cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3Work experience Implementing serverless architectures using AWS Lambda functions with PythonWork experience building and orchestrate workflows using AWS Step Functions and AWS State MachinesWork experience designing, developing, and implementing SOAP-based web services using services technologiesWork experience with XML, XSD, WSDL, and other related technologiesAgile methodologies and SCRUM frameworkExperience with CI/CD pipelines and DevOps practicesExperience with containerization (e.g., Docker) and orchestration (e.g., Kubernetes)AWS certifications (e.g., AWS Certified Developer).Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes). ThanksArchanak@hankersystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "364_Lead Azure Cloud Engineer": "Harsh,\nAtlas Cloud\nharsh@acloudinc.com\nReply to: harsh@acloudinc.com\nHi.Hope you are doing great,We have requirement with our client let us know if u have suitable profile. Client : AetnaRole : Lead Azure Cloud EngineerLocation : Location: Tampa Hybrid \u2013 3 Days onsiteExp: 7+ years Job Description : Platforms \u2013 Work with development teams to stand up desired cloud environments.Requirements Elaboration \u2013 Identify the needs for build automation and implement appropriate solutions.System Performance \u2013 Contribute to solutions that satisfy performance requirements, tune application performance issues, ensure service uptime and response time meet SLAs.Standards \u2013 Be aware of CICD standards and best practices and utilize them in solutions effectively.Documentation \u2013Develop and maintain system documentation.Support team in managing client expectations and resolving issues on time. Talents Needed for Success:Bachelor\u2019s degree in computer science, Applied Computer Science, or related field.5+ yePassion for technology innovation, a curious mind, and an entrepreneur mindset.Abars or related experience.ility to present technical information clearly to different management levels.Azure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plusOther experience to include the following technologies: CICD patterns, Terraform.Experience using the following tools: GIT, Bit Bucket, Jira, Confluence, Jenkins.Experience with Python scripting preferred.Working knowledge of AWS is a plus.Knowledge of different software development methodologies (Waterfall, Agile, Scrum, Kanban)Platforms \u2013 Work with development teams to stand up desire Harsh,Delivery Lead,Atlas Cloud, solution for your IT.P: +1 4193797060E: harsh@acloudinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "365_Mid-Level AWS Postgres Database Engineer with Python (DBA).............100% Remote": "Vijay,\nTechrakers\nvijaym@techrakers.com\nReply to: vijaym@techrakers.com\nMid-Level AWS Postgres Database Engineer with Python (DBA)Location: 100% Remote Long Term Position DescriptionSuccessful candidate will join an agile team that enhances, maintains, and supports CRISP and DASL, two Postgres-centric enterprise data platforms that are critical to FINRA. They will serve as a Postgres and Python expert. All members of the team provide production support; this will occasionally require work during off hours (weekends and nights.)Requirements:Five or more years of experience in Amazon Web Services RDS Postgres or Aurora Postgres. Experience should include development, maintenance, monitoring, and tuning.Any additional experience with AWS services such as AWS Database Migration Service (DMS), Elastic Container Service, Lambda, Fargate, EC2 or S3 is also desirable.Python development expertise and professional experience is required, including experience with boto3.Experience with all phases of the SDLC software development life cycle from analysis, design, development, testing, implementation and maintenance and production support with timely delivery against deadlines.Proficient in using Unix shell Scripting, SFTP. Created Shell Scripts for invoking python/SQL scripts. Experience with implementing Jenkins Job pipelines. Experience using git for version control.Familiar with and able to use Scrum principles in an enterprise environment.Excellent communication, interpersonal and analytical skills, and a highly motivated team player with the ability to work independently. This position is on a central data platform team\u2014the ability to work effectively with our partner teams is essential. Technical SkillsLanguage: Python, SQL, HTML, XML, Shell Script, JavaScript, Spark SqlDatabase: Postgres, Aurora, Oracle, MongoDBCloud Services: AWS RDS, DMS, Lambda, S3, EC2, ECS/FargateVersioning Tools: Git, CVS, SVN, BitbucketTools IDE: PyCharm: Eclipse, IntelliJ, JupiterOperation Systems: Windows, Linux, Unix\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "366_Aws Cloud Native Developer | Atlanta, GA 30334 (Hybrid) | Locals only": "Charan,\nAnveta\ncharan@anveta.com\nReply to: charan@anveta.com\nRole: Aws Cloud Native Developer Location: Atlanta, GA 30334 (Hybrid)Note: Looking for Locals onlyDuration: 6 +Months Interview Type: Web Cam Interview Only Job Description Key Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team. Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills. Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes). Thanks & RegardsCharanAnveta, Inc.1333 Corporate Drive, Suite #108Irving, TX 75038charan@anveta.comhttps://www.linkedin.com/in/charan-reddy-ba6450236/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "367_Requirement :: AWS CLOUD NATIVE DEVELOPER :: Atlanta GA   :: contract": "navya gupta,\nStellentit\nnavya@stellentit.com\nReply to: navya@stellentit.com\nHello, I hope you are doing well!Kindly acknowledge me, are you Comfortable with this Position then please share me your updated resume. Looking only local candidate.Any visa workable. Job Description:- AWS CLOUD NATIVE DEVELOPERLocation: 47 Trinity Ave, Atlanta GA Contractlocal candidates within 50 mile distance is highly preferred, the client will look at those candidate first. Job description:As an AWS Cloud Developer you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platformProject Overview: The GTA CJEP project is a modernization of the Criminal Justice Exchange Program that shares data between different agencies, counties, cities, etc. This project will take the current SoftwareAG centric solution and produce a Cloud Native solution in AWS to move and consolidate data for counties that sign-up for the service. The touchpoints for this data sharing will be third party vendors as well as state and county systems. Current API\u2019s and web services will be leveraged to facilitate this modernized solution as much as possible.Position Overview: As an AWS Cloud Developer at GTA you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platform. You will collaborate closely with cross-functional teams in a SCRUM Agile environment to deliver high-quality software solutions that meet business objectives. The ideal candidate will have extensive experience with SOAP-based web services,custom header implementation, and handling MTOM (Message Transmission Optimization Mechanism) attachmentsKey Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills.Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).Skills: Proven experience as a software developer with a strong understanding of cloud computing principles and practices - Required, 8 YearsWork experience designing, developing, and deploying cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3 - Required, 5 YearsWork experience Implementing serverless architectures using AWS Lambda functions with Python - Required, 5 YearsWork experience building and orchestrate workflows using AWS Step Functions and AWS State Machines - Required, 5 YearsWork experience designing, developing, and implementing SOAP-based web services using services technologies - Required, 5 YearsWork experience with XML, XSD, WSDL, and other related technologies - Required, 5 YearsAgile methodologies and SCRUM framework - Required, 3 YearsExperience with CI/CD pipelines and DevOps practices - Highly desired Experience with containerization (e.g., Docker) and orchestration (e.g., Kubernetes) - Highly desired AWS certifications (e.g., AWS Certified Developer) - Highly desiredQuestions:1. What is your candidate's email address?2. The client has agreed to allow the selected local candidate to work a hybrid situation with on-site work to be determined by the manager. Do you and your candidate accept this requirement?3. \u200b\u200b\u200b\u200b\u200b\u200b\u200bIf selected for engagement, your candidate will be expected to start no later than 2 weeks (10 business days) after the client's selection date? Do you agree to this requirement?4. Candidates must be located in and work in GA to start this position if offered. Do you and your candidate accept this requirement?5. If selected and your candidate is not local, is your candidate wiling to relocate to the Atlanta area beginning Day 1?6. In what City and State is your candidate located? NOTE: An answer of \"Yes\" WILL NOT be accepted. Navya GuptaSr. IT Technical RecruiterPhone:- 3212189059Email: navya@stellentit.comGtalk: navya@stellentit.comLinkedin id:- linkedin.com/in/navya-gupta-1a879024a\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "368_Urgent Job Role - Cloud Engineer-Developer III - Onsite": "garima solanki,\nInceptra Solutions\ngarima.solanki@inceptrasolutions.com\nReply to: garima.solanki@inceptrasolutions.com\nRole: Cloud Engineer/Developer IIILocation: Onsite, Cincinnati, OhioDuration: 6+ months Job DescriptionJob Summary: Responsible for the design, implementation, and ongoing operational efforts of the infrastructure services, server OS, MDM service, patch deployment/protection, backup and restoration, directory service capabilities, and certificate management. Primary Responsibilities: Development of Infrastructure Services strategy to support standard-based directory services, single sign-on solutions, server processing and storage consolidation, standard backup and restoration services, comprehensive security and patch protection capabilities, and a robust services infrastructure. Ongoing maintenance and advancement of the server environment through effective engineering and implementation of solutions, which support the user and application needs of the organization. Installs and maintains cloud-based applications, systems, or associated infrastructures. Analyzes and defines business requirements to determine specifications and standards; proposes and develops solutions. Deploys and oversees implementation and integration of web-based applications, ensuring that appropriate information security standards are met. Architect, deploy, and administer computing services in various Cloud Infrastructure providers Troubleshoot Converged Infrastructure Systems to resolve compute, storage, network and other infrastructure problems Engineer and perform Capacity Management for Infrastructure Solutions & Services Monitors system uptime and performance, troubleshooting and resolving errors. Stays informed of emerging cloud technologies and evaluates their value to the organization's operations. Qualifications: Requires experience with one or more cloud computing platforms. Expert level experience with server-level operating systems (Windows & Linux), facilitating high-level engineering, architectural design and support. Experience with Desktop Virtualization Experience with Windows Active Directory Services and Identity Management Solutions Experience with automation/configuration management \u2022 Expertise in continuous integration and continuous delivery concepts and practices Ability to use a wide variety of open source technologies and cloud services (experience with AWS is required) Understanding of code and scripting languages (PHP, Python, Perl, Powershell and/or Ruby) Expertise in automated build and deploy processes and tools \u2022 Experience monitoring and designing solutions for the cloud Expert level knowledge of distributed version control systems like GIT Experience working on Agile\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "369_Immediate interview available for DevOps Track Architect position located in Multiple Locations (USA)": "Gobinath Sithanathan,\nLorven Technologies Inc.\ngobinath@lorventech.com\nReply to: gobinath@lorventech.com\nHi, Greetings from Gobinath - Talent Acquisition Team \u2013 Lorven Technologies Inc., We are urgently looking out for the Candidates for the below position. If you are interested in below role to apply, please give a quick response hereIn case this job does not match your profile and you know somebody, who can fit into this role, kindly forward this mail to them. Position: DevOps Track ArchitectLocation: Multiple Locations (USA)Job Mode: Fulltime / Contract Job Description:8-10 years of strong experience in migration, consolidation of Jira, Confluence instances to a centralized instanceRich experience in conducting Jira, Confluence DC to DC migrations, Jira, Confluence DC to Cloud migrations.Expert in conducting Jira, Confluence current state assessments and carry out the consulting activities, prepare plan for the migrations.Excellent in preparing Jira, Confluence migration plan, preparing check list and validation plan etc.,Strong in developing, customizing Jira workflows, and integrating them in Software development lifecycle.Excellent in knowing Jira, Confluence governance, controls and standards.Developing API, Utilities as part of the migration process for various activitiesVery good in leveraging Script Runner plug-in and writing Groovy ScriptsExcellent in performing Day-Day Jira, Confluence administration activities.Excellent Communication and interpersonal skills\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "370_AWS CLOUD NATIVE DEVELOPER -Need AWS certifications": "sana,\nVrddhi Solutions, LLC\nsana@vrddhisolutions.com\nReply to: sana@vrddhisolutions.com\nRole: AWS CLOUD NATIVE DEVELOPERLocation: 47 Trinity Ave Atlanta GAVisa: No OPT, CPT.Experience: 8 + years requiredCandidates must be located in and work in GA to start this position if offered.Should have AWS certifications (e.g., AWS Certified Developer) Position Overview: As an AWS Cloud Developer at GTA you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platform. You will collaborate closely with cross-functional teams in a SCRUM Agile environment to deliver high-quality software solutions that meet business objectives. The ideal candidate will have extensive experience with SOAP-based web services, custom header implementation, and handling MTOM (Message Transmission Optimization Mechanism) attachmentsKey Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills.Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "371_Sr. Cloud Engineer at Atlanta ,GA": "Dheeraj srivastava,\nDatum Software\ndheeraj@datumsoftware.com\nReply to: dheeraj@datumsoftware.com\nCurrently, we have an opening for Sr. Cloud Engineer with our Client in Atlanta, GA || Hybrid. I appreciate your time and look forward to hearing from you. Job Details: Job Title: Sr. Cloud EngineerDuration: Long-Term ContractLocation: Atlanta, GA || Hybrid Responsibilities:Provide technical leadership and design expertise for AWS Foundation integration.Define standards and patterns for when and how to efficiently leverage AWS services.Lead automation of AWS integration processes and tasks.Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners.Develop frameworks and tools to enable dev teams to consume authorized AWS services.Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins.Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS.Ensure Security-by-Design is a foundational component of every app deployed into AWS.Ensure enterprise monitoring standards are applied to each AWS app deployment.Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration.Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies).Perform cost benefit analysis to determine the best system architecture via software, hardware, internal cloud, externally hosted or another defined model.Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs.Partner with peer Infrastructure teams to provide enterprise class AWS integration.Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designs. Qualifications:4+ years of experience in design and implementation of large enterprise public cloud environments.2+ years of hands-on experience integrating mission critical applications into AWS.Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.)Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools.Fluency with one or more scripting/coding languages such as Java, JavaScript, REST, JSON, Python, Bash.Experience implementing and leveraging logging and monitoring solutions - Experience in cloud native architectures and micro-services design.Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects.Experience in design of complex distributed systems environments.Demonstrated ability to think strategically about business, product, and technical challenges.Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail.Independent and self-motivated and very thorough worker. PreferredCandidate must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "372_AWS Cloud Engineer - REMOTE": "Swati Gupta,\nTechnocraft Solutions LLC\nswati.gupta@technocraftsol.com\nReply to: swati.gupta@technocraftsol.com\nHello, Hope you are doing well! My name is Swati and I'm a Talent Acquisition Executive at Technocraft solutions. We provide IT Consulting Services to our customers\u2019 immediate and long-term resource needs. I am contacting you either because your resume has been posted to one of the internet job sites to which we subscribe or you had previously submitted your resume to Technocraft solutions. AWS/Cloud EngineerREMOTERequirements:5-7 years\u2019 Experience in API and microservices development on AWS cloud platformAPI design and documentation. API and microservices development on AWS cloud platform, AP testing, integration and deployment.5-7 years\u2019 AWS Stack Experience (LAMBDA, AURORA, RDS, EVENT BRIDGE)CI / CD tools Jenkins or GitHubExperience in Technical Specification Documentation\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "373_Azure Data Architect - Hybrid": "Nadeem Ahmed Razvi,\nMsys Inc\nresume@msysinc.com\nReply to: resume@msysinc.com\nTitle: Azure Data Architect - HybridLocation: Princeton, NJ, USA/New York City, NYLength: Long termRestriction: W2 or C2CSend resume to: nadeem@msysinc.com Description:**** Webcam interview; *** Long term project *** Hybrid ****Job Description:Key ResponsibilitiesDesign & Develop the ETLGood experience in writing SQL, Python and Pyspark programmingCreate the Pipelines (simple and complex) using ADF.Work with other Azure stack modules like Azure Data Lakes, SQL DWMust be extremely well versed with handling large volume of data.Understand the business requirements for Data flow process needs.Understand requirements, functional and technical specification documents.Development of mapping document and transformation business rules as per scope and requirements/Source to target.Responsible for continuous formal and informal communication on project statusGood understanding of JIRA stories process for SQL development activitiesRequired Skills:Overall, 4+ years of developer skills with SQL, Python with Spark (Pyspark)Experience in Azure Data Factory, Data Sets, Data Frame, Azure Blob & Storage ExplorerImplement data ingestion pipelines from multiple data sources using ADF, ADB (Azure data bricks)Experience in creating Data Factory Pipelines, custom Azure development, deployment, troubleshoot data load / extraction using ADF.Extensive experience on SQL, python, Pyspark in Azure databricks.Able to write Python code in Pyspark frame by using Dataframes.Have good understanding on Agile/Scrum methodologies.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "374_Running Interview | AWS LEAD (DevOps exp 12yrs to 14yrs) San Antonio, TX (Onsite) | Contract to Hire": "tyson,\nsydatainc\ntyson@sydatainc.com\nReply to: tyson@sydatainc.com\nID: 11124874510AWS LEAD (DevOps exp 12yrs to 14yrs)San Antonio, TX (Onsite) Contract to HireClient: HCLRate: $65/hr. on C2CPlease Note: For H1b, H4EAD, GC-EAD candidates Need Passport Number, LinkedIn is mandatory Mandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, Elatics SearchOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Required Skills AWS Certifications (Please confirm the detailsAWS Services AWS Services-Lambda AWS EC2 AWS S3 Bucket AWS EKS Terraform IAAS Dynamo DB SQS/SNS API integration AWS services (S3 Bucket, RDS , Lambda function, AWS EKS, EC2) SQS, Dyanamo DB, Terraform SNS/ SQS, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, Kibana/ Grafana/ Prometheus / Elastic Search. Job Description:\u00b7 Participate in producing conceptual, solution and component-level architectures and associated artifacts.\u00b7 Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.\u00b7 Develop common components for shared business capabilities with Standards and best practices. \u00b7 Develop the transition architecture using services offered by AWS and private cloud.\u00b7 Migrate legacy applications to AWS and private cloud. \u00b7 Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7 Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7 Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7 Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7 Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7 Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7 Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7 Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.\u00b7 Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.\u00b7 Ability to design patterns for moving legacy applications to AWS and private cloud.\u00b7 Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)\u00b7 Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)\u00b7 Experience with SAFe framework or other similar agile frameworks.\u00b7 Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.\u00b7 Guidewire experience is big plus. Regards,Tyson MSr. IT RecruiterSydata Inc.Email: tyson@sydatainc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "375_AWS Cloud Native Developer": "Rizwan Khan,\nThoughtwave software and Solutions Inc\nrizwan.k@thoughtwavesoft.com\nReply to: rizwan.k@thoughtwavesoft.com\nHi All,Greetings from Rizwan and Team Thoughtwave Software and Solutions,Please find below the exciting Job opening with our esteem client with details Job description and if you are looking for a new Job opening here is the position and you respond with the latest/updated resume to email enclosed or feel free call us.Client: GTA AWS CLOUD NATIVE DEVELOPER (742293)Job title: AWS CLOUD NATIVE DEVELOPERDuration : 12months contract- Need local to GALocation: Atlanta, GA- Hybrid role \u2013 Web Cam Interview Complete Description: Project Overview: The GTA CJEP project is a modernization of the Criminal Justice Exchange Program that shares data between different agencies, counties, cities, etc. This project will take the current SoftwareAG centric solution and produce a Cloud Native solution in AWS to move and consolidate data for counties that sign-up for the service. The touchpoints for this data sharing will be third party vendors as well as state and county systems. Current API\u2019s and web services will be leveraged to facilitate this modernized solution as much as possible.Position Overview: As an AWS Cloud Developer at GTA you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platform. You will collaborate closely with cross-functional teams in a SCRUM Agile environment to deliver high-quality software solutions that meet business objectives. The ideal candidate will have extensive experience with SOAP-based web services,custom header implementation, and handling MTOM (Message Transmission Optimization Mechanism) attachmentsKey Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills.Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes). SkillRequired / DesiredAmountof ExperienceProven experience as a software developer with a strong understanding of cloud computing principles and practicesRequired8Years Work experience designing, developing, and deploying cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3Required5YearsWork experience Implementing serverless architectures using AWS Lambda functions with PythonRequired5Years Work experience building and orchestrate workflows using AWS Step Functions and AWS State MachinesRequired5YearsWork experience designing, developing, and implementing SOAP-based web services using services technologiesRequired5Years Work experience with XML, XSD, WSDL, and other related technologiesRequired5YearsAgile methodologies and SCRUM frameworkRequired3Years Experience with CI/CD pipelines and DevOps practicesHighly desired Experience with containerization (e.g., Docker) and orchestration (e.g., Kubernetes)Highly desired AWS certifications (e.g., AWS Certified Developer)Highly desired Thanks and Regards Rizwan Khan| SR US IT Recruiter | Thoughtwave Software and Solutions Inc314 N. Lake St, Suite 6, Aurora IL 60506Email: Rizwan.k@thoughtwavesoft.comDirect: 630-491-8638 Ext # 155 | Fax: 630-689-5746LinkedIn: https://www.linkedin.com/in/rizwan-aquib-khan-aa6384176/Web: http://www.thoughtwavesoft.comA Certified Minority Business Enterprise, Disadvantaged Business Enterprise, SAM.gov, SOC2 & ISO2005,Vendors for:\u00b7 STATES: IL, PA, TN, AK, OR, CT, GA, VA, ID, IA, UT, FL, MN & CO,\u00b7 NATIONAL LABS: ARGONNE & FERMI,\u00b7 COUNTIES: HENNEPIN, MN, FULTON & GA,\u00b7 PUBLIC SCHOOLS: ATLANTA.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "376_Active Directory (Azure Security Engineer)   :: Contract :: Quincy, MA OR NJ (Day 1 onsite)": "Jaya Verma,\nGtech llc\njverma@greattechglobal.com\nReply to: jverma@greattechglobal.com\nRole : Active Directory (Azure Security Engineer)Location : Quincy, MA OR NJ (Day 1 onsite) Job Type : Contract Job Description \u2013 We are seeking a highly skilled Azure Security Engineer with expertise in security products, authentication, authorization, and identity and access management (IAM). As a key member of our security team, you will play a vital role in ensuring the secure and compliant implementation of Azure AD solutions, with a focus on IAM, MFA, and SSO.Responsibilities:1. Design, implement, and manage Azure Active Directory solutions to ensure secure and efficient authentication and authorization processes aligned with industry best practices2. Drive the onboarding of applications, app registration, enterprise application setup, and role-based access management (RBAC).3. Lead the implementation of Multi-Factor Authentication (MFA) and Single Sign-On (SSO) for enhanced security.4. Expertise in configuring and troubleshooting authentication protocols, including OAuth, OpenID Connect, and SAML for secure authentication and authorization5. Configure and manage conditional access policies to control access based on specific conditions, locations, and device compliance6. Collaborate with cross-functional teams to support and troubleshoot IAM-related issues, ensuring solutions are secure, compliant, and scalable.7. Understand and implement security best practices for Azure products, services, and solutions.8. Utilize Azure Sentinel for monitoring, creating alerts, and developing automation scripts for incident response.9. Provide production support, responding to and resolving security incidents in a timely manner.10. Establish and maintain identity governance frameworks, including privileged identity management (PIM) for elevated access11. Stay informed of Azure updates, security threats, and industry best practices to enhance our security posture.12. Collaborate with DevOps and development teams, demonstrating a basic understanding of tools and requirements. Qualifications:1. Proven experience in implementing security solutions on Azure, with a focus on IAM, MFA, and SSO.2. In-depth knowledge of Azure AD, Azure AD B2C, related authentication/authorization components and security protocols which including SAML, OAuth, and OpenID3. Strong scripting and automation skills (PowerShell, Azure CLI)4. Excellent understanding of cloud security principles5. Microsoft Certified: Azure Security Engineer Associate certification is a plus.6. Experience with Azure Sentinel for monitoring, alerting, and automation.7. Strong troubleshooting skills for identifying and resolving IAM-related issues.8. Ability to work in a dynamic environment and adapt to evolving security challenges.9. Excellent communication and collaboration skills for working with cross-functional teams.10. Commitment to maintaining a secure, compliant, and scalable IAM solution Thanks and Regards,GTECH LLCJaya Verma | Senior Technical Recuriter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "377_Urgent requirement for Azure Architect with DevOps at Chicago, IL- Day 1 Onsite": "vasu,\nMagic force\nvasu@magicforce.us\nReply to: vasu@magicforce.us\nHi, Greetings from Magic Force\u2026!!!My name is Vasu we have a job opportunity for you as a Azure Architect with DevOps one of our client based at Chicago, IL- Day 1 Onsite. Please find the Job description below, if you are available and interested, please send us your word copy of your resume with following detail to vasu@magicforce.us to discuss more about this position. Job Title: Azure Architect with DevOpsLocation: Chicago, IL- Day 1 OnsiteDuration: 1+ YearStart Date: ASAP Job Description: This role will be responsible with, architecting, implementing, and supporting enterprise-wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelor's degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Preferred Qualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure Administrator).- Experience with other cloud platforms (AWS, Google Cloud).- Familiarity with containerization and orchestration tools like Docker and Kubernetes Preferred tool experience:- Microsoft Azure DevOps, JIRA, BitBuket, Bamboo, ALM, Load Runner/Performance Center, JMeter, GitHub, Jenkins, Microfocus UFT/LeanFT, Selenium, Eclipse, ServiceNow, Perfecto, SeaLights, Gremlin, ToxiProxy, ChaosMonkey, Selenium Grid. If you are interested in this position, kindly fill the details and revert back me with Any ID Proof Copy. Requirement id / Title Full Name Work Authorization and validity Present location Passport Number Contact Number E-mail Address DOB LinkedIn ID Last 4 digits Of SSN Currently on the project (Yes / No) Availability to join the project Availability for onsite / remote Overall, USA IT experience Overall relevant IT experience Bill rate Year of arrival in USA and the employer\u2019s name USA working experience mentioned in the resume is correct? Highest degree and passing year Is the candidate submitted for the same client earlier?\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "378_AWS Developer - Erie, PA (Onsite) - Contract": "Divya Sai,\niTech US Inc\ndivya.c@itechus.net\nReply to: divya.c@itechus.net\nHelloThis is Divya Sai from iTech US, we have an urgent Job opening from one our client, please go through below JD if you are interested, please share me your updated resume.Role: AWS DeveloperLocation: Erie, PA (Onsite)Type : ContractDescription:DeveloperAWS developer - build code around AWS services, test, deploy code and maintain code.Digital : Amazon Web Service(AWS) Cloud Computing, Software Engineering8-10AWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for aws services. Aurora, Amazon kafka, S3, EMRAWS developer, Aurora, AWS kafka, EMR, REST APIThanks & RegardsDivya Sai ChanduiTech US Inc,Email:divya.c@itechus.netPh:18028510921 Ext:308\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "379_Urgently looking for Cloud Security Architect  in Reston VA (Hybrid Role)": "Anubhav,\nHMG America\nanubhav@hmgamerica.com\nReply to: anubhav@hmgamerica.com\nOne of our direct clients is looking for Cloud Security Architect in Reston VA. Below is the detailed job description.Role:- Cloud Security Architect Rate- $80/hr. C2C MaxLocation-Reston VA Day 1 onsite in hybrid set up.NOTE:- Need strong Cloud (AWS only) security and architecture experience.Job Description \u2013The Cloud Security Architect (CSA) will leverage broad technical knowledge of cloud security best practices of key public cloud offerings of providers such AWS, Azure, and GCP to establish secure design patterns, to architect integrations among cloud and/or on-premises infrastructures. This individual must be able to assist in ensuring the security and compliance of the cloud environment based on enterprise cloud security policies, standards, and procedures. The CSA will ensure that solutions operating on the cloud comply with enterprise security requirements in both off-premises and hybrid environment models. The position will work with Enterprise Architects and Application Dev teams to come up with Security Architecture for applications and enterprise tech capabilities migrating to Cloud. Must-Haves: Required qualifications to be successful in this role: 10 years of total IT experience with the following must haves: \u2022 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect \u2022 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u2022 Excellent written and communication skills to report, document and communicate security architecture \u2022 Excellent coordination skills and must be detail oriented Nice-to-Haves: \u2022 Cloud agnostic security architecture experience a plus \u2022 1-2 years of experience in working with NIST assessments of business applications \u2022 Container Security experience to protect container workloads during build and run-time \u2022 API Security architecture experience with industry standard API Gateways \u2022 Security engineering/administration background leveraging SIEM, Network firewalls, host-based security, and security configuration \u2022 One or more industry standard security certification such as CISSP, CCSP or relevant GIAC certifications (ANY) \u2022 One or more Cloud Service Providers Security Specialty Certifications such as AWS Security Specialty or Azure AZ-500 Certification \u2022 The group of skills related to Security including designing and evaluating security systems, identifying security threats, securing computers, assessing vulnerability, etc. \u2022 The group of skills related to Relationship Management including managing and engaging stakeholders, customers, and vendors, building relationship networks, contracting, etc. \u2022 Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand \u2022 The group of skills related to Risk Assessment and Management including evaluating and designing controls, conducting impact assessments, identifying control gaps, remediating risk, etc. \u2022 Experience identifying and determining levels of risk to an organization's networks and systems using cybersecurity techniques \u2022 Working with people with different functional expertise respectfully and cooperatively to work toward a common goal \u2022 Skilled in cloud technologies and cloud computing \u2022 The group of skills related to Influencing including negotiating, persuading others, facilitating meetings, and resolving conflict Key Areas of Responsibility: \u2022 Partner with Enterprise/Portfolio Architecture team and Business Units development squads to collaboratively develop security architectures/designs leveraging approved patterns that ensure applications migrating from on-premise to Cloud, achieving high standards of security practices and compliance. \u2022 Drive the development and adoption of cloud security standards, best practices, and technologies within Enterprise IT infrastructure \u2022 Liaise on security-related issues with internal business stakeholders, InfoSec, Enterprise Architecture, and application development squads \u2022 Work to develop, enhance and document security architecture, security policies, patterns, procedures, guidelines and standards required to design cloud-based solutions \u2022 Educate application, portfolio and solution architects on secure solution design and industry best security practices \u2022 Work on assessments of compliance and standards including and not limited to NIST, FedRAMP, FIPS, etc. \u2022 Support threat modeling and update application security architecture as needed. \u2022 Support application development squads with Security implementations and issues Thanks & Regards,Anubhav Jain IT RecruiterHMG America LLCanubhav@hmgamerica.comhttp://www.hmgamerica.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "380_Lead Azure Dot Net Developer": "Anusha,\nHanker Systems Inc\nanushaj@hankersystems.com\nReply to: anushaj@hankersystems.com\nLead Azure /Dot Net Developer Richmond, VA 232196+ Monthsin Person- Interview SkillsAmount UsedLast UsedExperience with ASP.NET, MVC, C#,WEB API Experience with Angular, Blazor or similar front end technologies Experience with .NET Core Experience with Microservices architecture, Azure architecture, Domain driven architecture Experience with Docker and azure cloud services such as AKS, Azure functions, Azure Container Service, Azure App services, Azure API Management Experience with Azure cloud databases Experience with Azure Monitoring, Application Inisghts Experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM Templates Azure certified developer Experience with Infrastructure As Code Experience with Project Online Rest API's Strong scripting skills in Python Strong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtual networks, VPN, and Express Route Experience working with Azure Data factory, Azure Data Lake. Experience developing power BI reports Job Description:VDOT ITD is looking for an Azure Developer with Dot Net Experience who will play a pivotal role in designing, implementing, and optimizing data solutions within the Microsoft Azure cloud ecosystem.Able to design and plan cloud infrastructure, including computing resources, storage, networking, and security components. Collaborate with other teams to design cloud-native applications and migrate existing applications to the cloud and Implement security measures to safeguard data and applications in the cloud.Minimum Qualification- Hands-on experience in design, development and testing .NET solutions (.NET Core, MVC.NET, ASP.NET,C#, Web Services (WCF, Web API), JQuery, T-SQL, PL/SQL, etc.) within both on-premises and cloudenvironments.- Hands-on experience with Angular / Blazor or any similar front-end technologies.- Hands-on experience with project online API's- Experience with Microservices architecture, Azure Architecture, Domain driven architecture.- Experience with Azure native application development, Azure App Services, Azure Kubernetes Service, Azure Container instances, Kubernetes & Containers (Docker), Azure Integration Services (Logic Apps, API Management, Service Bus & Event Grid) ,Azure SQL Database, Azure Web Jobs, SQL Server IaaS, Azure Monitoring and Application Insights- Knowledge and experience with configuration management and automation technologies such as Azure Devops, PowerShell, Terraform, Chef, and ARM Templates- Collaborate with architects and other senior developers to define software architecture, making well-informed design decisions that align with VDOT ITD business needs- Experience working in agile methodology.Preferred Qualification- Strong knowledge of implementing hybrid connectivity between Azure and on-premise systems using virtualnetworks, VPN, and Express Route- Strong scripting skills in Python- Experience working with Azure Data factory, Azure Data Lake.- Experience developing power BI reports\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "381_Title : Sr Cloud Network Engineer: Onsite": "Amol,\nChabeztech\namol@chabeztech.com\nReply to: amol@chabeztech.com\nTitle : Sr Cloud Network Engineer: OnsiteLocation can work from Onsite from any of the locations \u2013 Hybrid 3 days Alpharetta GAFrisco TXKansas City KansasBellevue WABelow is the Job Description Key ResponsibilitiesOversee the network onboarding process for new users and systems into Cloud environmentProvision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.Conduct regular network assessments and audits to ensure compliance with internal and external requirements.Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.Provide technical support and troubleshooting for Cloud network-related issuesStay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms QualificationsExperience with network onboarding and provisioning in cloud environments.Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, MacieIn-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.Familiarity with security regulations, standards, and compliance requirements for cloud network environments.Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.Strong communication and documentation skills for collaborating with cross-functional teams.Experience with automation tools (e.g., Python, Terraform) for network configuration and managementRelevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "382_AWS Cloud Developer": "priya,\nAnveta INC\npriya@anveta.com\nReply to: priya@anveta.com\nHello Everyone, AWS CLOUD NATIVE DEVELOPERLocation: Atlanta, GA 30334 (Hybrid)Duration: 6 +Months Key Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team. Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills. Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes). Please Share the Profile to priya@anveta.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "383_Cloud infrastructure security Technical Architect": "narra venkatesh,\nprospectinfosystem\nvenkatesh@prospectinfosystem.com\nReply to: venkatesh@prospectinfosystem.com\nHi We have an Immediate Position of Technical Architect Madison, WI Need Locals OR willing to relocate. please let me know Interest Technical Architect 4 positionsLocation: Madison, WI Need Locals OR willing to relocate Top Skills: 5+ years of experience in the following: Experience with multiple Cloud infrastructure in AWS & AzureExperience providing technical direction/leadership to a team of developers and engineers Complex web, data, and middleware software architecture and developmentKnowledge, Skills & Abilities1. Proven experience as a Technical Architect or similar role.2. Strong knowledge of cloud platforms (AWS, Azure, GCP) and associated services.3. Familiarity with DAM systems, integration patterns, and SSO protocols.4. Excellent communication and collaboration skills.5. Experience as a Windows System Administrator in an enterprise environment.6. Strong knowledge of Windows Server operating systems and Active Directory.7. Experience with M365 products, including SharePoint, Teams, and Power Platform.8. Strong understanding of network and security principles, including firewallconfigurations and access controls.9. Excellent problem-solving and troubleshooting skills.10. Ability to work independently and lead a team, with outstanding communication andcollaboration skills.11. Strong organizational and project management skills.12. Knowledge of Project management methodologies with effective results focus within aninformation systems environment13. Expertise in C#, .NET Core, and the .NET ecosystem as a whole14. Knowledge of object oriented design principles such as SOLID or GRASP15. Familiarity with multiple types of software architecture (Clean architecture, DDD, TDD,Vertical slice architecture)16. Comfortable working in legacy codebases17. Comfortable with version control and automated testing and deployment frameworks(CI/CD)18. Knowledge of industry best practices and a commitment to implementing them.19. Knowledge of Cloud platforms such as AWS and Azure.20. Scripting and automation skills (e.g., PowerShell, Python).21. Familiarity with security principles, including encryption, authentication, andauthorization.22. Ability to conduct research into systems issues and products as required.23. Ability to communicate ideas in both technical and user-friendly language.24. Ability to effectively prioritize and execute tasks in a high-pressure environment.25. Ability to quickly master new technologies as needed.26. Experience providing guidance and leadership to novice systems engineers27. Excellent written and oral communication skills28. Strong analytical and creative problem-solving abilities\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "384_Devops Specialist GCP || Charlotte NC || ONLY H1B OR H4EAD WITH PASSPORT NUMBER": "adarsh,\nexecutive technocrats\nadarsh@executivetechnocrats.com\nReply to: adarsh@executivetechnocrats.com\nRole: GCP Devops SpecialistLocation: Charlotte NC - Onsite ONLY H1B OR H4EAD WITH PASSPORT NUMBER Job Description: In this role, you will:\u2022 Serve as a senior-level GCP Cloud networking Subject Matter Expert (SME) for high visibility cloud computing initiatives.\u2022 Design and influence cloud solutions and services in alignment with a product model to support the adoption of public cloud across the enterprise while ensuring alignment with architecture, security, and other key technology frameworks.\u2022 Seek out opportunities for continuous improvement of our engineering practices and services and drive the implementation of these improvements across the respective engineering teams.\u2022 Mentor junior engineers and promote secure development practices and test-driven development.\u2022 Utilize analytical and problem-solving skills to define problems, proactively innovate, provide credible challenge, and translate vision into documented strategic solutions.\u2022 Work closely with the GCP Networking Product Owner to define a body of work to be picked up by more junior engineers.\u2022 Leverage the agility and scalability of RexSecQps and automated governance utilizing secure Infrastructure as Code principles; proactively partner to update CI/CD pipelines based on trends identified in the environment.\u2022 Support and partner with key stakeholders including product owners, engineering managers, risk partners, and on-prem networking teams to design compliant and cost-effective solutions.\u2022 Proactively prioritize, analyze, and address complex problems and incidents to ensure the highest level of environment stability and availability.Required Qualifications, US:\u2022 7+ years of engineering experience; or 7+ years of experience in a technical management role.\u2022 5+ years of experience building, deploying, and securing public cloud platforms (GCP, AWS, Azure, etc.).\u2022 5+ years of experience as an engineer in the network domain.\u2022 Ability to manage initiatives related to the following: architecture roadmap that aligns with business strategy and the broader architecture community; hardware and capacity planning, consolidation and rationalization; related emerging technology assessment and incubation.\u2022 Strong verbal, written, and interpersonal communication skills.Job Expectations:\u2022 Willingness to work on-site at the stated location on the job opening.\u2022 Ability to travel up to 5% of the time.E-mail Is the best way to reach.Thanks & RegardsAdarsh Tyagi || Talent AcquisitionExecutive Technocrats || Princeton, NJ 08540Adarsh@executivetechnocrats.com || LinkedInAn E-verify Employer\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "385_AWS Architect   Onsite-San Antonio, TX": "Asim,\nIntellicept\naahamed@intellicept.com\nReply to: aahamed@intellicept.com\nAWS Architect Location: San Antonio, TX Mandatory SkillsetOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns. Job Description:\u00b7Participate in producing conceptual, solution and component-level architectures and associated artifacts.\u00b7Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations.\u00b7Develop common components for shared business capabilities with Standards and best practices. \u00b7Develop the transition architecture using services offered by AWS and private cloud.\u00b7Migrate legacy applications to AWS and private cloud. \u00b7Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.\u00b7Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.\u00b7Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and Terraform\u00b7Strong hands-on experience in one or more development languages including Java, python, Golang.\u00b7Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.\u00b7Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.\u00b7Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.\u00b7Experience in explaining complex technology decisions and developing high trust relationships with stakeholders.\u00b7Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture.\u00b7Ability to design patterns for moving legacy applications to AWS and private cloud.\u00b7Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.)\u00b7Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.)\u00b7Experience with SAFe framework or other similar agile frameworks.\u00b7Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others.\u00b7Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "386_Cloud Architect Madison, WI- Hybrid": "sunil kumar,\npransu tech solutions\nsunil@pransutechsolutions.com\nReply to: sunil@pransutechsolutions.com\nRole: Cloud ArchitectLocation: Madison, WI- HybridVisa: Except OPT , CPT, H1B Need candidate with 3 references and also nearby to the client location.Top skills and years of experience: 5+ years of experience in the following:\u00b7 Experience with multiple Cloud infrastructure in AWS & Azure\u00b7 Experience providing technical direction/leadership to a team of developers and engineers\u00b7 Complex web, data, and middleware software architecture and developmentNice to have skills:\u00b7 Experience implementing multiple SSO technologies (Entra ID, Okta) with SaaS and on-prem systems\u00b7 Experience with Digital Asset Management Systems\u00b7 Strategic planning\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "387_Snowflake Architect with AWS Experience--REMOTE-16+ years": "sharon,\nNorthpole\nsharon@npoleinc.com\nReply to: sharon@npoleinc.com\nHi,please let me know if your interested Role: Snowflake Architect with AWS ExperienceLocation: REMOTEContract JD:Looking for Snowflake architect with AWS Experience Regards,SharonSenior US IT Recruiter (O): 512.999.7446Sharon@npoleinc.com | www.npoleinc.com 3600 Brushy Creek Rd, #12, Cedar Park, TX,78613A Certified Minority-Owned Business(MBE)/Women-Owned Business (WBE) Enterprise\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "388_Senior Devops Engineer - Remote - Direct Client - 15+ years profile": "Megha,\nInfologitech\nmveeranki@infologitech.com\nReply to: mveeranki@infologitech.com\nHiGreetings from InfoLogitech, Inc.Here is our open requirement which can be filled immediately. Kindly respond to this requirement with your resume, contact and current location info to speed up the interview process. Job Position : Senior DevOps EngineerJob Location : RemoteJob Duration : 1 Year Job Description : Key skills : AWS, Terraform, Kubernetes, Jenkins, CI/CDThanks & Regards, MeghaUS IT Recruiter | InfoLogitech, Inc.3 Independence Way, Suite 117, Princeton, NJ - 08540.Direct 609-212-0605Email: mveeranki@infologitech.com | www.infologitech.comInfologitech \u2013 Technology Company\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "389_Azure Data Engineer, Onsite Day1, Weehawken NJ": "Sunita Sharma,\nTech Source Inc\nsunita@tsourceinc.net\nReply to: sunita@tsourceinc.net\nAzure Data AnalystLocation:-Weehawken NJOnsite Role from Day1, KRA'sManage and analyze large datasets using Azure data services to inform business decisions. Build and maintain data pipeline architecture within Azure. Assemble complex data sets that meet functional business requirements. Identify, design, and implement internal process improvements.responsible for collecting, analyzing, and visualizing data from various sources using Azure services. They work with large amounts of data to identify trends, patterns, and insights that can help businesses make informed decisions.dissect complex data sets, identify patterns, and derive insights that can drive business decisions.analyze and interpret data within the Microsoft Azure cloud ecosystem\u2022Responsible for designing, building, and maintaining scalable data pipelines and infrastructure\u2022Hands on with Microsoft Azure Databricks, Microsoft Azure Data Factory, Microsoft Azure SQL, Microsoft Azure Cloud Services, Azure Data Lake ADLS Azure Data Lake Storage\u2022Day to day activities will include collaborating with cross functional teams, analyzing data requirements, and implementing data solutions using your expertise in Data Engineering\u2022Design and develop scalable data pipelines and infrastructure using Data Engineering expertise\u2022Ensure data quality and integrity by implementing data governance and best practices\u2022Optimize data processing and storage for performance and cost efficiency\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "390_Azure Data Architect - Hybrid": "Nadeem Ahmed Razvi,\nMsys Inc\nresume@msysinc.com\nReply to: resume@msysinc.com\nTitle: Azure Data Architect - HybridLocation: Princeton, NJ, USA/New York City, NYLength: Long termRestriction: W2 or C2CSend resume to: nadeem@msysinc.com Description:**** Webcam interview; *** Long term project *** Hybrid ****Job Description:Key ResponsibilitiesDesign & Develop the ETLGood experience in writing SQL, Python and Pyspark programmingCreate the Pipelines (simple and complex) using ADF.Work with other Azure stack modules like Azure Data Lakes, SQL DWMust be extremely well versed with handling large volume of data.Understand the business requirements for Data flow process needs.Understand requirements, functional and technical specification documents.Development of mapping document and transformation business rules as per scope and requirements/Source to target.Responsible for continuous formal and informal communication on project statusGood understanding of JIRA stories process for SQL development activitiesRequired Skills:Overall, 4+ years of developer skills with SQL, Python with Spark (Pyspark)Experience in Azure Data Factory, Data Sets, Data Frame, Azure Blob & Storage ExplorerImplement data ingestion pipelines from multiple data sources using ADF, ADB (Azure data bricks)Experience in creating Data Factory Pipelines, custom Azure development, deployment, troubleshoot data load / extraction using ADF.Extensive experience on SQL, python, Pyspark in Azure databricks.Able to write Python code in Pyspark frame by using Dataframes.Have good understanding on Agile/Scrum methodologies.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "391_Azure devops or SRE :: Chicago(Downtown), IL": "Shikha singh,\nkk software associates\nshikha.s@kksoftwareassociates.com\nReply to: shikha.s@kksoftwareassociates.com\nRole name: EngineerLocation: Chicago(Downtown), IL (Only local candidates)Experience (Years):10 & Above Role Description: This role will be Responsible for application observability, maintenance, and support, identifying and implementing preventive measures proactively, evaluates and makes recommendation on techniques, practices, or technologies that would enhance business needs. As a SRE associate you will collaborate with Application Support and Development teams to implement business solution through agile practice and manage production issues. The ideal candidates will possess excellent leadership and communication skills coupled with a solid understanding of modern cloud technologies preferably in the financial sector. Principal Responsibilities: Lead production stability effort by preventing production issue and improve production stability.Defining and enforcing Service Level Objectives (SLOs) and Agreements (SLAs), Error Budgets to guarantee system reliability and availability. Attention to key performance indicators, such as response times, error rates, and uptime to align operational performance with overarching business objectives. Proactively identify continuous improvement opportunities such as reducing manual effort, automation of tasks/resolutions or decreasing production incidents. Involving in defining and deploying monitoring, metrics, and logging systems and developing application dashboards. Ensure near-zero downtime with monitoring and alerting, self-healing automation, and continuous improvement. Provide reactive, break-fix support and Communicate issue/resolution status (written and verbal) to project team and management. Develop to become a Subject Matter Expert for assigned application domain.Should be able to interpret the alerts like SiteScope, Dynatrace and ELK etc. & refer to it while doing the RCA of the issue.He Should be flexible for upskilling to new tech stack & should be ready to do hands on development. Provide regular and high-quality updates to all the stakeholders on the progress of the work including user stories and ITSM problems. Attend regular meetings with Project/Development teams to discuss production issues for prioritization, fixes, and release. Skills / Experience: \u2022 5-6 plus years of application development experience using modern technologies and architecture, including experience collaborating with technology teams. \u2022 2 plus years of Site Reliability Engineering experience. \u2022 Good Understanding of at least one public cloud, preferably Microsoft Azure/ Pivotal Cloud Foundry. \u2022 Strong understanding of REST APIs and how to use them in practice. \u2022 Strong Experience with continuous integration and collaboration tools like Azure DevOps, JIRA, Bitbucket, GitHub and Confluence. \u2022 Good knowledge and Hands on CLI \u2013 Bash, Linux, Azure CLI etc., \u2022 Experience in some of the following technologies: Java, J2EE, Pivotal Cloud Foundry, Cloud Computing (IaaS, PaaS, and SaaS), RESTful interfaces, GIT, Gradle, Maven, NPM, Spring (Spring Batch and Spring Boot), CSS3, HTML4, React.js, Node.js/JavaScript, Essential Skills: This role will be Responsible for application observability, maintenance, and support, identifying and implementing preventive measures proactively, evaluates and makes recommendation on techniques, practices, or technologies that would enhance business needs. As a SRE associate you will collaborate with Application Support and Development teams to implement business solution through agile practice and manage production issues. The ideal candidates will possess excellent leadership and communication skills coupled with a solid understanding of modern cloud technologies preferably in the financial sector.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "392_Urgent Opening for Aws Solution Architect with BSS in Dallas, TX (ONSITE)": "Bharani,\nVy Systems\nbharani@vysystems.com\nReply to: bharani@vysystems.com\nAs a Solutions Architect should have ability to model efficient architectures and evaluate relevant technologiesFamiliarity with common EA frameworks (e.g., TOGAF)Analyze existing architectures and make recommendations for improvement and collaborate with multiple Architects in organization( Frontend Architects, Security Architects etc) and come up with best Architecture and design.Design and document new architectures, taking into account scalability, reliability, security, and performanceIdentify opportunities for technology enhancements and improvementsResearch and evaluate emerging technologies and their potential applicationsWork with departmental leads to ensure alignment between business and technology strategies.Should have good knowledge in Telecom BSS/OSS domain(end to end integration).In depth knowledge for customer onboarding through Sales buyflow, order fulfilment, billing, payment, Supply chain management, Network provisioning and Product catalog.Should be able to work with multiple business teams, stake holders, Partners( Amazon, Target, Wallmart, Apple etc).Sound knowledge in Distributed systems and we'll familiar with Microservices design patterns and design principles.Good knowledge in PCRF, discount, quotation, refund and charging for Telecom customers.AWS Solution Architect certified with good knowledge in Security, firewall, authentication, Api gateway, Lambda, SQS, SNS, Kafka and ECS, EC2.Worked on enterprise architecture tools( Lucid chart, Draw.io, figma) in the organization and/or with enterprise architecture tools of clients and takes into account how the owner/client and stakeholder react on it\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "393_Hiring  AWS CLOUD NATIVE DEVELOPER at Richmond, VA (only locals)": "srinath,\nCalabitek\nsrinath@calabitek.com\nReply to: srinath@calabitek.com\nGTA AWS CLOUD NATIVE DEVELOPER (742293) Job DetailsJob Title:AWS CLOUD NATIVE DEVELOPERJob Location:Richmond, VAJob Duration:6 MonthsWork Authorization:Any VisaEmployment Type:ContractClient:State of VARate:65.00 per hour on C2CInterview ModeBoth Web CamWork Arrangement:HybridProject Overview: The GTA CJEP project is a modernization of the Criminal Justice Exchange Program that shares data between different agencies, counties, cities, etc. This project will take the current SoftwareAG centric solution and produce a Cloud Native solution in AWS to move and consolidate data for counties that sign-up for the service. The touchpoints for this data sharing will be third party vendors as well as state and county systems. Current API\u2019s and web services will be leveraged to facilitate this modernized solution as much as possible.Position Overview: As an AWS Cloud Developer at GTA you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platform. You will collaborate closely with cross-functional teams in a SCRUM Agile environment to deliver high-quality software solutions that meet business objectives. The ideal candidate will have extensive experience with SOAP-based web services,custom header implementation, and handling MTOM (Message Transmission Optimization Mechanism) attachmentsKey Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills.Preferred Qualifications:AWS certifications (e.g., AWS Certified Developer) are a plus.Experience with CI/CD pipelines and DevOps practices.Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "394_Snowflake Architect with AWS Experience--REMOTE-16+ years": "sharon,\nNorthpole\nsharon@npoleinc.com\nReply to: sharon@npoleinc.com\nHi,please let me know if your interested Role: Snowflake Architect with AWS ExperienceLocation: REMOTEContract JD:Looking for Snowflake architect with AWS Experience Regards,SharonSenior US IT Recruiter (O): 512.999.7446Sharon@npoleinc.com | www.npoleinc.com 3600 Brushy Creek Rd, #12, Cedar Park, TX,78613A Certified Minority-Owned Business(MBE)/Women-Owned Business (WBE) Enterprise\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "395_Need Local only  Application Architect ( LLM + Azure) - Newport Beach, CA.": "Sofia,\niAgami LLC\nsofia@iagami.com\nReply to: sofia@iagami.com\nHi,I would like to touch base with you regarding a job opportunity, appreciate it if you could go through the below job details and share your consultant updated Resume. Application ArchitectLocation: Anywhere in Bay area ( preferred Newport Beach, CA ) Duration - Long-termSkills:Advanced Python programmingLarge-scale implementation of Large Language Models (LLMs) in production environmentsExperience in application development and API optimizationKnowledge of databases relevant to LLMsStrong understanding of LLMs and transformer modelsProficiency with Azure Cloud services, including deployment and management of cloud-based applicationsFamiliarity with containerization and orchestration tools (e.g., Docker, Kubernetes)Experience with CI/CD pipelines and automated testing frameworksExcellent problem-solving skills and ability to tackle complex challengesStrong communication and teamwork skillsBest Regards,Sofiasofia@iagami.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "396_Need AWS WAF - Security engineer  || Atlanta, GA|| Day-1 Onsite": "Pavan,\nSoftcom Systems Inc\npavan@softcomsystems.com\nReply to: pavan@softcomsystems.com\nHi, Iam Pavan from Softcom Systems Kindly respond to this requirement with updated profile Details : Role: AWS WAF - Security engineer Location: Atlanta, GA|| Day-1 Onsiteclient: InfosysType : Contract Detailed Job Description: Skills: AWS WAF Key Responsibilities:\u2022 Design, implement, and manage AWS WAF to protect web applications from security threats.\u2022 Develop and maintain automation scripts for WAF deployment and management.\u2022 Collaborate with the security team to integrate AWS WAF with other security tools and services.\u2022 Monitor and respond to security incidents related to web application threats.\u2022 Create and manage Infrastructure as Code (IaC) templates for AWS WAF and related services.\u2022 Conduct regular security assessments and audits of web applications.\u2022 Provide technical support and troubleshooting for AWS WAF-related issues.\u2022 Stay up-to-date with the latest security trends, threats, and technologies. Requirements:\u2022 Proven experience with AWS Web Application Firewall (WAF).\u2022 Proficiency in scripting languages such as Python, Bash, or PowerShell.\u2022 Experience with Infrastructure as Code (IaC) tools like AWS CloudFormation, Terraform, or Ansible.\u2022 Strong understanding of web application security principles and best practices.\u2022 Familiarity with AWS services and architecture.\u2022 Experience with continuous integration and continuous deployment (CI/CD) pipelines.\u2022 Excellent problem-solving skills and attention to detail.\u2022 Strong communication and teamwork abilities.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "397_Need AWS Python with Golang, Plano, TX (Need Locals Only)": "Srinath,\nIT Strategies\nsrinath@itstrategiesinc.com\nReply to: srinath@itstrategiesinc.com\nPosition: AWS Python with GolangLocation: Plano, TXDuration: Long term Contract\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "398_Azure Technical Architect": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Azure Technical ArchitectAzure IAAS & PAAS & SAAS \u2022 Azure Data Bricks, Data Factory, Pipelines, Event Hubs, Function \u2022 apps, Data Engineering \u2022 Azure Monitor, Log Analytics Workspaces, Application Insights, \u2022 Storage accounts, Dashboards, KQL \u2022 Azure Security, Governance and Compliance \u2022 Azure solution design and architecture \u2022 Foundational SQL experience/usage \u2022 Adhere to response and resolution SLAs and escalation processes \u2022 in order to ensure fast resolution of customer issues that exceed expectations \u2022 Submit well-documented bugs and feature requests arising from customer submitted requests and work with Engineering towards a resolution. \u2022 Design and develop frameworks and core functionality. \u2022 Identify gaps and come up with working solutions. \u2022 Application architecture and solution design in Azure \u2022 Analyze production workloads and develop strategies \u2022 Utilize technical acumen and in-depth understanding of \u2022 business processes and practices to influence the creation, \u2022 modification, and execution of operational and strategic plans \u2022 Create, maintain, and administer database platforms and applications for company-wide use \u2022 Maintain data integrity and database performance, stability,and scalability. Make recommendations to optimize database and application performance and efficiency \u2022 Diagnose and resolve database, network, and security issues. Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "399_AWS DevOps": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: AWS DevOpsProvide a stable cloud environment with proven experience in cloud engineering and a deep understanding of AWS and/or GCP services and offerings. Develop processes and pipelines with hands-on experience in CI/CD technologies - eg: Argo CD (preferred), and Argo Workflow (preferred), Chef or Puppet, Ansible, Jenkins. Gitlab etc. Create and provide support for monitoring, alerting, fault analysis, and other common reliability engineering concepts. Provide incident response and RCA using strong problem-solving skills and the ability to adapt to a fast-paced, evolving environment. Develop and deploy containerized applications with proven experience in Kubernetes and container orchestration.Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "400_Do not share without Canary exp---DevOps Engineer at Plano, TX-Only local": "Akshat,\nTanishasystems.inc\nakshat@tanishasystems.com\nReply to: akshat@tanishasystems.com\nGreetings..!! My name is Akshat, and I am a Technical Recruiter at Tanisha Systems Inc. Tanisha Systems Inc is a global contingency staffing firm servicing. We have an excellent job opportunity with one of our clients. Job Title : DevOps EngineerLocation : Plano, TX(Hybrid-3 days per week onsite)Type: ContractJob Summary :\u2022 we need DevOps with Java and Canary(monitoring tool)Qualification:BSc in Computer Science, Engineering or relevant fieldExperience as a DevOps Engineer or similar software engineering roleProblem-solving attitudeCollaborative team spiritGood knowledge of AWS, Cloud Watch.RegardsAkshat KumarAkshat@tanishasystems.comwww.tanishasystems.comlinkedin.com/in/akshat-kumar-7245821a299 Wood Ave South, Suite # 308, Iselin, New Jersey, NJ 08830, USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "401_Urgent opening for Azure Architect in Houston TX (Hybrid)": "Kundan Mishra,\nGlobal Applications Solutions\nkundan.mishra@globalapplications.com\nReply to: kundan.mishra@globalapplications.com\nHiThis is Kundan Mishra from Global Applications Solution LLC. I have an urgent opening Data Engineer in ALPHARETTA, GA (onsite)Position: Azure ArchitectLocation: Houston TX (2 days onsite)Contract Role Description:Design and implement advanced functions of Office 365 and/or Azure AD Premium including security, governance and complianceProvide second level support for issues wrt to compute, network and storage on AWS and AzureStrong understanding of features and capabilities of the Microsoft Cloud Platform (Security, Firewalls, RedisCache, Key Vault, Service Bus, ASR, Networking, OMS, Blob Storage, Resource Groups, NSG, Application Insights, SQL Server, Sql DB, Load-Balancers)Design and implementation of ADFS identity solutions for more than Microsoft Office 365Designing Solutions in Microsoft Azure as per project specifications and needsDrive architecture and technical discussion to lead cloud security, privacy, and compliance decisions scale, performance, agility, HA/DR considerationsConfigure and manage customer AWS / Azure cloud environmentAbility to analyze systems and identify problems migrating to Azure cloud Kundan MishraSr. Technical RecruiterGlobal Applications Solution LLC (USA)Email : kundan.mishra@globalapplications.com linkedin.com/in/kundan-mishra-30662269 www.globalapplications.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "402_Azure Architect  Houston TX": "rozy kumari,\nglobalappplecation\nrozy@globalapplications.com\nReply to: rozy@globalapplications.com\nAzure ArchitectHouston TX (2 days onsite)Contract Design and implement advanced functions of Office 365 and/or Azure AD Premium including security, governance and complianceProvide second level support for issues wrt to compute, network and storage on AWS and AzureStrong understanding of features and capabilities of the Microsoft Cloud Platform (Security, Firewalls, RedisCache, Key Vault, Service Bus, ASR, Networking, OMS, Blob Storage, Resource Groups, NSG, Application Insights, SQL Server, Sql DB, Load-Balancers)Design and implementation of ADFS identity solutions for more than Microsoft Office 365Designing Solutions in Microsoft Azure as per project specifications and needsDrive architecture and technical discussion to lead cloud security, privacy, and compliance decisions scale, performance, agility, HA/DR considerationsConfigure and manage customer AWS / Azure cloud environmentAbility to analyze systems and identify problems migrating to Azure cloudRegards,Rozy kumariGlobal Applications SolutionRozy@globalapplications.comwww.globalapplications.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "403_Lead Cloud Automation Engineer NYC, NY, Local": "Farha khan,\nTek Inspirations LLC\nfarha.khan@tekinspirations.com\nReply to: farha.khan@tekinspirations.com\nJob Description -Job Description -Title : Lead Cloud Automation Engineer Location: NYC, NY, Mostly Remote with some onsite workDuration: 6+ monthsExp: more than 12 years of exp, Lead is required but Architect level will be prefered if submitted. Note: please do not copy paste skills and experience from JD into the resume. Note: Client is looking for a Lead Cloud Developer, need good Developement experience and heavy network. GCP cloud is mandatory. someone Comining from Enterprise, Hands-on architecture and automation experience required. Job DescriptionRole and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform.Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform. Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture.Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code.Automate the existing code components and integrate technologies to eliminate manual deployment steps.Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience.Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages.Experience with automating and integrating Serverless cloud provided PaaS solutions.Ability to troubleshoot code and logic errors for cloud-based network services.Understanding of deployment platforms and databases via CI/CD pipeline. Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms.Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus.Proficiency in cloud automation using cloud native CLI/API.Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP.Must have working experience with Cloud Native Networking technologies.Must have experience using PowerShell to configure Cloud Networking components.Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS.Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes.A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team.Able to work under tight timeline and deliver on complex problems.Bachelor's degree in computer science, engineering or a related field, or equivalent work experienceCertifications: CISSP, CCSP, Microsoft MCSE Azure \u2013 400 or 500\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "404_Urgent Opening _____Azure Architect location Houston TX (Hybrid - 2 Days Onsite Per Week)": "Avinash,\nGlobal Applications\navinash.kumar@globalapplications.com\nReply to: avinash.kumar@globalapplications.com\nHi,Hope you are doing well!!We are trying to reach you for new requirement of our client if you are interested then revert me back with your updated resume for further step. Job Title: Azure ArchitectJob Location: Houston TX (Hybrid - 2 Days Onsite Per Week)Job Type: Contract Job Description: Design and implement advanced functions of Office 365 and/or Azure AD Premium including security, governance and complianceProvide second level support for issues wrt to compute, network and storage on AWS and AzureStrong understanding of features and capabilities of the Microsoft Cloud Platform (Security, Firewalls, RedisCache, Key Vault, Service Bus, ASR, Networking, OMS, Blob Storage, Resource Groups, NSG, Application Insights, SQL Server, Sql DB, Load-Balancers)Design and implementation of ADFS identity solutions for more than Microsoft Office 365Designing Solutions in Microsoft Azure as per project specifications and needsDrive architecture and technical discussion to lead cloud security, privacy, and compliance decisions scale, performance, agility, HA/DR considerationsConfigure and manage customer AWS / Azure cloud environmentAbility to analyze systems and identify problems migrating to Azure cloud Regards,Avinash KumarEmail: avinash.kumar@globalapplications.comGlobal Applications Solution LLC (USA)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "405_Urgent Opening _____AWS Developer at location Erie, PA (Onsite)": "Avinash,\nGlobal Applications\navinash.kumar@globalapplications.com\nReply to: avinash.kumar@globalapplications.com\nHi,Hope you are doing well!!We are trying to reach you for new requirement of our client if you are interested then revert me back with your updated resume for further step. Job Title: AWS DeveloperJob Location: Erie, PA (Onsite)Job Type: Contract Job Description: AWS developer - build code around aws services, test, deploy code and maintain code. Experience (Years): 8-10 Essential Skills: AWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for aws services. Aurora, Amazon kafka, S3, EMR Desirable Skills: AWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for aws services Keywords: aws developer, Aurora, aws kafka, EMR, REST API Regards,Avinash KumarEmail: avinash.kumar@globalapplications.comGlobal Applications Solution LLC (USA)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "406_Need AWS Python with Golang, Plano, TX (Need Locals Only)": "Srinath,\nIT Strategies\nsrinath@itstrategiesinc.com\nReply to: srinath@itstrategiesinc.com\nPosition: AWS Python with GolangLocation: Plano, TXDuration: Long term Contract\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "407_Sr Cloud network Architect :: Hybrid onsite  Alpharetta GA or Frisco TX or Kansas City Kansas or Bellevue WA :: Contract": "Vikas Kumar,\nTanisha Systems\nvikas.kumar@tanishasystems.com\nReply to: vikas.kumar@tanishasystems.com\nRole: : Sr Cloud network ArchitectLocation: Hybrid onsite \u2013 (Alpharetta GA/Frisco TX/Kansas City Kansas /Bellevue WA- Onsite from any of the locations)Type: Contract Key ResponsibilitiesOversee the network onboarding process for new users and systems into Cloud environmentProvision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.Conduct regular network assessments and audits to ensure compliance with internal and external requirements.Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.Provide technical support and troubleshooting for Cloud network-related issuesStay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications Experience with network onboarding and provisioning in cloud environments.Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, MacieIn-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.Familiarity with security regulations, standards, and compliance requirements for cloud network environments.Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.Strong communication and documentation skills for collaborating with cross-functional teams.Experience with automation tools (e.g., Python, Terraform) for network configuration and managementRelevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred. Thanks & Regards Vikas KumarTechnical RecruiterTanisha Systems Inc.Mob: 7323699904Email: vikas.kumar@tanishasystems.comWeb: www.tanishasystems.comAddress: [99 Wood Ave South Suite # 308, Iselin, NJ 08830]\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "408_Urgent Need: AWS Cloud Architect || Local to Oregon|| Skype": "Ashish Kumar,\nTek inspirations LLC\nashish.kumar@tekinspirations.com\nReply to: ashish.kumar@tekinspirations.com\nJob Description -Title \u2013 AWS Cloud ArchitectLocation \u2013 Remote Must be local to Oregon Duration \u2013 6 months Contract MOI: Phone then VideoUtility experience would be greatTop 3 Must-Haves (Hard and/or Soft Skills):1. AWS services, including but not limited to EC2, Lambda, API Gateway, S3, RDS, DynamoDB, Athena, Glue, Kinesis, and other2. Data modeling, dimensional modeling, and database design3. Strong problem-solving, analytical, and decision-making skills.Top 3 Nice-To-Haves (Hard and/or Soft Skills)1. Familiarity with enterprise architecture frameworks (e.g., TOGAF, Zachman2. Experience with DevOps practices, including CI/CD pipelines, IaC, and automated3. Knowledge of data integration, ETL/ELT processes, and tools.Education Requirements (Experience in Lieu of Degree): Bachelor's degree in Computer Science, Information Technology, or a related field, or equivalent experience.Certification Requirements (Any Preferences): TOGAF , AWS certifications etc.How many years of experience are you looking for?: 7+ yearsJob Description:We are seeking an experienced AWS Full Stack Architect to design, implement, and optimize end-to-end cloud solutions for PGE's business needs. This role combines expertise in data architecture, IT solution architecture, and full-stack development on the AWS platform. The ideal candidate will have a strong background in architecting scalable, secure, and cost-effective solutions using a wide range of AWS services.Responsibilities:Cloud Architecture:\u2022 Design and implement cloud architectures on AWS for various workloads, including web applications, microservices, data pipelines, and analytics solutions.\u2022 Leverage AWS services such as EC2, Lambda, API Gateway, S3, RDS, DynamoDB, Amplify and others to build robust and scalable solutions.\u2022 Optimize cloud architectures for performance, scalability, reliability, security, and cost-effectiveness.\u2022 Implement DevOps practices, including CI/CD pipelines, infrastructure as code (IaC), and automated deployment processes.Data Architecture:\u2022 Design and implement data architectures on AWS using services like S3, Athena, Glue, Kinesis, and others.\u2022 Build automated data pipelines for ETL/ELT processes using AWS data integration services.\u2022 Implement data governance, data quality, master data management, and metadata management processes.\u2022 Collaborate with data engineers, analysts, and scientists to understand data requirements.IT Solution Architecture:\u2022 Gather and analyze business requirements and translate them into technical specifications and architectural designs.\u2022 Design and document the architecture for new IT solutions, including system integrations, data flows, and AWS components.\u2022 Evaluate and recommend appropriate AWS services, third-party tools, and best practices to meet PGE's needs.\u2022 Conduct architectural reviews and provide guidance to ensure adherence to standards and governance policies.General:\u2022 Provide technical leadership, mentorship, and knowledge-sharing to other IT and data professionals.\u2022 Collaborate with cross-functional teams to ensure successful implementation of designed solutions.\u2022 Ensure compliance with PGE's security, governance, and regulatory requirements.Requirements:\u2022 7+ years of experience in cloud architecture, data architecture, IT solution architecture, or related roles.\u2022 Strong expertise in a wide range of AWS services, including but not limited to EC2, Lambda, API Gateway, S3, RDS, DynamoDB, Athena, Glue, , Kinesis, and others.\u2022 Proficiency in data modeling, dimensional modeling, and database design.\u2022 Knowledge of data integration, ETL/ELT processes, and tools.\u2022 Familiarity with enterprise architecture frameworks (e.g., TOGAF, Zachman) is a plus\u2022 Strong programming skills in languages such as Python, Java, or Node.js.\u2022 Experience with DevOps practices, including CI/CD pipelines, IaC, and automated deployment.\u2022 Strong problem-solving, analytical, and decision-making skills.\u2022 Excellent communication and collaboration abilities.\u2022 Bachelor\u2019s degree in computer science, Information Technology, or a related field, or equivalent experienceRegards, Ashish Kumar Senior Technical RecruiterTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035.Direct: +1 469-898-0378 | Email: - ashish.kumar@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "409_Need AWS Devops with Infrastructure, Plano, Tx (Need Locals Only)": "Srinath,\nIT Strategies\nsrinath@itstrategiesinc.com\nReply to: srinath@itstrategiesinc.com\nRole: AWS Devops with InfrastructureLocation: Plano, TX (Onsite)Duration: Contract\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "410_Azure Solution Architect": "vikas Roy,\nscalable-systems\nvikas.roy@scalable-systems.com\nReply to: vikas.roy@scalable-systems.com\nPosition \u2013 Azure Solution Architect ( Need strong candidate )Location -Chicago, IL (Need Only IL Candidate )Client -TCSQualifications:- Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect,Job DescriptionThis role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation.2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines.3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software.4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools.5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications:- Bachelor\u2019s degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Vikas RoyScalable SystemsDelivery ManagerMailto: vikas.roy@scalable-systems.com\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026Scalable Systems | Inspiring InnovationBig Data | Analytics | Integration | Intelligencewww.scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "411_Need----Technical Program Manager with AWS Cloud &amp; Mongo DB---Pleasanton, CA (Hybrid)": "Rama Kant Tiwari,\nSmart IT Frame LLC.\nramakant@smartitframe.com\nReply to: ramakant@smartitframe.com\nJD:Job title : PROGRAM MANAGER Location: Pleasanton \u2013 CA (Hybrid)Job summary : We are seeking a highly skilled Program Manager with 10 to 12 years of experience to join our team as a Business Associate. The ideal candidate will have expertise in Google BigTable, AWS Machine Learning, Docker, Kubernetes, MongoDB, and API. This role involves overseeing multiple projects, ensuring timely delivery, and collaborating with cross-functional teams to drive business success. Experience : 10 to 12Yrs Required Skills : Google Big Table, Kubernetes, API, Docker, AWS Machine Learning, Mongo DB Roles & Responsibilities : Oversee the planning, execution, and delivery of multiple projects to ensure they align with business objectives and timelines.Collaborate with cross-functional teams to define project scope, goals, and deliverables.Utilize Google BigTable to manage and analyze large datasets efficiently.Implement AWS Machine Learning models to enhance data-driven decision-making processes.Deploy and manage containerized applications using Docker and Ensure the integrity and performance of MongoDB databases.Develop and maintain APIs to facilitate seamless integration between systems. Monitor project progress and provide regular updates to stakeholders. Identify and mitigate potential risks to ensure project success. Foster a collaborative and innovative environment within the team. Provide technical guidance and support to team members as needed. Ensure compliance with industry standards and best practices. Conduct post-project evaluations to identify areas for improvement. Qualifications \u00b7 Possess a strong background in Google BigTable, demonstrating the ability to manage and analyze large datasets. \u00b7 Have extensive experience with AWS Machine Learning, showcasing the ability to implement and optimize machine learning models. \u00b7 Demonstrate proficiency in Docker and Kubernetes for deploying and managing containerized applications. \u00b7 Exhibit expertise in MongoDB, ensuring database integrity and performance. \u00b7 Show a solid understanding of API development and maintenance for system integration. \u00b7 Hold a Bachelors degree in Computer Science, Information Technology, or a related field. \u00b7 Possess excellent communication and interpersonal skills to collaborate effectively with cross-functional teams. \u00b7 Have a proven track record of successfully managing multiple projects simultaneously. \u00b7 Display strong problem-solving and analytical skills to identify and address project challenges. - Be detail-oriented with a focus on Thanks, and RegardsRama Kant TiwariTeam LeadEmail: ramakant@smartitframe.comPhone: +1 201-201-1445Smart IT Frame LLC.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "412_AWS WAF CONSULTANT position at Atlanta, GA location": "Hari prasad,\nSmartfolks\nprasad@smartfolksinc.com\nReply to: prasad@smartfolksinc.com\nHi,Greetings from Smart Folks Inc!My name is Hari Prasad we have a job opportunity for AWS WAF CONSULTANT for one of our clients. Please find the Job description below, if you are available and interested, please send us your word copy of your resume with following detail to Prasad@smartfolksinc.com or reach me at 469-425-3345 Role: AWS WAF CONSULTANT- Web Application FirewallWork Location & Reporting Address: Atlanta, GA 30322Contract duration (in months): 6-12Implementation: InfosysMinimum work experience: 10-12 Years Job Details: Key Responsibilities:Design, implement, and manage AWS WAF to protect web applications from security threats.Develop and maintain automation scripts for WAF deployment and management.Collaborate with the security team to integrate AWS WAF with other security tools and services.Monitor and respond to security incidents related to web application threats.Create and manage Infrastructure as Code (IaC) templates for AWS WAF and related services.Conduct regular security assessments and audits of web applications.Provide technical support and troubleshooting for AWS WAF-related issues.Stay up-to-date with the latest security trends, threats, and technologies. Requirements:Proven experience with AWS Web Application Firewall (WAF).Proficiency in scripting languages such as Python, Bash, or PowerShell.Experience with Infrastructure as Code (IaC) tools like AWS CloudFormation, Terraform, or Ansible.Strong understanding of web application security principles and best practices.Familiarity with AWS services and architecture.Experience with continuous integration and continuous deployment (CI/CD) pipelines.Excellent problem-solving skills and attention to detail.Strong communication and teamwork abilities.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "413_Need----Program Manager with Google Bigtable &amp; AWS---Pleasanton, CA (Hybrid)": "Rama Kant Tiwari,\nSmart IT Frame LLC.\nramakant@smartitframe.com\nReply to: ramakant@smartitframe.com\nJD:Job title : PROGRAM MANAGER Location: Pleasanton \u2013 CA (Hybrid)Job summary : We are seeking a highly skilled Program Manager with 10 to 12 years of experience to join our team as a Business Associate. The ideal candidate will have expertise in Google BigTable, AWS Machine Learning, Docker, Kubernetes, MongoDB, and API. This role involves overseeing multiple projects, ensuring timely delivery, and collaborating with cross-functional teams to drive business success. Experience : 10 to 12Yrs Required Skills : Google Big Table, Kubernetes, API, Docker, AWS Machine Learning, Mongo DB Roles & Responsibilities : Oversee the planning, execution, and delivery of multiple projects to ensure they align with business objectives and timelines.Collaborate with cross-functional teams to define project scope, goals, and deliverables.Utilize Google BigTable to manage and analyze large datasets efficiently.Implement AWS Machine Learning models to enhance data-driven decision-making processes.Deploy and manage containerized applications using Docker and Ensure the integrity and performance of MongoDB databases.Develop and maintain APIs to facilitate seamless integration between systems. Monitor project progress and provide regular updates to stakeholders. Identify and mitigate potential risks to ensure project success. Foster a collaborative and innovative environment within the team. Provide technical guidance and support to team members as needed. Ensure compliance with industry standards and best practices. Conduct post-project evaluations to identify areas for improvement. Qualifications \u00b7 Possess a strong background in Google BigTable, demonstrating the ability to manage and analyze large datasets. \u00b7 Have extensive experience with AWS Machine Learning, showcasing the ability to implement and optimize machine learning models. \u00b7 Demonstrate proficiency in Docker and Kubernetes for deploying and managing containerized applications. \u00b7 Exhibit expertise in MongoDB, ensuring database integrity and performance. \u00b7 Show a solid understanding of API development and maintenance for system integration. \u00b7 Hold a Bachelors degree in Computer Science, Information Technology, or a related field. \u00b7 Possess excellent communication and interpersonal skills to collaborate effectively with cross-functional teams. \u00b7 Have a proven track record of successfully managing multiple projects simultaneously. \u00b7 Display strong problem-solving and analytical skills to identify and address project challenges. - Be detail-oriented with a focus on Thanks, and RegardsRama Kant TiwariTeam LeadEmail: ramakant@smartitframe.comPhone: +1 201-201-1445Smart IT Frame LLC.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "414_Urgent Req:DevOps Engineer || Hybrid in Austin, TX (Local Candidate only)": "Rajesh Kumar,\nPivotal Technologies, Inc\nrajesh.kumar@pivotal-technologies.com\nReply to: rajesh.kumar@pivotal-technologies.com\nTitle: DevOps EngineerLocation: Hybrid in Austin, TX (Local Candidate only)Duration: 6+ MonthsVisa: Any but not H1B Send back filled out form and resume. But also attach linked in and make sure the data on the linked in is the same as the resumes company, title, date (mm/dd)The Worker will perform highly advanced DevOps related work as part of a cross-functional team under the direction and guidance of the Shared Application Services manager. The Worker must have strong hands-on professional software development and/or IT operations experience building, testing, and deploying secure production applications and systems using continuous integration and continuous delivery/deployment (CI/CD) pipelines in a large-scale enterprise environment. The worker will be responsible for designing, building, and maintaining our CI/CD pipelines, monitoring applications and infrastructure for performance issues, and conducting regular assessments to ensure systems are performing optimally. The worker will also provide training on DevOps practices and stay updated on latest technologies. They should have expertise in a variety of DevOps tools, knowledge of DevOps automation, and strong experience with Linux administration, containerization technologies, and microservices architecture. The worker is expected to proactively address potential security risks and performance issues to ensure the security, stability, and efficiency of our CI/CD pipelines.CPA WILL REQUIRE THE WORKER TO WORK ON THE FOLLOWING INITIATIVES AND WILL PERFORM ADVANCED TASKS SUCH AS:\u00b7 CI/CD Modernization and automation of the entire pipeline from code check-in to deployment utilizing industry best practices such as Infrastructure as Code (IaC), Configuration as Code (GitOps), and Blue-Green and Canary Deployment Strategies.\u00b7 Implement \"Shift Left\" security approach by integrating security tools and automating security checks and compliance into the CI/CD pipeline.\u00b7 Monitoring and Observability to provide comprehensive monitoring, logging, and alerting for the CI/CD pipeline.\u00b7 Participating in all phases of SDLC.\u00b7 Performing extensive code reviews and analysis.\u00b7 Writing reports on code analysis to determine if industry standards and secure coding best practices are being followed; provide analysis to address found short comings.\u00b7 Providing guidance and knowledge sharing to existing development staff.MUST HAVE7 Required Professional experience in DevOps engineering, Software Development, or related field6 Required Experience with programming languages such as Java and .NET5 Required Experience with scripting languages such as Bash, Python, and PowerShell to automate repetitive tasks such as monitoring, deployments, and configuration management5 Required Experience setting up and managing Jenkins servers, creating and maintaining CI/CD pipelines, integrating with other tools (e.g., Git, Maven, SonarQube), writing Groovy scripts for pipeline automation, and monitoring and optimizing Jenkins performance.5 Required Experience with Infrastructure as Code tools like Ansible, Terraform, or Chef5 Required Experience with containerization and orchestration tools such as Docker and Kubernetes5 Required Experience with automation of infrastructure provisioning and configuration management5 Required Experience with Maven in building and managing Java projects, maintaining POM files, troubleshooting build issues, dependency management and versioning, and integrating with CI/CD pipelines5 Required Experience with Artifactory set up, configuration, managing binary repositories, integrating with build tools (e.g., Maven and Jenkins), managing artifact lifecycle and versioning, and implementing security and access controls.5 Required Experience with microservices architecture, design, development and containerization and orchestration5 Required Experience with SQL and NoSQL databases5 Required Experience designing, developing, testing, integrating, and implementing secure REST APIs5 Required Experience with code reviews and in-depth code analysis5 Required Experience with highly complex application security requirements5 Required Experience with Git, Bitbucket, Subversion and version control systems4 Required Experience with SonarQube set up, configuration, integrating with CI/CD pipelines, and analyzing code quality and security vulnerabilities4 Required Experience with Jira and Confluence4 Required Experience with Agile teams3 Required Experience with coaching, training, mentoring and knowledge transferPREFERRED4 Preferred Experience in Cybersecurity and implementing and automating security best practices into CI/CD pipelines4 Preferred Experience with security testing tools such as SAST, DAST, or IAST3 Preferred Experience with cloud technologies and platforms such as AWS and Azure 3 Preferred Experience working with legacy applications/services3 Preferred Experience in modern web technologies such as JavaScript, Node.js, React.js, Redux, HTML5, CSS33 Preferred Public sector experience (Federal, State or Local Government)2 Preferred Proficient with the Microsoft Office products, including Outlook, TEAMS, Microsoft Project, Word, Visio, Excel and PowerPoint\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "415_Urgent Opening || WS DevOps LEAD (TECHNICAL LEAD) with Insurance Domain | San Antonio, TX  - Onsite 5 Days": "Hariom Dubey,\n1st-recruit LLC\nhariom@1st-recruit.com\nReply to: hariom@1st-recruit.com\nMandatory Skillset: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS, Dynamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "416_AWS Developer || Erie, PA-- Onsite": "Anoop Chaubey,\nSiri info Solutions\nanoop.chaubey@siriinfo.com\nReply to: anoop.chaubey@siriinfo.com\nHi, This is Anoop from Siri Info Solutions, if you are looking for a job change or available in the job market. Please send me your updated resume. Job Title: AWS DeveloperLocation: Erie, PA (5 Days Onsite)Duration: Contract-2-Hire Job Description: AWS Developer - build code around AWS services, test, deploy code and maintain code.Competencies: Digital : Amazon Web Service(AWS) Cloud Computing, Software EngineeringExperience (Years): 8-10Essential Skills: AWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for AWS services. Aurora, Amazon Kafka, S3, EMR\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "417_JD regarding GTA AWS CLOUD NATIVE DEVELOPER (Hybrid)": "Pavan Kalyan,\nUnicorn Technologies\npavan@unicorntek.com\nReply to: pavan@unicorntek.com\nHiHope you are doing good,Role : GTA AWS CLOUD NATIVE DEVELOPER (Hybrid)Client : State of Georgia (Only Locals)Key Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:*Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).*Proven experience with XML, XSD, WSDL, and other related technologies*Proven experience as a software developer with a strong understanding of cloud computing principles and practices.*Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.*Proficiency in Python programming language; familiarity with other languages is a plus.*Experience with AWS Step Functions and State Machines is highly desirable.*Familiarity with Agile methodologies and SCRUM framework.*Strong problem-solving skills and ability to work effectively in a team environment.*Excellent verbal and written communication skills.Preferred Qualifications:*AWS certifications (e.g., AWS Certified Developer) are a plus.*Experience with CI/CD pipelines and DevOps practices.*Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).Thanks and Regards,Pavan KalyanTechnical RecruiterUnicorn Technologies LLCDesk: 4708702980 EXT :- 118pavan@unicorntek.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "418_Azure Engineer Microsoft Azure Fabric": "Sonu Uprati,\nValiantIQ Inc\nsuprati@valiantiq.com\nReply to: suprati@valiantiq.com\nJob Title: Azure Engineer - Microsoft Azure FabricPosition Type: ContractLocation: Bellevue, WA(Hybrid)Client: Larsen & Toubro Infotech / Parent Primary SkillsAzureJob descriptionPrimary Skill setExperience in ADF and Microsoft FabricMaintaining and monitoring of Data PipelinesMonitor data quality pillars like uniqueness completeness correctness timeliness consistency availabilityAbility to understand the issues with pipelines and perform root cause analysisGood documenting skills for capturing issue logs sequence of events root cause analysis and short term long term fix detailsUpdating wiki page for each pipeline enhancement or changeAssisting partner teams on information related to data issues so as to arrive at issue resolutionSend regular updates on status of open issues resolved issuesPeriodic review of Wiki pages and project documentation to ensure correctness of information and data quality rulesUpdating ADO daily including discussion threads at user story level tracking efforts against well defined tasks closure of tasksReporting on day to day progress to tech lead Secondary Skill setExperience in ADB Synapse SQL Cosmos Scope scriptingKnowledge of MSBI tech stackPerform data validation quality assurance analysis around datasetsEvaluate existing data quality rules and update them based on source field level or schema changesWorking Knowledge on MSBI technology stackRefer to existing PBI dashboards and monitor regularly for trends anomalies and take necessary action Thanks & Regards,Sonu UpratiTechnical Recruiter- ValiantIQ Inc.\"Searching Best Minds \u25a0 Searching Best Minds\"Email: suprati@valiantiq.com F. (302) 482-3672Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line for automatic removal. And mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "419_AWS Observability Engineer Location: NJ (Onsite)": "Praneeth,\nRumis tech\npraneethb@rumistech.com\nReply to: praneethb@rumistech.com\nRole: AWS Observability EngineerLocation: NJ (Onsite)Duration: Long TermJob Description:\u00b7 Design and develop observability and monitoring solutions by configuring and managing observability tools on AWS.\u00b7 Extensive hands-on experience with installing necessary agents on servers, virtual machines, and AWS-supported services, and forwarding logs to a centralized location with configured aggregators.\u00b7 Deep understanding of logs, metrics, and tracing services and their capabilities.\u00b7 Conduct assessments related to AWS platform and observability, providing detailed comparative analysis, cost metrics, advantages of various monitoring tools, and setting benchmarks.\u00b7 Develop and maintain documentation processes by creating templates for observability, including assessment checklists, questionnaires, presentations, and proof of concepts with a hands-on approach.\u00b7 Participate in sessions and workshops for clients and internal team members on observability, delivering high-quality presentations.\u00b7 Participate in solution design and proposal development activities.\u00b7 Proven experience in IT monitoring and observability with a focus on cloud environments.Requirements:Bachelor\u2019s degree in computer science, Information Technology, or a related field.At least 10 years of IT experience, with a minimum of 6 years focused on AWS Cloud with an emphasis on observability.Extensive experience with AWS services and a thorough understanding of compute, storage, networking, security, and database services in the cloud such as EC2, S3, VPCs, Network Flow Logs, RDS, etc.Expertise in AWS monitoring solutions such as Amazon CloudWatch and AWS CloudTrail.Proficiency in AWS-supported monitoring solutions such as Dynatrace, AppDynamics, DataDog, and Sumo Logic.Experience in monitoring infrastructure, APIs, microservices, JVMs, and RUM, with the ability to create necessary dashboards for visualization.Experience integrating with notification systems and incident management systems.Understanding of self-healing concepts and AIOps.Proficiency in programming languages such as Python, Java, or Go.Ability to work independently and as part of a team, demonstrating leadership when required.Relevant cloud certifications are required. -----Regards,Praneeth Kumar,Sr. IT RecruiterT (775) 667 - 9143 | F (000)000-0000 | praneethb@rumistech.com371 Hoes Ln, Suite 200, Piscataway, NJ 08854www.rumistech.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "420_Aws Cloud Developer at Atlanta, GA || Hybrid": "manindra,\nschrilltech\nmanindra@schrilltech.com\nReply to: manindra@schrilltech.com\nHi Good Morning,Hope you are doing wellWe have a position for the below requirement. Role: Aws Cloud DeveloperLocation : Atlanta, GA || HybridJob Id: 742293 Client: State of GA \u2013GTA Need State experience Mandatory Key Responsibilities:\u00b7 Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.\u00b7 Implement serverless architectures using AWS Lambda functions with Python.\u00b7 Build and orchestrate workflows using AWS Step Functions and AWS State Machines.\u00b7 Design, develop, and implement SOAP-based web services using services technologies.\u00b7 Create and manage custom headers for web services to ensure security, authentication, and data integrity.\u00b7 Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.\u00b7 Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.\u00b7 Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.\u00b7 Troubleshoot and resolve issues in development, testing, and production environments.\u00b7 Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:\u00b7 Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).\u00b7 Proven experience with XML, XSD, WSDL, and other related technologies\u00b7 Proven experience as a software developer with a strong understanding of cloud computing principles and practices.\u00b7 Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.\u00b7 Proficiency in Python programming language; familiarity with other languages is a plus.\u00b7 Experience with AWS Step Functions and State Machines is highly desirable.\u00b7 Familiarity with Agile methodologies and SCRUM framework.\u00b7 Strong problem-solving skills and ability to work effectively in a team environment.\u00b7 Excellent verbal and written communication skills. Preferred Qualifications:\u00b7 AWS certifications (e.g., AWS Certified Developer) are a plus.\u00b7 Experience with CI/CD pipelines and DevOps practices.\u00b7 Familiarity with containerization (e.g., Docker) and orchestration (e.g., Kubernetes).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "421_AWS DATA INTERGRATION ENGINEER - 13+ PROFILES ONLY - HYBRID - CA": "TEJASHWINI,\nATLANTIS IT GROUP\nteja@atlantisitgroup.com\nReply to: teja@atlantisitgroup.com\nTitle: AWS Integration EngineerWork location: Los Angeles, CA (90013) - HYBRIDDuration: 28 Months This role work: Hybrid. In office Tues/Wed/Thurs \u201cMust-have\u201d skills?1. AWS Data Integration2. Data Warehousing3. Data Modelling Job duties:\u2022 Define the direction and use of the AWS services such as AWS Glue, AWS Lambda, Amazon S3, Amazon Redshift\u2022 Provide expertise in AWS security best practices, including VPC, IAM, and data encryption.\u2022 Provide expertise on the approach to data modeling, including conceptual, logical, and physical data models within AWS\u2022 Develop data integration and transformation of data using tools such as AWS Glue, Python/Pyspark, SNP Glue\u2022 Optimization data integrations and debugging of issues in the data flow\u2022 Contribute and author deliverables which data integration strategy and the design, build and testing of data integrations. Thanks & Regards, Tejashwini.Technical RecruiterEmail ID: teja@atlantisitgroup.comwww.atlantisitgroup.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "422_Urgent role ||| Lead Cloud Automation Engineer |||  NYC NY Mostly Remote with some onsite work": "Rajeev,\nTek Inspirations LLC\nrajeev.kharwar@tekinspirations.com\nReply to: rajeev.kharwar@tekinspirations.com\nHello All,I hope you are doing well,I have a role of Lead Cloud Automation Engineer Please let me know if you are comfortable with this role,At the time of submission I need DL and Visa. Job Description \u2013 Title : Lead Cloud Automation Engineer Location: NYC, NY, Mostly Remote with some onsite workDuration: 6+ monthsExp: more than 12 years of exp, Lead is required but Architect level will be prefered if submitted. Note: Client is looking for a Lead Cloud Developer, need good Developement experience and heavy network. GCP cloud is mandatory. someone Comining from Enterprise, Hands-on architecture and automation experience required. Role and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform.Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform. Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture.Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code.Automate the existing code components and integrate technologies to eliminate manual deployment steps.Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience.Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages.Experience with automating and integrating Serverless cloud provided PaaS solutions.Ability to troubleshoot code and logic errors for cloud-based network services.Understanding of deployment platforms and databases via CI/CD pipeline. Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms.Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus.Proficiency in cloud automation using cloud native CLI/API.Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP.Must have working experience with Cloud Native Networking technologies.Must have experience using PowerShell to configure Cloud Networking components.Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS.Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes.A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team.Able to work under tight timeline and deliver on complex problems.Bachelor's degree in computer science, engineering or a related field, or equivalent work experience Certifications: CISSP, CCSP, Microsoft MCSE Azure \u2013 400 or 500 Regards,Rajeev kharwarSr.Technical Recruiter DevOps SpecialistTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035Desk # 469-393-0216 | E: rajeev.kharwar@tekinspirations.comWhatsapp : - 7525894499LinkedIN : - https://www.linkedin.com/in/rajeev-kharwar-8869251b5/ Reach out if you have candidates of ( Site Reliability, AWS/Azure, Cloud Computing, GCP, Infrastructure, ETL/Informatica, Data-warehouse, DataStage, Data Modeling, Python, Data Engineer, Data scientist, ML, Data Architect, Hadoop, Big Data ) Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line to remove@tekinspirations.com. And mention all the e-mail addresses to be removed with any e-mail addresses,which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "423_AWS Integration Engineer-HYBRID-CA-13+ Years of Experience": "keerthana,\nAtlantis IT group\nkeerthana@atlantisitgroup.com\nReply to: keerthana@atlantisitgroup.com\nTitle: AWS Integration EngineerWork location: Los Angeles, CA (90013)Duration: 28 Months This role work: Hybrid. In office Tues/Wed/Thurs\u201cMust-have\u201d skills?1. AWS Data Integration2. Data Warehousing3. Data Modelling Job duties:\u2022 Define the direction and use of the AWS services such as AWS Glue, AWS Lambda, Amazon S3, Amazon Redshift\u2022 Provide expertise in AWS security best practices, including VPC, IAM, and data encryption.\u2022 Provide expertise on the approach to data modeling, including conceptual, logical, and physical data models within AWS\u2022 Develop data integration and transformation of data using tools such as AWS Glue, Python/Pyspark, SNP Glue\u2022 Optimization data integrations and debugging of issues in the data flow\u2022 Contribute and author deliverables which data integration strategy and the design, build and testing of data integrations Thanks & Regards, KeerthanaEmail ID: keerthana@atlantisitgroup.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "424_Cloud Engineer with  AWS Architecture - C2C -Remote": "Subathraa,\nVysystems\nsubathraa@vysystems.com\nReply to: subathraa@vysystems.com\n6+ years of Hands-on experience with AWS ecosystem, infrastructure automation and monitoring toolsKernel, Networking and OS fundamentals.Public and Private cloud solutions (AWS Preferred)Experience in distributed systems and micro-service based architectures.Knowledge about CI/CD practices, Deployment patterns and relevant toolsets.Practical experience with K8s, envoy, API gateway, Service MeshInfrastructure as code & Configuration management with tools like Terraform, Chef, Cloudformation etcWorked on observability practices and toolchains (Monitoring, Metrics, Logging, Alerts & Tracing) (Prometheus, Cloudwatch, Relic, Datadog, ELK, EFK )Programming with Python or Java is a must.Strong in analytical problem solving and looking beyond what\u2019s obvious.Proven influencing skills and experience \u2013 the ability to drive change and to influence globally dispersed teams not under Direct ManagementMulti-cloud experience with AWS / Azure / GCP if possibleProfessional certifications with AWS Solution Architect/ DevOpsGood to haveYou have extended Kubernetes and/or are familiar with Kubernetes operators.You have built distributed solutions for configuration, monitoring and auto-mitigation of services.You have experience with cloud automations using Python, Lambda and have concepts on service control policies/ AWS Control Tower etc.You have knowledge about service discovery, networking security, multi-tenancy, database access, concurrency control or cache consistency.You are familiar with one or more open source projects, such as Prometheus, Grafana, ElasticSearch, Kibana, Envoy.You have experience mentoring junior engineers or leading projects\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "425_Azure DevOps Solution Architect, Chicago, IL": "Manasa,\nCalabitek\nmanasa@calabitek.com\nReply to: manasa@calabitek.com\nHello, Hope you are doing well, Job Title: Azure DevOps Solution ArchitectJob Location: 333 S Wabhash Ave Chicago, IL 60604 (100% Day One Onsite)Job Type: Long TermExperience: 14+Openings: 1Hiring Type: Video Call on C2C Software Solution Architect This role will be responsible with, architecting, implementing and supporting enterprise wide solutions, managing and maintaining the hardware and software owned by SQM. This will require supporting internal teams as well as interfacing with external teams and vendors on a regular basis. Key Responsibilities:1. Azure Cloud Migration: - Lead the planning, design, and execution of cloud migration projects to Azure. - Assess existing on-premises infrastructure, applications, and data for cloud readiness. - Develop migration strategies, including rehosting, refactoring, and rearchitecting as needed. - Execute migration tasks, including data transfer, application reconfiguration, and validation. 2. Infrastructure as Code (IaC) with Terraform: - Design and implement infrastructure using Terraform for repeatable and consistent deployment. - Develop and maintain Terraform scripts and modules for provisioning Azure resources. - Collaborate with DevOps and development teams to integrate IaC practices into CI/CD pipelines. 3. CI/CD Management: - Design, implement, and manage CI/CD pipelines using tools such as Azure DevOps, Jenkins, or GitLab. - Automate build, test, and deployment processes to enhance efficiency and reliability. - Monitor and troubleshoot CI/CD pipeline issues to ensure seamless delivery of software. 4. Enterprise Tools: - Set up and manage enterprise testing tools. - Integrate testing tools with CI/CD pipelines to enable automated testing. - Develop and enforce testing best practices and standards to ensure high-quality deliverables. - Provide support activities for the queries, issues, access, installation and configuration for supported tools - In depth knowledge and hands on experience with a broad range of industry leading commercial and open source SDLC (Agile and Waterfall), test management, test automation and CICD tools. 5. Collaboration and Documentation: - Work closely with architects, developers, and operations teams to align cloud migration efforts with business objectives. - Document cloud architectures, migration plans, Terraform scripts, and CI/CD workflows. - Provide training and support to team members on cloud and DevOps best practices. Qualifications: - 14+ Yrs of experience in IT/DevOps/Cloud Environment.- Bachelor's degree in Computer Science, Information Technology, or a related field.- Proven experience in Azure cloud migration projects.- Strong expertise in Terraform and infrastructure as code (IaC) principles.- Hands-on experience with CI/CD pipeline tools such as Azure DevOps, Jenkins, or GitLab.- Proficiency in setting up and managing enterprise testing tools like Selenium, JUnit, TestNG, and LoadRunner.- Solid understanding of cloud computing concepts, networking, and security in Azure.- Excellent problem-solving skills and attention to detail.- Strong communication and collaboration abilities. Preferred Qualifications: - Azure certifications (e.g., Microsoft Certified: Azure Solutions Architect, Azure Administrator).- Experience with other cloud platforms (AWS, Google Cloud).- Familiarity with containerization and orchestration tools like Docker and Kubernetes Preferred tool experience: - Microsoft Azure DevOps, JIRA, Bitbucket, Bamboo, ALM, Load Runner/Performance Center, JMeter, GitHub, Jenkins, Microfocus UFT/Lean FT, Selenium, Eclipse, ServiceNow, Perfecto, SeaLights, Gremlin, ToxiProxy, Chaos Monkey, Selenium Grid. Please share valid resume to manasa@calabitek.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "426_Cloud application Architect Los Angeles, CA (Hybrid 3 days onsite and 2 days remote)": "Rahul Reddy,\nCodebase\nrahul.b@codebaseinc.com\nReply to: rahul.b@codebaseinc.com\nRole: Cloud application Architect\u00b7 Years of Experience: 15+ years\u00b7 Travel Required: Yes\u00b7 Location: Los Angeles, CA (Hybrid 3 days onsite and 2 days remote)\u00b7 Your responsibilities in this role will include:\u00b7 Job Description: \u00b7 The Cloud Application Architect is responsible for leading the creation of a technology framework and providing technical leadership in support of modernization and vulnerability reduction initiatives in cloud computing and automation, with a focus on the design of systems and services that run on cloud platforms natively or as hybrid solutions.\u00b7 Additionally, the Cloud Application Architect will be responsible for ensuring that applications are designed to support the performance, security, monitoring, high availability, and disaster recovery requirements.\u00b7 The ideal candidate will have experience in designing large and complex IT operations in large organizations.\u00b7 The position requires strong leadership, communication, collaboration, and team-building skills, and must be able to collaborate effectively with multiple groups and stakeholders on multiple projects as a technical SME (subject matter expert).\u00b7 Cloud Architecture and Design and Development (50%)\u00b7 Demonstrate knowledge of AWS cloud architecture and implementation features (serverless, authentication, authorization, API design)\u00b7 Demonstrate knowledge of Azure DevOps, GitHub or experience in any other DevOps tool\u00b7 Demonstrate knowledge of CI/CD Pipelines, including pipeline automation for tools such as Checkmarx, SonarQube, NPM audit\u00b7 Demonstrate knowledge of infrastructure-as-code with tools such Terraform, AWS CloudFormation, etc.\u2026\u00b7 Act as a Subject Matter Expert to the organization provide cloud end-to-end reference architectures, including AWS for both native and hybrid cloud solutions.\u00b7 Develop a library of deployable and documented cloud design patterns, based on modernization application portfolio, as a basis for deploying services to the cloud.\u00b7 Demonstrate leadership ability to back decisions with research and the \u201cwhy,\u201d and articulate several options, the pros and cons for each, and communicate solutions with leadership\u00b7 Maintain overall industry knowledge on latest trends, technology, etc.\u00b7 Develop solutions architecture and evaluate architectural alternatives for private, public and hybrid cloud models, including SaaS, PaaS, and other cloud services.\u00b7 Contribute to DevOps development activities and complex development tasks.\u00b7 Define optimal design patterns and solutions for high availability and disaster recovery for applications.\u00b7 Collaborate with cyber-security and data privacy to meet security requirements, and translate requirements to developers, infrastructure, and cloud office teams. Consultation (20%)\u00b7 Drive scope definition, requirements analysis, functional and technical design, application build, product configuration, unit testing, and production deployment.\u00b7 Ensure delivered solutions meet/perform to technical and functional/non-functional requirements.\u00b7 Provide technical expertise and ownership in the diagnosis and resolution of an issue, including the determination and provision of workaround solution or escalation to service owners.\u00b7 Provide guidance on best practices and recommendation on technical governance, expertise related to cloud architectures, deployment, and operations. Thought Leadership (20%)\u00b7 Provide thought leadership in industry and to fellow team members, business stakeholders and program management.\u00b7 Communicate technology framework and solutions to ARB/CARB (cloud & architectural review boards).\u00b7 Advocate and define cloud architecture vision from a strategic perspective, including internal and external platforms, tools, budget, and systems. Mentoring (10%)\u00b7 Act as a mentor to team members and technical staff.\u00b7 Lead the definition and development of cloud reference architecture and management systems.\u00b7 Conduct product and governance work reviews with team members.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "427_Urgent Hiring For DevOps Engineer :::: Dearborn, MI": "Kayla Cooper,\nAdame Services\nkayla@adameservices.com\nReply to: kayla@adameservices.com\nHi Partner,We are hiring for the below job role. Please have a look and let me know if you have any suitable candidates. Position: DevOps Engineer location: Dearborn, MI (hybrid) Notes: Need local to MI only (with DL) and last 4 digits of SSN.Top Requirements:1. Bachelor Degree in Computer Science or other applicable degree2. A strong foundation in System Engineering and Administration. 5+ years of experience as a DevOps engineer or a similar software engineering role.3. Linux and Windows with one or more of the following scripting languages (Bash, Power shell, Python)4. Experience with one or more of the following version controls and repository management (Github, Git, Gitlab)5. Experience with on of the following CI/CD tools ( Jenkins, Gitlab) Skills Required:A strong foundation in System Engineering and Administration. -Linux -Windows Experience with one or more of the following scripting languages.Bash -PowerShell -Python Experience with one or more of the following Version Control/Repository Management tools.Git -GitHub -GitLab Experience with one of the following CICD tools. -Jenkins -GitLab CI/CD -GitHub Actions -Circle-CI Experience with one or more of the following Observability tools.Prometheus -Graphana -Datadog -CloudWatch -Lens Experience with one or more of the following log management tools.Elastic Stack -Grafana Loki -FluentD Experience with one or more of the following Configuration Management tools.Ansible -Chef -Puppet Experience with one or more of the following Cloud Platforms.GCP -AWS -Azure Experience Required:A strong foundation in System Engineering and Administration. -5+ years of experience as a DevOps engineer or a similar software engineering role. Education Required:Bachelor\u2019s degree in computer science or other applicable degree. Thanks | RegardsKayla Cooper |Technical RecruiterC: +13473777344https://www.linkedin.com/in/khushboo-kundra-5b8a47251/www.adameservices.comAdame Services 1309 Coffeen Avenue STE 1200, Sheridan, Wyoming, 82801 | USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "428_AWS Developer --Erie, PA--Onsite": "Vipul Kumar,\nGlobal Application Solutions\nvipul@globalapplications.com\nReply to: vipul@globalapplications.com\nHi Hope You're Doing Well. AWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for aws services. Aurora, Amazon kafka, S3, EMRAWS Developer , experience in AWS services build using Terraform, understand CICD pipeline, build and maintain code for aws services Email : Vipul@globalapplications.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "429_Need AWS Python with Golang, Plano, TX (Need Locals Only)": "Srinath,\nIT Strategies\nsrinath@itstrategiesinc.com\nReply to: srinath@itstrategiesinc.com\nPosition: AWS Python with GolangLocation: Plano, TXDuration: Long term Contract\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "430_Locals Only:AWS Architect:Los Angeles, CA": "umamahesh,\nHCL Global System\numa@hclglobal.com\nReply to: uma@hclglobal.com\nJob Title \u2013 AWS ArchitectLocation: Los Angeles, CA **Responsibilities:**- Design and implement AWS cloud solutions that meet client requirements.- Provide technical leadership and guidance to development teams.- Ensure the architecture is scalable, reliable, and secure.- Stay updated with the latest AWS services and technologies.**Qualifications:**- Strong experience as an AWS Architect with a deep understanding of AWS services.- Ability to design and manage enterprise-wide scalable solutions.- Excellent communication skills with the ability to engage effectively with clients and teams.- A proactive individual who takes full responsibility for their work.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "431_AWS Certified Network Engineer - Denver, CO (Onsite)": "Praveena Kanakanti,\nGAC Solutions\npraveena@gacsol.com\nReply to: praveena@gacsol.com\nHi, This is Praveena from GAC solutions. Hope you are doing good. Role: AWS Certified Network EngineerJob Location: Denver, CO (Onsite)Job Type: Contract Job Description:\u2022 Bachelor\u2019s degree in computer science or related field \u2022 Cisco Certified Network Professional (CCNP) certification required. \u2022 AWS Certified Advanced Network required. \u2022 Proven experience in designing, implementing, and managing AWS network environments. \u2022 5+ years managing LAN/WAN networks (2+ years working as a Network Engineer) \u2022Thorough knowledge and experience with Cisco network devices, including but not limited to: 7200 / 2901 Routers, PIX 515 / ASA 5510 Firewalls, 2900 / 3500 / 3560 switches, CSS 11500, AP 1200 / 1142, ACS 1121 \u2022 Advanced level understanding of protocols such as TCP/IP, UDP, SNMP, etc. \u2022 Working knowledge of internet services such as DNS, RADIUS, and LDAP. \u2022 Design, configuration and troubleshooting experience: \u2022 Routing: Virtual interfaces, BGP, OSPF, MPLS, EIGRP, RIP, PBR, QoS, Static Routing, Unsymmetrical Routing, Router NAT, HSRP/VRRP, ACL\u2019s, NTP, SNMP, Password Recovery \u2022 Switching: VLAN, VLAN Bleed, 802.1D, 802.1Q, 802.1X, VTP, Multilayer switching/routing, Multicasting \u2022 Firewalls: Interface priority, public vs Private IP space, Conduits/ACLs, Cryptography, Firewall NAT (static & dynamic), Hot Failover, Common TCP/UDP Ports, IPSEC, VPN, Policy Maps, Logging, Routing \u2022 Load Balancing: Routing, VIPs, Service Interfaces, Certificate Intercept, Owner Mapping, Load Balancing Algorithms, Content Grouping \u2022 Analytical and detailed oriented, with the ability to plan, organize and prioritize work to meet target project dates. \u2022 Independent-thinker and self-starter, who can work well within a team environment. \u2022 Communicates clearly and in an understandable manor with a wide range of people such as managers, customers, vendors, and the general public. \u2022 Demonstrated ability to effectively advice and counsel both managers and non-supervisory employees on sensitive work-related issues. \u2022 Demonstrated analytical and problem-solving skills, and the ability to organize and prioritize several projects and tasks at one time.\u2022 We are seeking a highly skilled and experienced on both On-premise and AWS Cloud Network to join our dynamic team.\u2022 Design, configure, and optimize AWS networking components, such as VPCs, subnets, route tables, and security groups. \u2022 Implement security best practices and compliance standards within the AWS network infrastructure. \u2022 Configure and manage network security groups, network ACLs, and other security features to protect sensitive data and resources. \u2022 Develop and maintain automation scripts and templates for network provisioning, configuration, and scaling using AWS CloudFormation or other infrastructure as code (IAC) tools. Network On-premise \u2022 Maintain comprehensive documentation of network configurations, diagrams, and standard operating procedures (SOPs) to ensure efficient knowledge sharing and compliance. \u2022 Optimize network resources to control costs while maintaining performance and scalability. \u2022 Identify opportunities for cost savings in network architecture and usage \u2022 Monitor network usage trends and plan for capacity expansion or optimization based on business requirements. \u2022 Monitor and optimize network performance, ensuring low latency, high availability, and fault tolerance. \u2022 Implement AWS services like Amazon CloudFront, Elastic Load Balancing (ELB), and Amazon Route 53 for optimal traffic distribution. \u2022 Design, configure and monitor network equipment, network usage and systems availability. \u2022 Support and troubleshoot various IP technologies, including but not limited to: Network connectivity, IP addressing, Routing Protocols (BGP, OSPF), HSRP, VPNs, IPSec, Load balancing, QOS, SNMP and 802.11x \u2022 Provide configuration and troubleshooting support for Ethernet Switches, Routers, Load-Balancers, VPN Concentrators, Firewalls, and networked servers. \u2022 Provide network management support as needed for event identification, event correlation, event escalation and event triage as related to network infrastructure. Cisco Certified Network Professional (CCNP), AWS Certified Advanced Network, Firewall, Load Balancing, Routing, Switching Thanks,Praveena KanakantiE: praveena@gacsol.comhttps://www.linkedin.com/in/praveena-kanakanti-058816194/www.gacsol.com\u2018Experts in Digitalization and Engineering - Enterprise 4.0\u2019\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "432_DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDescription: DevOps EngineerRemote Mobile CICD automationKubernetesGitOpsMicrosoft Azure is must.Firebase/Amplify5+ years with source control systems (GitHub, Bitbucket, Azure Repositories, Team Foundation Server).5+ years with CI/CD tools (GitHub Actions, Jenkins, Azure DevOps, Gitlab CI).5+ years supporting DevOps/DevSecOps practices.5+ years of public cloud experience (Azure).Experience with configuration management tools (Puppet, Chef, Ansible) is a plus.3+ years with mobile app deployments.3+ years with Docker and Kubernetes (AKS).5+ years with Infrastructure as Code (Terraform, Bicep, ARM Templates).Experience with Adobe AEM Cloud and GitOps is a plus.Familiarity with observability tools (Dynatrace, Application Insights, New Relic, Elastic).Experience with database platforms (SQL Server, MongoDB, DB2) is a plus.Understanding of networking concepts (DNS, firewalls, load balancers).Proficient in scripting languages (PowerShell).Bachelor s degree in computer science, IT, or related field\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "433_Urgent Opening _____Network Developer with AWS or Virtualization at location Remote": "Avinash,\nGlobal Applications\navinash.kumar@globalapplications.com\nReply to: avinash.kumar@globalapplications.com\nHi,Hope you are doing well!!We are trying to reach you for new requirement of our client if you are interested then revert me back with your updated resume for further step. Job Title: Network Developer with AWS/VirtualizationJob Location: RemoteJob Type: Contract Job Description: Skills: C++, Pub Sub Model, Linux IPC, Networking L3 layer development experience Docker , YAML, Cloud Migration experience Solution design and implementation of various applications in the OSS Service Fulfilment domain and their integrationAnalysis of existing architecture and interfaces. Implementation of the captured requirementsDevelop trusted relationship with project and program stakeholders.Continuous professional development to keep abreast of emerging technologies, methods, and best practices. Regards,Avinash KumarEmail: avinash.kumar@globalapplications.comGlobal Applications Solution LLC (USA)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "434_SeniorDevOps Engineer": "Ashish sharma,\nAdventatech\nasharma@adventatech.com\nReply to: asharma@adventatech.com\nJob Description -ROLE: Senior DevOps Engineer (HYBRID) LOCAL TO NJ-NY MINIMUM EXP : 15+ years of experience in IT Work Location: HYBRID FOR BROOKLYN, NY LinkedIn:MANDATORY SKILLS/EXPERIENCE\u00b7 At least 16 years of experience in software development and DevOps tools.\u00b7 Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.\u00b7 Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.\u00b7 Hands-on expertise in supporting and developing Java applications and Microservices.\u00b7 Familiarity with Microsoft Entra ID (Azure Active Directory).\u00b7 Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.\u00b7 Skilled in using Java project build tools like Maven, Gradle, and ANT.\u00b7 Extensive hands-on experience with Spring Boot, Python, JPA, Hibernate, and JSP.\u00b7 Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.\u00b7 Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.\u00b7 Effective oral and written communication skills.\u00b7 Knowledge of Terraform, Ansible, Helm, Kubernetes.\u00b7 Self-motivated with a proven track record as a team player.TASKS:\u00b7 Develop automation scripts and CI/CD pipelines across various Azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.\u00b7 Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.\u00b7 Participate in cloud architecture and design sessions for CI/CD pipelines.\u00b7 Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.\u00b7 Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.\u00b7 Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.\u00b7 Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.\u00b7 Update build agents, extend Terraform and Ansible scripts.\u00b7 Configure and manage cloud security and policy components.\u00b7 Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.\u00b7 Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.\u00b7 Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "435_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "436_AWS Software Engineer": "Ashish sharma,\nAdventatech\nasharma@adventatech.com\nReply to: asharma@adventatech.com\nJob Description -AWS Software EngineerLocation: West Chester, PA (Hybrid: 3 days on-site, 2 days remote)Duration: 1 year +MOI: Skype No Google Voice NumberNeed Linkedin and Local DL.REQUIREMENTSMinimum of 5 years related work experience as a Software Engineer Cloud development using Python (Java will be considered as well)Implementing, configuring and deploying AWS services: API Gateway, SQS, Event Bridge, Lambda, ECS, DynamoDB, etc.JOB DESCRIPTIONProvides intermediate level system analysis, design, development, and implementation of applications and databases. Integrates third party products.Translates technical specifications into code for moderately complex new or enhancement projects for internal clients. Writes programs, develops code, tests artifacts, and produces reports. Employs software development techniques to ensure tests are implemented in a way that support automation.Elevates code into the development, test, and production environments on schedule. Provides follow up production support. Submits change control requests and documents.Follows software development methodology. Follows development architecture standards.Participates in design, code, and test inspections throughout the life cycle to identify issues. Participates in systems analysis activities.Start/On-boarding Process:Once offer is made, they will do a background check (they run) that includes:- 10 years background- Drug test- Finger Printing * Timeline is 2 weeks from offer to start based on background process\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "437_AWS developer !! West Chester, PA (Hybrid: 3 days on-site, 2 days remote)": "shivam rai,\nadventa tech\nshivam@adventatech.com\nReply to: shivam@adventatech.com\nAWS developerLocation: West Chester, PA (Hybrid: 3 days on-site, 2 days remote)Domain: FinancialDuration: 1 year +Visa: OPT, TN visas, Citizens and Greencard holders OnlyMOI: Skype No Google Voice NumberNeed Linkedin and Local DL. REQUIREMENTSMinimum of 5 years related work experience as a Software Engineer Cloud development using Python (Java will be considered as well)Implementing, configuring and deploying AWS services: API Gateway, SQS, Event Bridge, Lambda, ECS, DynamoDB, etc.JOB DESCRIPTIONProvides intermediate level system analysis, design, development, and implementation of applications and databases. Integrates third party products.Translates technical specifications into code for moderately complex new or enhancement projects for internal clients. Writes programs, develops code, tests artifacts, and produces reports. Employs software development techniques to ensure tests are implemented in a way that support automation.Elevates code into the development, test, and production environments on schedule. Provides follow up production support. Submits change control requests and documents.Follows software development methodology. Follows development architecture standards.Participates in design, code, and test inspections throughout the life cycle to identify issues. Participates in systems analysis activities.Start/On-boarding Process:Once offer is made, they will do a background check (they run) that includes:- 10 years background- Drug test- Finger Printing * Timeline is 2 weeks from offer to start based on background process\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "438_Cloud Enterprise Security Architect  -Frisco, TX (Onsite Mandatory)": "Manjusha,\nSparinfosys\nmanjusha.murugummala@sparinfosys.com\nReply to: manjusha.murugummala@sparinfosys.com\nHi,Hope you\u2019re doing well,Please find the below JD and let me know your interest. Job Role: Cloud Enterprise Security Architect Job Location: Frisco, TX (Onsite Mandatory)Job duration: Long term contract Job/Responsibility Profile: Design and develop multi-tenant solutions for enabling cloud platform as serviceDeploy and Operate multi-cloud security solutions/platforms at Enterprise scaleDevelop end-to-end technical solutions in security spaceDevelop self-service solutions to onboard customers and manage users on the platformsAssess the customers' security architecture, requirements and provide guidanceDesign and develop policies to improve security posture and prevent threat exposureIdentify and adapt modern tools, principles and technologies to improve security across cloud landscapeSupport cloud customers through cloud-native architecture guidance, security architecture guidance, policy remediations, etc.Work with ITSM functions (Change management, Incident management, Problem management, Request management) as they apply to tools and platforms used by the team Technical Skills/Experience:Experience working in DevOps/GitOps teamsExperience developing Infrastructure and Operations code, Platforms, and AutomationsExperience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate Understanding of all the basic services provided by CSPs (AWS, Azure and GCP)Knowledge and hands-on experience of interacting with CSP APIsDeep knowledge of IAM, Policies, Network and other security servicesoAuthoring IAM policiesoAuthoring Organization Policies oDeveloping private network based applications (using private endpoints, Vnet integrations, IPSec)Developing Git Pipelines for managing platforms and operationsExperience in Java Springboot/Python/GoLang developmentExperience in developing SAML, OAuth based applicationsExperience working with IaC tools such as Terraform, CloudFormation, or ARM templates.Experience in K8s developmentGeneral experience working within ITSM processes (Change, Incident, Problem, Request management) in an Enterprise contextCertifications such as AWS Certified Security Specialty, Azure Security Engineer Associate, or GCP Professional Cloud Security Engineer are a plus Thanks & Have a Great Day!Manjusha\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "439_Lead Cloud Automation Engineer || NYC ||  12 years of exp": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nTitle : Lead Cloud Automation Engineer Location: NYC, NY, Mostly Remote with some onsite workDuration: 6+ monthsExp: more than 12 years of exp, Lead is required but Architect level will be prefered if submitted. Job DescriptionRole and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform.Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform. Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture.Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code.Automate the existing code components and integrate technologies to eliminate manual deployment steps.Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience.Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages.Experience with automating and integrating Serverless cloud provided PaaS solutions.Ability to troubleshoot code and logic errors for cloud-based network services.Understanding of deployment platforms and databases via CI/CD pipeline. Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms.Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus.Proficiency in cloud automation using cloud native CLI/API.Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP.Must have working experience with Cloud Native Networking technologies.Must have experience using PowerShell to configure Cloud Networking components.Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS.Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes.A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team.Able to work under tight timeline and deliver on complex problems.Bachelor's degree in computer science, engineering or a related field, or equivalent work experienceCertifications: CISSP, CCSP, Microsoft MCSE Azure \u2013 400 or 500\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "440_Azure Data Engineer only USC or GC Need only 15+year only": "Badal kanojia,\nStellentit\nbadal@stellentit.com\nReply to: badal@stellentit.com\nAzure Data EngineerRemotePhone + SkypeJD:Non-negotiable skills:SnowflakeAzure CloudData VaultWe are seeking a skilled Senior Data Engineer to join our dynamic technical team that supports the Investment Management line of business. As a key member of our data warehousing initiative, you will play a crucial role in designing, building, and maintaining scalable data solutions that support our investment strategies. This is transformational work on an exciting team of passionate data professionals, some of the best you'll meet.The Contributions You'll MakeDevelop and optimize data pipelines for our cloud-based data warehouse using Azure Cloud services.Design and implement efficient ETL processes to ingest, transform, and load data into our Snowflake-based data warehouse.Collaborate with data architects and analysts to design and implement data models that meet the business requirements.Implement and maintain data vault 2.0 methodologies to ensure data integrity, scalability, and flexibility in our warehouse architecture.Continuously monitor and optimize the performance of data pipelines and warehouse operations.Work closely with business stakeholders to understand data requirements and translate them into technical solutions.Lead efforts to enhance data quality and governance across the organization. Minimum Experience and KnowledgeBachelor's degree in Computer Science, Engineering, or related field; advanced degree preferred.Proven experience (5+ years) in data engineering, with a focus on data warehousing, ETL, and data modeling.Strong proficiency in cloud-based data technologies, particularly Azure Cloud services.Extensive hands-on experience with Snowflake or similar cloud-based data warehousing platforms.Solid understanding and implementation experience of data vault 2.0 methodology.Proficiency in programming languages such as SQL, Python, or Java for data manipulation and automation.Experience with data governance frameworks and best practices.Excellent problem-solving skills and ability to work effectively in a fast-paced, collaborative environment.Strong communication skills with the ability to explain complex technical concepts to non-technical stakeholders.Preferred Knowledge and ExperienceExperience in the investment management industry or financial services sector.Familiarity with big data technologies such as Hadoop, Spark, or Kafka. Certifications related to cloud platforms (Azure, AWS) or data engineering (e.g., Microsoft Certified: Azure Data Engineer).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "441_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "442_Devops Solution Architect": "priyanka,\nStellent IT\npriyanka@stellentit.com\nReply to: priyanka@stellentit.com\nDevops Solution Architect RemotePhone+SkypeJob DescriptionPartner with development, product management, architecture, information security and infrastructure teams to deliver software and technology products that meet requirements and produce desired business outcomes. Adhere to established architectural principles and associated processes for adopting new technology or integrating with a new third-party solution. Advise product management on technologically specific aspects of product roadmap development and recommend adoption of emerging technology appropriate to stated goals. Demonstrate the company\u2019s core values of respect, honesty, integrity, diversity, inclusion, and safety. Key ResponsibilitiesDevelop the overall software & deployment architecture of application solutions across teams.Partner with engineering staff to deliver solutions.Partner with 3rd party vendors to integrate capabilities as required by the solution under development.Partner with Senior Solutions Architects during large, cross-functional project discovery to develop high level architecture and design.Lead architectural design reviews as required.Draft, review, and maintain architectural diagrams, interface specifications and other architectural artifacts as required by the delivery teams to clearly communicate the needs of the solution under development.Interact with/present engineering and product management with the presence to influence and drive the decision makers to a solution.Identify technology overlaps and gaps and inform Senior Solutions Architecture staff.Assess emerging technologies and make appropriate recommendations based on business needs.Mentor engineering team members in software development and architectural principles, patterns, processes, and practices.Promote the capture and reuse of intellectual capital, including code objects and components.Contribute to Solutions Architecture Community of Practice\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "443_Cloud Security Architect || Urgent Requirement": "Aditya,\nHMG America\naditya.kumar@hmgamerica.com\nReply to: aditya.kumar@hmgamerica.com\nRole:- Cloud Security Architect Location-Reston VA Day 1 onsite in hybrid set up.NOTE:- Need strong Cloud (AWS only) security and architecture experience.Job Description \u2013The Cloud Security Architect (CSA) will leverage broad technical knowledge of cloud security best practices of key public cloud offerings of providers such AWS, Azure, and GCP to establish secure design patterns, to architect integrations among cloud and/or on-premises infrastructures. This individual must be able to assist in ensuring the security and compliance of the cloud environment based on enterprise cloud security policies, standards, and procedures. The CSA will ensure that solutions operating on the cloud comply with enterprise security requirements in both off-premises and hybrid environment models. The position will work with Enterprise Architects and Application Dev teams to come up with Security Architecture for applications and enterprise tech capabilities migrating to Cloud. Must-Haves: Required qualifications to be successful in this role: 10 years of total IT experience with the following must haves: \u2022 4+ Years of experience in Cyber Security field as an Information Security Architect or Cloud Security Architect \u2022 2-4 years of experience in AWS as a Cloud Security Architect/Engineer and must be certified in at least one of the cloud technologies/infrastructures\u2022 Excellent written and communication skills to report, document and communicate security architecture \u2022 Excellent coordination skills and must be detail oriented Nice-to-Haves: \u2022 Cloud agnostic security architecture experience a plus \u2022 1-2 years of experience in working with NIST assessments of business applications \u2022 Container Security experience to protect container workloads during build and run-time \u2022 API Security architecture experience with industry standard API Gateways \u2022 Security engineering/administration background leveraging SIEM, Network firewalls, host-based security, and security configuration \u2022 One or more industry standard security certification such as CISSP, CCSP or relevant GIAC certifications (ANY) \u2022 One or more Cloud Service Providers Security Specialty Certifications such as AWS Security Specialty or Azure AZ-500 Certification \u2022 The group of skills related to Security including designing and evaluating security systems, identifying security threats, securing computers, assessing vulnerability, etc. \u2022 The group of skills related to Relationship Management including managing and engaging stakeholders, customers, and vendors, building relationship networks, contracting, etc. \u2022 Skilled in presenting information and/or ideas to an audience in a way that is engaging and easy to understand \u2022 The group of skills related to Risk Assessment and Management including evaluating and designing controls, conducting impact assessments, identifying control gaps, remediating risk, etc. \u2022 Experience identifying and determining levels of risk to an organization's networks and systems using cybersecurity techniques \u2022 Working with people with different functional expertise respectfully and cooperatively to work toward a common goal \u2022 Skilled in cloud technologies and cloud computing \u2022 The group of skills related to Influencing including negotiating, persuading others, facilitating meetings, and resolving conflict Key Areas of Responsibility: \u2022 Partner with Enterprise/Portfolio Architecture team and Business Units development squads to collaboratively develop security architectures/designs leveraging approved patterns that ensure applications migrating from on-premise to Cloud, achieving high standards of security practices and compliance. \u2022 Drive the development and adoption of cloud security standards, best practices, and technologies within Enterprise IT infrastructure \u2022 Liaise on security-related issues with internal business stakeholders, InfoSec, Enterprise Architecture, and application development squads \u2022 Work to develop, enhance and document security architecture, security policies, patterns, procedures, guidelines and standards required to design cloud-based solutions \u2022 Educate application, portfolio and solution architects on secure solution design and industry best security practices \u2022 Work on assessments of compliance and standards including and not limited to NIST, FedRAMP, FIPS, etc. \u2022 Support threat modeling and update application security architecture as needed. \u2022 Support application development squads with Security implementations and issues\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "444_Need: AWS Architect (Enterprise Architect) - San Antonio, TX (Only Locals)": "Sri,\nShrive Technologies\nsri@shrivetechnologies.com\nReply to: sri@shrivetechnologies.com\nRole: AWS Architect (Enterprise Architect) Location: San Antonio, TX (Only Locals)Mandatory Skills: AWS Architecture, AWS Services-Lambda Functions, AWS EC2, AWS S3Bucket, AWS EKS, RDS, SQS , Dyanamo DB, Terraform SNS/ SQS, Java/J2EE, SOA , API Integration, Docker/Container/ Kubernetes, IAAS, AWS Cloud Watch, Kibana, Grafana, PrometheusOverview:Looking for talented cloud Developer (Lead) responsible for developing design and architecture to support cross domain business capabilities for P&C division. Candidate will work closely with technologists from P&C domains and help migrate them to private and public cloud. Candidate will focus on the intentional architecture and provide mentorship on emergent architecture to the technical teams. Candidate will have hands-on experience in cloud architectures (SaaS, PaaS, IaaS) and deep interest and experience in broad enterprise platforms and cloud patterns.Job Description:Participate in producing conceptual, solution and component-level architectures and associated artifacts. Develop cross domain business requirements reference architecture for transition and target state and contribute to the cloud related patterns and implementations. Develop common components for shared business capabilities with Standards and best practices. Develop the transition architecture using services offered by AWS and private cloud.Migrate legacy applications to AWS and private cloud. Hands on experience with AWS cloud networking including VPCs, Subnets, Security Groups, ACLs, Transit Gateways, ALB/NLB, Route53, ACM, API Gateway and related technologies.Understanding of networking processing, protocols, and standards - TCP/IP, UDP, DNS, HTTPHands on experience with security mechanisms including mTLS, x509, OpenID Connect, JWT/JWE, OAuth2, PEP/PDP, SAML, WS-Security, Basic Auth and ABAC/RBAC based policies.Hands on \u201ccode first\u201d development of cloud configuration and components from the ground up using tools like Gitlab and TerraformStrong hands-on experience in one or more development languages including Java, python, Golang.Experience with Open Trace, AWS Cloud Watch, DataDog, Prometheus, ELK, Grafana, Hystrix,, App Dynamics, NetCool and other tools to ensure the cloud is operating as expected.Hands on experience with continuous delivery (CD) tooling including Jenkins, GitLab, Travis CI, GoCD and others.Hands on experience with Containers including tools like Docker, Kubernetes, ECS/ECR, and other related technologies and tools.Experience in explaining complex technology decisions and developing high trust relationships with stakeholders. Ability to develop target state architecture using services offered by AWS and private cloud and vision to develop the transition architecture. Ability to design patterns for moving legacy applications to AWS and private cloud. Hand on experience with common architecture patterns (microservices, event-driven, REST, NO SQL databases, Caches, etc.) and services offered by AWS (S3, EKS, Lambda, RDS, elastic search, etc.) Hands on experience addressing operational and non-functional concerns (e.g. scalability, performance, maintainability, load distribution, resilience and recovery, etc.) Experience with SAFe framework or other similar agile frameworks. Experience with Java, Kafka, Spring, Redis, Kubernetes, OpenShift and others. Guidewire experience is big plus.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "445_AWS Principal Network Engineer - Denver, CO - Onsite": "Sukumar,\nGAC Solution Inc\nsukumar@gacsol.com\nReply to: sukumar@gacsol.com\nGreetings,My name is Sukumar, and I represent GAC Solution Inc., a staff augmentation firm providing a wide-range of talent on-demand and total workforce solutions. We have excellent domain expertise in all verticals. Repositioning professionals is what we do, and we do it very well. I am reaching out to you today as your profile matches an immediate job opportunity we have with our premier client.Please take a look at the job description below and let me know your interest. Role: AWS Principal Network Engineer Location: Denver, CO / OnsiteContract Job Description:\u2022 Bachelor\u2019s degree in computer science or related field\u2022 Cisco Certified Network Professional (CCNP) certification required.\u2022 AWS Certified Advanced Network required.\u2022 Proven experience in designing, implementing, and managing AWS network environments.\u2022 5+ years managing LAN/WAN networks (2+ years working as a Network Engineer)\u2022Thorough knowledge and experience with Cisco network devices, including but not limited to: 7200 / 2901 Routers, PIX 515 / ASA 5510 Firewalls, 2900 / 3500 / 3560 switches, CSS 11500, AP 1200 / 1142, ACS 1121\u2022 Advanced level understanding of protocols such as TCP/IP, UDP, SNMP, etc.\u2022 Working knowledge of internet services such as DNS, RADIUS, and LDAP.\u2022 Design, configuration and troubleshooting experience:\u2022 Routing: Virtual interfaces, BGP, OSPF, MPLS, EIGRP, RIP, PBR, QoS, Static Routing, Unsymmetrical Routing, Router NAT, HSRP/VRRP, ACL\u2019s, NTP, SNMP, Password Recovery\u2022 Switching: VLAN, VLAN Bleed, 802.1D, 802.1Q, 802.1X, VTP, Multilayer switching/routing, Multicasting \u2022 Firewalls: Interface priority, public vs Private IP space, Conduits/ACLs, Cryptography, Firewall NAT (static & dynamic), Hot Failover, Common TCP/UDP Ports, IPSEC, VPN, Policy Maps, Logging, Routing\u2022 Load Balancing: Routing, VIPs, Service Interfaces, Certificate Intercept, Owner Mapping, Load Balancing Algorithms, Content Grouping\u2022 Analytical and detailed oriented, with the ability to plan, organize and prioritize work to meet target project dates.\u2022 Independent-thinker and self-starter, who can work well within a team environment.\u2022 Communicates clearly and in an understandable manor with a wide range of people such as managers, customers, vendors, and the general public.\u2022 Demonstrated ability to effectively advice and counsel both managers and non-supervisory employees on sensitive work-related issues.\u2022 Demonstrated analytical and problem-solving skills, and the ability to organize and prioritize several projects and tasks at one time.\u2022 We are seeking a highly skilled and experienced on both On-premises and AWS Cloud Network to join our dynamic team.\u2022 Design, configure, and optimize AWS networking components, such as VPCs, subnets, route tables, and security groups.\u2022 Implement security best practices and compliance standards within the AWS network infrastructure.\u2022 Configure and manage network security groups, network ACLs, and other security features to protect sensitive data and resources.\u2022 Develop and maintain automation scripts and templates for network provisioning, configuration, and scaling using AWS CloudFormation or other infrastructure as code (IAC) tools. Network On-premise\u2022 Maintain comprehensive documentation of network configurations, diagrams, and standard operating procedures (SOPs) to ensure efficient knowledge sharing and compliance.\u2022 Optimize network resources to control costs while maintaining performance and scalability.\u2022 Identify opportunities for cost savings in network architecture and usage\u2022 Monitor network usage trends and plan for capacity expansion or optimization based on business requirements.\u2022 Monitor and optimize network performance, ensuring low latency, high availability, and fault tolerance.\u2022 Implement AWS services like Amazon CloudFront, Elastic Load Balancing (ELB), and Amazon Route 53 for optimal traffic distribution.\u2022 Design, configure and monitor network equipment, network usage and systems availability.\u2022 Support and troubleshoot various IP technologies, including but not limited to: Network connectivity, IP addressing, Routing Protocols (BGP, OSPF), HSRP, VPNs, IPSec, Load balancing, QOS, SNMP and 802.11x\u2022 Provide configuration and troubleshooting support for Ethernet Switches, Routers, Load-Balancers, VPN Concentrators, Firewalls, and networked servers.\u2022 Provide network management support as needed for event identification, event correlation, event escalation and event triage as related to network infrastructure.Cisco Certified Network Professional (CCNP), AWS Certified Advanced Network, Firewall, Load Balancing, Routing, Switching,I would also appreciate it if you could send me an updated resume. Best Regards, Sukumar Sr Technical RecruiterE: sukumar@gacsol.com www.gacsol.com www.muffins.ai \u2018Experts in Digitalization and Platform Engineering - Enterprise 4.0\u2019\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "446_Cloud Enterprise Architect - Raritan, NJ (Onsite)": "Manish,\nepeopletech\nmanish@epeopletech.com\nReply to: manish@epeopletech.com\nHi,We have an urgent opening with Cloud Enterprise Architect and I have sent you job description, please go through it and let me know your comfort with it and also send me your updated resume ASAP Role - Cloud Enterprise ArchitectLocation \u2013 Raritan, NJ (4-5 Days onsite)Duration: Contract Cloud Enterprise Architect:Jd:Should have strong Google Cloud Platform (GCP) experience with all the different components implementation experience like Data Flow, Vertex AIStrong implementation experience in Micro Services ArchitectureExcellent knowledge of cloud computing technologies and current computing trends in GCP & AWSDevelop and maintain data management standards, policies, and guidelines.Assist in the development of overall project plans and timetables, analysis and identification of intermediate deliverables.Designing and implementing integration solutions that meet business needsProvide thought leadership; identify and analyze technical innovations and concepts supporting business objectives.Ability to write technical and design documents for proposed solutionShould have good understanding on the Cloud Infrastructure implementations and API integrations across the business programBe informed on current and emerging technological trends and communicate them with business to drive new opportunitiesFacilitate the establishment and execution of the roadmap and vision for information delivery and data management for analytics programsProvide advice, guidance, direction, and authorization to carry out major business program plansShould be a Computer Science Technical Graduate (B.E / B.Tech) or Masters in Computer Sciences. Thanks & Regards Manish SharmaePeople Technologies Inc255 Baldwin Road, Suite 205,Parsippany, NJ 07054Email:- Manish@epeopletech.comLinkedin: linkedin.com/in/manish-sharma-46aa1a21ahttp://www.epeopletech.comP Go Green! Please do not print this e-mail unless necessary Note: ePeople Technologies Inc is an Equal Employment Opportunity employer. All qualified applicants will receive consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence, and performance of the essential functions of their positions. We promote and support a diverse workforce at all levels in the company. This is not an unsolicited mail and if it is not intended for you or you are not interested in receiving our e-mails please forward this email to with \"remove\" in the subject line and mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are extremely sorry if our email has caused any inconvenience to you.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "447_Title: DevOps with Open shift": "viraat,\nReliant vision\nviraat@reliantvision.com\nReply to: viraat@reliantvision.com\nTitle: DevOps with Open shiftLocation: Berkeley Heights, NJ (5 Days Onsite) Job Description:Must have skills: DevOps with Open Shift\u2022 Build, manage, and deploy CI/CD pipelines.\u2022 Strive for continuous improvement and build continuous integration, continuous development, and constant deployment pipeline.\u2022 Implementing various development, testing, automation tools, and IT infrastructure.\u2022 Optimize and automate release/development cycles and processes.\u2022 Be part of and help promote our DevOps culture.\u2022 Identify and implement continuous improvements to the development practice. What you need to have:\u2022 6+ years expertise with Microsoft Azure cloud-based infrastructure and services\u2022 6+ years hands-on experience with CI/CD (YAML) pipelines for Azure DevOps\u2022 6+years hands-on experience with K8S environment including Helm Charts & Config Maps.\u2022 6+ years sound knowledge on scripting in languages such as PowerShell, bash scripting.\u2022 5+ years' experience in containerization with Azure Kubernetes, Docker etc. \u2022 5+ years' experience with Infrastructure as Code\u2022 5+ years high proficiency with Windows-based systems and Linux-based systems\u2022 Ability to write and update documentation.\u2022 Demonstrate a logical, process orientated approach to problems and troubleshooting.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "448_Sr. Software Engineer Big DATA Engineer Azure DATA Engineer": "Ravindra,\nUnicomtec\nravindra@unicomtec.com\nReply to: ravindra@unicomtec.com\nThis Sr. Software Engineer will join an experienced team focusing on migrating legacy data feeds of merchandising platforms to modern domain architecture, by way of automating processes. This resource will aid in building services and orchestrated views of Kroger\u2019s bulk interfaces which enable consumers of the data to self-serve. This resource will also collaborate with the Tech Lead, team, architects & product managers to deliver on the product roadmap.Required Skills:3 to 6 years of hands-on experience in software development.Proficiency in programming languages such as Java or Python.Experience with streaming technologies like EventHub or Kafka.Ability to write complex SQL queries and create views through table joins.Demonstrated experience working in the Software Development Lifecycle (SDLC).Strong understanding and practical experience in Agile Scrum methodologies.Proficiency in using Jira for project management and Confluence for documentation.Experience with GitHub for version control and building CI/CD pipelines. Preferred Skills:Familiarity with advanced data technologies including Databricks, Azure Data Lake, and Azure Data Factory.Design and implement real-time data pipelines using PySpark, leveraging Spark SQL, DataFrames, and Datasets for handling high-velocity and high-volume data streaming solutions.Experience working with cloud technologies, preferably Azure. Proficiency with Azure Databricks Delta tables, workflows and in Python scripting. Soft Skills and Teamwork:Excellent collaboration and teamwork skills, enthusiastic about participating in mobbing sessions.Strong problem-solving abilities, capable of conducting thorough code reviews.Effective communication and teamwork across multiple teams.Experience in gathering and comprehending requirements.Availability during business hours for ad-hoc calls or meetings.Skillful prioritization and strong analytical, creative thinking.Adaptability to new tech stacks and responsiveness to feedback. With Best RegardsRavindra KumarSr. US IT RecruiterUNICOM Technologies, Inc. A Certified MBE896 S Frontenac st. ste#112, Aurora IL 60504.Phone: 630-741-4464 (Ext 108)|Email : ravindra@unicomtec.com |web : www.unicomtec.comhttps://www.linkedin.com/in/ravindra-ron/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "449_Cloud Network Developer - C2H- No H1B - Need locals to NJ": "Paul,\nhighbrow\npaul.raj@highbrow-tech.com\nReply to: paul.raj@highbrow-tech.com\nJob Title: Cloud Network Developer - C2H- No H1B - Need locals to NJJob ID: 2024-12807Job Location: Mt Laurel, NJ or Toronto, ON (2 days/week onsite)Job Travel Location(s): # Positions: 2Employment Type: C2H / FTE Canada Candidate Constraints: Duration: Long Term# of Layers: 0 Work Eligibility: All Work Authorizations are Permitted - No Visa Transfers Key Technology: DevOps, Cloud Networks, PowerShell, Scripting Job Responsibilities: Develop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform.Collaborate with developers in client cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform.Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture.Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code.Automate the existing code components and integrate technologies to eliminate manual deployment steps.Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Skills and Experience Required: Required:5+ years of automation and IT experience. 3+ years in DevOps and cloud experience.Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages.Experience with automating and integrating Serverless cloud provided PaaS solutions.Ability to troubleshoot code and logic errors for cloud-based network services.Understanding of deployment platforms and databases via CI/CD pipeline.Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms.Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus.Proficiency in cloud automation using cloud native CLI/API.Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP.Must have working experience with Cloud Native Networking technologies.Must have experience using PowerShell to configure Cloud Networking components.Education/Certifications: CISSP, CCSP, Microsoft MCSE Azure \u2013 400 or 500-- Thanks & RegardsPaul Raj | SR US IT Recruiterpaul.raj@highbrow-tech.comHighbrow LLChttps://highbrow-tech.com/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "450_Azure Cloud Engineer -- 1 year Contract -- Princeton, NJ OR New York, NY (Onsite -- Only Local)": "Balaji,\nLorven Technologies\nbalaji.ke@lorventech.com\nReply to: balaji.ke@lorventech.com\nJob Title: Azure Cloud EngineerLocation: Princeton, NJ / New York, NY (Onsite)Duration: 1 year ContractJob Description:Azure Cloud Engineer 4-5 Years Azure experienceWorked on Azure APIMFront DoorApp GatewayLoad BalancerLogic AppsFunction AppsStorageKey VaultsARM templatesDockerKubernetesGH ActionsAzure DevOPs etc.Warm Regards,Balaji K E | Technical RecruiterCall: 609-732-3209 Email: balaji.ke@lorventech.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "451_Developer DevOps Security Tools - Tampa, FL": "Ragul Saravanan,\nKK Software Associates\nragul.s@kksoftwareassociates.com\nReply to: ragul.s@kksoftwareassociates.com\nHi Everyone,Hope you are doing well,Please shre profiles to ragul.s@kksoftwareassociates.com.Role name:DeveloperRole Description:Good Communication SkillCompetencies:Digital : DevOps, Digital : DevOps Security ToolsExperience (Years):6-8Essential Skills:Good Communication SkillDesirable Skills:Good Communication SkillCountry:United StatesBranch | City | Location:Boynton Beach,FLTAMPATampa, FLKeywords:Good Communication SkillRegards,Ragul Saravanan |Technical Recruiter | KK Software Associates (Global) Pvt Ltd.Contact: +1 (469) 343-1682555 Metro Place North, Suite 100 Dublin, OH 43017 | 8751 Collin-McKinney Pkwy, #1302, McKinney, TX 75070Bangalore | Pune | Hyderabad | Chennai | NoidaEmail Id: ragul.s@kksoftwareassociates.com URL: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "452_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "453_Devops Engineer": "bhavani,\nBrillius\nbhavanip@brillius.com\nReply to: bhavanip@brillius.com\nRole: Devops Engineer (Need H1B, PP NUM MUST & Need Certifications)Location: Austin, TX / Sunnyvale, CAExperience (Years): 8-10 Role Description: Experience in at-least one Container Orchestration Tool - Kubernetes / Docker Swarm / Nomad-Consul \u2022 Experience in Devops Tools like Jenkins and Docker. \u2022 Experience with Scripting. Should have ability to automate repeated tasks. (Python preferred) \u2022 Experience with Server-Side technologies such as Nginx, Haproxy, Apache. \u2022 Experience with Configuration Management tools - Puppet/Ansible/Terraform. \u2022 Strong knowledge in AWS cloudKeywords: DevOps\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "454_Devops Engineer- Mainframe Exp- Rexx, JCL, IBM DB2": "Navneet,\nSiriinfo\nnavneet.jha@siriinfo.com\nReply to: navneet.jha@siriinfo.com\nHello, My Name is Navneet Jha. I am reaching out to you today because we have your resume in our database and wanted to check if you are currently available in the market actively looking for an opportunity. Please see the below mentioned requirement and I believe you would be a good match for the role. I would appreciate if you could respond with your updated resume along with the good time to speak so we can move forward with the process. Role: Devops Engineer- Mainframe Exp- Rexx, JCL, IBM DB2Location: Boston, MA - onsiteDuration: Contract to HireMode: VideoJob Details: Role Description: Job Description \u2013 zDevOps engineerDuties and responsibilities:Senior DevOps and Automation Engineer for State Street\u2019s Platform Engineering department Specific duties of the position include:\uf0a7 Implement a DevOps platform for supporting the needs of developers building applications for the mainframe\uf0a7 Help design and integrate a DevOps tool chain that includes Issue tracking, IDE, source code management, CICD, test automation and code analysis.\uf0a7 Help maintain the existing legacy DevOps tool chain (Based on IBM Engineering Workflow Management or RTC) for the duration of the migration project\uf0a7 Demonstrates proficiency in all aspects of the software life cycle, such as SCM/Build/Release/Deploy, and has specialized knowledge of tools like Git, Jenkins or Harness\uf0a7 Writes scripts in Python, Java, Shell scripts, Rexx, JCL, Ant, Ansible and other scripting technologies for automating integration and custom extensions.\uf0a7 Has excellent communications & presentation skills \u2013 able to explain complex tasks to management\uf0a7 Builds relationship and collaborates with technical & business teams to deliver projects on time Hybrid remote telecommuting permitted pursuant to Company policy.Education and experience requirement(s): \uf0a7 Masters\u2019s degree or its equivalent in Computer Science, Information Technology, or a related technical field, plus\uf0a7 5 years of experience as a Platform Engineer or other occupation/title providing relevant work experience with DevOps, mainframe, Automation and CI/CD/CT technologies. Special skills, knowledge, or certification and/or licensure required for position:Total experience above must include the following (can be gained concurrently):\u2022 Demonstrated familiarity with mainframe, including Unix and MVS file systems, mainframe COBOL and PL1 compilers/binder/DB2, 3270 emulators\u2022 Demonstrated familiarity with DevOps tools such as GitHub, Harness or Jenkins, Artifactory\u2022 Demonstrated coding skills in Java, Python, scripting languagesSpecial skills, knowledge, or certification and/or licensure preferred for position:\u2022 Knowledge of IBM Developer for z, Rational Team Concert, Dependency Based Build, Wazi Deploy, BMC DevX Code Pipeline, UCD, ISPW, Topaz\u2022 Knowledge of IBM HTTP Server and Websphere Liberty configuration/administration\u2022 Knowledge of Eclipse or VSCode with skills to code plugins/extensions if needed. Best Regards,Navneet JhaEmail: navneet.jha@siriinfo.comSiri InfoSolutions Inc, 3 Ethel Rd, Suite # 302, Edison NJ 08817. CPUC CertifiedWe respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "455_Azure Tech Lead || No DevOps Cloud Engineers OR H1B CPT || PP Required": "Awkash Kumar,\nLargeton INC\nawkash@largeton.us\nReply to: awkash@largeton.us\nJD :: Client may call onsite. Candidates should be local to Massachusetts US We\u2019re looking for someone who has worked in below skills: \u2022 ADF \u2022 Databricks \u2022 Log Analytics \u2022 Purview \u2022 DevOps \u2022 Team Management \u2022 Production Support Management Production Support management experience is a must to have. Responsibilities \u2022 Experience on ADLS, Azure Databricks, Azure SQL DB and Data warehouse(Azure Synapse Analytics) \u2022 SQL Server development, Azure Data Factory, Azure Automation, Power-shell scripting, SQL databases \u2022 Python scripting, Spark SQL & PySpark, Knowledge in ETL tools (SSIS, Talend etc) \u2022 Have in-depth ETL Processing \u2022 Good expertise in Data warehousing/Dimensional Modelling \u2022 Have knowledge in Azure Storage services (ADLS, Storage Accounts) \u2022 Handle Data Ingestion projects in Azure environment\u2022 Experience in Production Support as a Lead role managing all aspect of Production support ( L1/L2/L3) \u2022 Experience in Transition any Production support from Incumbent . \u2022 Operation and Performance reporting of Production activities \u2022 Automation or Drive Impact to clients while managing applications. \u2022 Lead Enhancement and Forward Dev as part of Production Support \u2022 Experience in Team Management \u2013 Onshore /Offshore model \u2022 CI/CD or Devops experience . \u2022 Release Management \u2022 Experience in working Agile Framework - Managing Sprints Qualifications we seek in you! Minimum qualifications \u2022 Domain \u2013 Consumer Goods, Life Sciences, Manufacturing, Banking& Capital Markets \u2022 Azure certified data engineering professional Preferred Qualifications \u2022 Ability to communicate efficiently with customer\u2019s key Business and IT folks to present/defend architecture/design \u2022 CI/CD- Azure Pipeline \u2022 Outstanding grasp on Azure Monitor, Redis Cache, Load Balancer, Application Gateway, Azure Functions, Azure Data Factory Integrations \u2022 Knowledge of microservices and API development \u2022 Excellent analytical, problem solving, communication and ability to communicate efficiently with individuals, business and can work as part of a team as well as independently. \u2022 Good knowledge of CI/CD pipelines such as Jenkins \u2022 Experience No SQL and Document databases\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "456_DevOps with Mainframe Experience - Boston, MA": "Mahesh Kumar,\nKK Associates LLC\nshetty.m@kksoftwareassociates.com\nReply to: shetty.m@kksoftwareassociates.com\nHi, We have a job opportunity with one of our clients. If you have any matching profiles please get in touch with me.Role name:EngineerRole Description:Job Description \u2013 zDevOps engineerDuties and responsibilities:Senior DevOps and Automation Engineer for State Street\u2019s Platform Engineering department Specific duties of the position include:\u00a7 Implement a DevOps platform for supporting the needs of developers building applications for the mainframe\u00a7 Help design and integrate a DevOps tool chain that includes Issue tracking, IDE, source code management, CICD, test automation and code analysis.\u00a7 Help maintain the existing legacy DevOps tool chain (Based on IBM Engineering Workflow Management or RTC) for the duration of the migration project\u00a7 Demonstrates proficiency in all aspects of the software life cycle, such as SCM/Build/Release/Deploy, and has specialized knowledge of tools like Git, Jenkins or Harness\u00a7 Writes scripts in Python, Java, Shell scripts, Rexx, JCL, Ant, Ansible and other scripting technologies for automating integration and custom extensions.\u00a7 Has excellent communications & presentation skills \u2013 able to explain complex tasks to management\u00a7 Builds relationship and collaborates with technical & business teams to deliver projects on time Hybrid remote telecommuting permitted pursuant to Company policy.Education and experience requirement(s): \u00a7 Masters\u2019s degree or its equivalent in Computer Science, Information Technology, or a related technical field, plus\u00a7 5 years of experience as a Platform Engineer or other occupation/title providing relevant work experience with DevOps, mainframe, Automation and CI/CD/CT technologies. Special skills, knowledge, or certification and/or licensure required for position:Total experience above must include the following (can be gained concurrently):\u2022 Demonstrated familiarity with mainframe, including Unix and MVS file systems, mainframe COBOL and PL1 compilers/binder/DB2, 3270 emulators\u2022 Demonstrated familiarity with DevOps tools such as GitHub, Harness or Jenkins, Artifactory\u2022 Demonstrated coding skills in Java, Python, scripting languagesSpecial skills, knowledge, or certification and/or licensure preferred for position:\u2022 Knowledge of IBM Developer for z, Rational Team Concert, Dependency Based Build, Wazi Deploy, BMC DevX Code Pipeline, UCD, ISPW, Topaz\u2022 Knowledge of IBM HTTP Server and Websphere Liberty configuration/administration\u2022 Knowledge of Eclipse or VSCode with skills to code plugins/extensions if needed.Competencies:Digital : Cloud DevOps, Mainframe DB2 - Application DevelopmentExperience (Years):4-6Essential Skills:Job Description \u2013 zDevOps engineerDuties and responsibilities:Senior DevOps and Automation Engineer for State Street\u2019s Platform Engineering department Specific duties of the position include:\u00a7 Implement a DevOps platform for supporting the needs of developers building applications for the mainframe\u00a7 Help design and integrate a DevOps tool chain that includes Issue tracking, IDE, source code management, CICD, test automation and code analysis.\u00a7 Help maintain the existing legacy DevOps tool chain (Based on IBM Engineering Workflow Management or RTC) for the duration of the migration project\u00a7 Demonstrates proficiency in all aspects of the software life cycle, such as SCM/Build/Release/Deploy, and has specialized knowledge of tools like Git, Jenkins or Harness\u00a7 Writes scripts in Python, Java, Shell scripts, Rexx, JCL, Ant, Ansible and other scripting technologies for automating integration and custom extensions.\u00a7 Has excellent communications & presentation skills \u2013 able to explain complex tasks to management\u00a7 Builds relationship and collaborates with technical & business teams to deliver projects on time Hybrid remote telecommuting permitted pursuant to Company policy.Education and experience requirement(s): \u00a7 Masters\u2019s degree or its equivalent in Computer Science, Information Technology, or a related technical field, plus\u00a7 5 years of experience as a Platform Engineer or other occupation/title providing relevant work experience with DevOps, mainframe, Automation and CI/CD/CT technologies. Special skills, knowledge, or certification and/or licensure required for position:Total experience above must include the following (can be gained concurrently):\u2022 Demonstrated familiarity with mainframe, including Unix and MVS file systems, mainframe COBOL and PL1 compilers/binder/DB2, 3270 emulators\u2022 Demonstrated familiarity with DevOps tools such as GitHub, Harness or Jenkins, Artifactory\u2022 Demonstrated coding skills in Java, Python, scripting languagesSpecial skills, knowledge, or certification and/or licensure preferred for position:\u2022 Knowledge of IBM Developer for z, Rational Team Concert, Dependency Based Build, Wazi Deploy, BMC DevX Code Pipeline, UCD, ISPW, Topaz\u2022 Knowledge of IBM HTTP Server and Websphere Liberty configuration/administration\u2022 Knowledge of Eclipse or VSCode with skills to code plugins/extensions if needed.Desirable Skills:Job Description \u2013 zDevOps engineerDuties and responsibilities:Senior DevOps and Automation Engineer for State Street\u2019s Platform Engineering department Specific duties of the position include:\u00a7 Implement a DevOps platform for supporting the needs of developers building applications for the mainframe\u00a7 Help design and integrate a DevOps tool chain that includes Issue tracking, IDE, source code management, CICD, test automation and code analysis.\u00a7 Help maintain the existing legacy DevOps tool chain (Based on IBM Engineering Workflow Management or RTC) for the duration of the migration project\u00a7 Demonstrates proficiency in all aspects of the software life cycle, such as SCM/Build/Release/Deploy, and has specialized knowledge of tools like Git, Jenkins or Harness\u00a7 Writes scripts in Python, Java, Shell scripts, Rexx, JCL, Ant, Ansible and other scripting technologies for automating integration and custom extensions.\u00a7 Has excellent communications & presentation skills \u2013 able to explain complex tasks to management\u00a7 Builds relationship and collaborates with technical & business teams to deliver projects on time Hybrid remote telecommuting permitted pursuant to Company policy.Education and experience requirement(s): \u00a7 Masters\u2019s degree or its equivalent in Computer Science, Information Technology, or a related technical field, plus\u00a7 5 years of experience as a Platform Engineer or other occupation/title providing relevant work experience with DevOps, mainframe, Automation and CI/CD/CT technologies. Special skills, knowledge, or certification and/or licensure required for position:Total experience above must include the following (can be gained concurrently):\u2022 Demonstrated familiarity with mainframe, including Unix and MVS file systems, mainframe COBOL and PL1 compilers/binder/DB2, 3270 emulators\u2022 Demonstrated familiarity with DevOps tools such as GitHub, Harness or Jenkins, Artifactory\u2022 Demonstrated coding skills in Java, Python, scripting languagesSpecial skills, knowledge, or certification and/or licensure preferred for position:\u2022 Knowledge of IBM Developer for z, Rational Team Concert, Dependency Based Build, Wazi Deploy, BMC DevX Code Pipeline, UCD, ISPW, Topaz\u2022 Knowledge of IBM HTTP Server and Websphere Liberty configuration/administration\u2022 Knowledge of Eclipse or VSCode with skills to code plugins/extensions if needed.Country:United StatesBranch | City | Location:Boston, MABOSTONBoston, MAKeywords:DevOps with Mainframe ExperienceEmail is the best way to reach me if I missed your callRegards,Mahesh KumarKK Associates LLC.8751 Collin McKinney Pkwy, # 1302, McKinney, TX 75070555 Metro Place North, Suite # 100, Dublin, OH 43017Email: shetty.m@kksoftwareassociates.comWeb: www.kksoftwareassociates.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "457_AWS Network CLoud Engineer       Remote       USC GC Only": "Ankit upadhyay,\nPivotal Technologies\nankit.upadhyay@pivotal-technologies.com\nReply to: ankit.upadhyay@pivotal-technologies.com\nThis role has been open before for Hitachi Consulting. If the candidate has been submitted to Hitachi in the past, don\u2019t submit.Candidates have failed with their Palo Alto and Panorama experience, they must have this with strong AWS. Company: Hitachi ConsultingLocation: Remote, mostly ES, but could be all 3Pay Rate in the $60+/hr rangeDuration: 14+ weeks MUST be a US Citizen or Green Card and show Passport or proof of residence Candidate MUST haves:AWS Infrastructure - Strong experience on AWS Infrastructure services (EC2, ELB, RDS, Route53, S3, vpc, vpn, tgw, cloudwatch, cloudtrail, eventbridge, etc. Strong Networking with Palo Alto Networking with Panorama Automation with TerraformHands on experience in Terraform IaC deployments. Strong Experience in building CI/CD (Gitlab, Jenkins) and deployment automation Title: - AWS DevOps Engineer with PaloAlto Firewall SkillsetEnd Client: Verizon Description Of Services:AWS Cloud Engineer with PaloAlto Firewall skillset What You will need (Qualifications) AAt least 3+ years of experience in AWS with hands-on contribution in DevOps engineering. Strong track record of implementing AWS services in a variety of distributed computing environments (Design and implementation experience and Cloud Networking). Strong experience on AWS Infrastructure services (EC2, ELB, RDS, Route53, S3, vpc, vpn, tgw, cloudwatch, cloudtrail, eventbridge, etc. Large scale implementation and migration experience of Data Center to Cloud Infrastructure automation through DevOps scripting (Ansible, Shell, Python, Ruby, PowerShell) Hands on experience in Terraform IaC deployments. Strong Experience in building CI/CD (Gitlab, Jenkins) and deployment automation. Good practical Linux / Windows-based systems administration skills in a Cloud or Virtualized environmentExperience working on FedRamp compliant projects is a plus. Must be a GC or US Citizen. Palo Alto & Panaroma Skillset requirements:Design and architect Palo Alto firewall solutions for AWS cloud-based infrastructures based on business requirements and security best practices.Ability to independently deploy Palo Alto firewalls & Panorama in cloud environments using IaC automation, ensuring proper network segmentation and security zones. Strong hands-on experience using Panorama to configure security policies, NAT rules, VPN tunnels, and other firewall settings to protect cloud assets.Implement high availability and failover configurations for firewall devices. Develop and enforce security policies, including threat prevention, URL filtering, application control, and user identification.Ability to Integrate Panorama with other security tools and systems, such as SIEM platforms, to enhance threat detection and response capabilities. Palo Alto Networks certifications (e.g., PCNSA, PCNSE) are highly desirable. What You Bring To The Team Can work autonomously, deliver with minimal supervision from a set of requirements Demonstrated ability to think strategically about business, product, and technical challenges Has excellent communication skills to work as a member of a team Ability to function in an agile-based environment and provide good daily feedback on team stand-up call Participate in code release and production deployment, have an aggressive approach to fixing bugs and defects Deliverables:-Process Flows -Mentor and Knowledge transfer to client project team members -Participate as primary, co and/or contributing author on any and all project deliverables associated with their assigned areas of responsibility -Participate in data conversion and data maintenance -Provide best practice and industry specific solutions -Advise on and provide alternative (out of the box) solutions -Provide thought leadership as well as hands on technical configuration/development as needed. -Participate as a team member of the functional team -Perform other duties as assigned. Acceptance Criteria:must be US Citizen or Green Card Holder Thanks & Regards, ANKIT UPADHYAY Technical RecruiterOffice: +1 (703) 570-8775 (Ext-217)Email- Ankit.Upadhyay@pivotal-technologies.comConnect with me:-- linkedin.com/in/ankit-upadhyay-a689a1232\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "458_Job Opening for Senior Cloud Engineer (Lead) In Atlanta, GA (Onsite)": "Rakesh Sharma,\nDatum Software\nrakesh.sharma@datumsoftware.com\nReply to: rakesh.sharma@datumsoftware.com\nPlease find the job description below: Job Details:Senior Cloud EngineerLong term contractAtlanta, GA (Onsite) Description:Provide technical leadership and design expertise for AWS Foundation integration.Define standards and patterns for when and how to efficiently leverage AWS services.Lead automation of AWS integration processes and tasks.Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners.Develop frameworks and tools to enable dev teams to consume authorized AWS services.Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins.Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS.Ensure Security-by-Design is a foundational component of every app deployed into AWS.Ensure enterprise monitoring standards are applied to each AWS app deployment.Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration.Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies).Perform cost benefit analysis to determine best system architecture via software, hardware, internal cloud, externally hosted or other defined model.Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs.Partner with peer Infrastructure teams to provide enterprise class AWS integration.Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designs. Quals--4+ years of experience in design and implementation of large enterprise public cloud environments .AWS Certified Professional preferred.2+ years of hands-on experience integrating mission critical applications into AWS Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.) Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools. Fluency with one or more scripting/coding languages such as Java, Javascript, REST, JSON, Python, bash Experience implementing and leveraging the logging and monitoring solutions.Experience in cloud native architectures and micro-services design - Understanding of the shared responsibility model in AWS Proven record of accomplishments in major enterprise level projects - Experience in design of complex distributed systems environments.Demonstrated ability to think strategically about business, product, and technical challenges - Ability to clearly communicate and present to various levels of the organization Strong organizational and analytical skills with attention to detail Independent and self-motivated and very thorough worker WHAT ELSE? Candidate must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "459_Urgent hiring for Azure Architect (E4)": "padmavathi,\nYochana IT solutions\npadmavathi@yochana.com\nReply to: padmavathi@yochana.com\nHello,I hope you are doing great.This is Padmavathi from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your timeTitle : Azure Architect (E4)Location : Dallas, Texas - Hybrid1. Technical Architect \u2013 Azure IaaS (L4)HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. o Position - Solutions Architect (Azure)o Designation - Consultant/Senior Consultant/Solutions Architecto Experience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions \u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions \u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction. \u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements \u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring \u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. \u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred. \u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Thanks & Regards,Padmavathi Vindeepu,Resource Specialist, Farmington Hills, MI,Email Id: padmavathi@yochana.comhttps://www.linkedin.com/in/padmavathi-vindeepu-25b540261/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "460_Urgent hiring for Cloud Architect": "padmavathi,\nYochana IT solutions\npadmavathi@yochana.com\nReply to: padmavathi@yochana.com\nHello,I hope you are doing great.This is Padmavathi from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your timeTitle : Cloud ArchitectLocation : Clayton, Missouri (Hybrid) Role/ JD:HCL Cloud CoE (Practice) team is looking for an experienced customer-focused Azure Solution Architect with technical depth and architecture skills. In this role, you\u2019ll work closely with pre-sales, business units and engineering on some of HCL's largest and most strategic customer engagements to help identify, define, and architect solutions on Azure. Specifically, you will be part of a customer engaged team of technical PMs and engineers responsible for implementing new landing zone(s) and execute POCs and pilot migration projects. o Position - Solutions Architect (Azure)o Designation - Consultant/Senior Consultant/Solutions Architecto Experience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company. Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions \u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions \u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction. \u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements \u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring \u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents. \u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred. \u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers) Thanks & Regards,Padmavathi Vindeepu,Resource Specialist, Farmington Hills, MI,Email Id: padmavathi@yochana.comhttps://www.linkedin.com/in/padmavathi-vindeepu-25b540261/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "461_SLG - 19 - 9686 - Senior DevOps Engineer - SLG-1316-Remote": "Srikanth Vanam,\nSligosoft\nsrikanth.vanam@sligosoft.com\nReply to: srikanth.vanam@sligosoft.com\nGreetings from Sligo Software Solutions, Inc. !!SLIGO is a Software Development & IT Consulting company. We have around 100+ IT associates working with our esteemed clients across several domains such as IT, Banking, Healthcare, and Government. We have an urgent requirement for Senior DevOps Engineer position with our client. Role: Senior DevOps EngineerLocation: N/A, New YorkDuration: 12 MonthsJob ID: SLG - 19 - 9686Client: NY StateRespond by Date: 07/22/24 1:00 PM Mandatory RequirementsExperience ( 15+Years)Duties: Develop automation scripts and CI/CD pipelines across various Azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.Participate in cloud architecture and design sessions for CI/CD pipelines.Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Update build agents, extend Terraform and Ansible scripts.Configure and manage cloud security and policy components.Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments. Mandatory Qualifications: At least 12 years of experience in software development and DevOps tools.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Hands-on expertise in supporting and developing Java applications and Microservices.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, Python, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Knowledge of Terraform, Ansible, Helm, Kubernetes.Self-motivated with a proven track record as a team player.Please have EDUCATION details on all resumes.NO LAYERS PLEASE \u2013 U.S. Citizens, GC holders or YOUR H1-B candidate that can produce a 797A document.All candidates must be first screened by me then be screened Desirable Qualifications: \u2022 Knowledge of AWS and Azure Networking.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "462_AWS DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nDescriptionThis position can be worked remotely anywhere in the United States. ICF is a mission-driven company filled with people who care deeply about improving the lives of others and making the world a better place. Our core values include Embracing Difference; we seek candidates who are passionate about building a culture that encourages, embraces, and hires dimensions of difference. Our Health Engineering Solutions team works side by side with customers to articulate a vision for success, and then make it happen. We know success doesn't happen by accident. It takes the right team of people, working together on the right solutions for the customer. We are looking for a seasoned DevOps Engineer who will be a key driver to make this happen. In this position, you will be part of the team building best in class health care reporting service. Learn and grow using AWS Infrastructure, DevSecOps, Agile Scrum, incremental delivery philosophy with highly supportive peers constantly sharing subject matter expertise. Our core work hours are 10am - 4pm Eastern Time with the option to start earlier or work later depending on your time zone. ResponsibilitiesImplement best in class cloud-based solutions in AWS using infrastructure as codeDeploy, setup, and run infrastructure configurations for various AWS services, utilizing Infrastructure as Code such as TerraformEngage with technical stakeholders including but not limited to application development, networking, infrastructure, information security, risk, enterprise identity and access management, and security operationsEnable and optimize the automation of application and infrastructure environmentsBe part of a team where you collaborate to build cloud infrastructure, with an understanding of AMI, Containers and serverless functionsDevelop, maintain and improve continuous integration/continuous delivery (CI/CD) pipelines for delivering features, fixes and system updates in development, integration and production environments.Set up, integrate, and maintain a scalable, stable set of CI/CD tools to support development, testing, and security scanning.Implement Amazon CloudWatch, Splunk and other third party monitoring solutions to provide continuous monitoring capabilities, track all aspects of the system, infrastructure, performance, application errors and roll up metrics.Analyze functional and non-functional business requirements, translate them into technical operational requirements, and propose CI/CD pipelines with tools and plugins.Making a big impact as part of a small team that s pushing boundaries. Required QualificationsBachelor's degree in computer science: Information Systems, Engineering or other related scientific or technical discipline3+ years of experience in setting up CI/CD Pipelines with integration with open-source plugins.3+ years of experience in DevOps/Agile/Scrum environments and development.3+ years of strong hands-on experience with configuration management, cloud orchestration and automation tools with AWS environments.3+ years experience with provisioning and managing infrastructure as well as applications in AWS cloud environments.2+ years of experience with identifying and implementing automation for Continuous Integration/Continuous Deployment.2+ years experience writing infrastructure as code using Terraform Professional SkillsOutstanding writing and verbal communication abilitiesMeticulous attention to detailWorking knowledge of LinuxCandidate must be able to obtain and maintain a Public Trust clearanceCandidate must reside in the US, be authorized to work in the US, and work must be performed in the USMust be able to travel approximately 5% Preferred SkillsDesign and implement automated monitoring capabilities to generate dashboards with trends, useful messages, and immediate notifications, and provide real-time metrics using Splunk or similar services.Knowledge of multi-account architecture, leveraging tools such as AWS Control Tower, SCPs, GuardRails, and Transit GatewaysWide technology experience that may include cloud architecture, cloud migrations, applications development, networking, security, storage, analytics, or machine learningAWS Solution Architect (Associate or Pro) certification.Familiar with standard concepts, practices, and procedures such as NIST, FISMA, FedRamp and Common Criteria regulations and standards.Familiarity with the MLOps, machine learning lifecycle and product landscape, for example: Amazon SageMaker, Apache Airflow, Looker, Trifacta etc. You don't need to be an expert in all these. Job LocationThis position requires that the job be performed in the United States. If you accept this position, you should note that ICF does monitor employee work locations and blocks access from foreign locations/foreign IP addresses, and also prohibits personal VPN connections. #Indeed#LinkedIn Working at ICF ICF is a global advisory and technology services provider, but we re not your typical consultants. We combine unmatched expertise with cutting-edge technology to help clients solve their most complex challenges, navigate change, and shape the future. We can only solve the world's toughest challenges by building an inclusive workplace that allows everyone to thrive. We are an equal opportunity employer, committed to hiring regardless of any protected characteristic, such as race, ethnicity, national origin, color, sex, gender identity/expression, sexual orientation, religion, age, disability status, or military/veteran status. Together, our employees are empowered to share their expertise and collaborate with others to achieve personal and professional goals. For more information, please read our EEO & AA policy. Reasonable Accommodations are available, including, but not limited to, for disabled veterans, individuals with disabilities, and individuals with sincerely held religious beliefs, in all phases of the application and employment process. To request an accommodation please email icfcareercentericf.com and we will be happy to assist. All information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. Read more about workplace discrimination rights, the Pay Transparency Statement, or our benefit offerings which are included in the Transparency in (Benefits) Coverage Act. Pay Range - There are multiple factors that are considered in determining final pay for a position, including, but not limited to, relevant work experience, skills, certifications and competencies that align to the specified role, geographic location, education and certifications as well as contract provisions regarding labor categories that are specific to the position. The pay range for this position based on full-time employment is:$76,848.00 - $130,642.00 Nationwide Remote Office (US99)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "463_Devops Engineer AWS AZURE": "Ramakanth,\nCaptrasolutions\nhr@captrastaffing.com\nReply to: hr@captrastaffing.com\nDevOps Engineer RemoteContractThis is what you will be doing:Designing, building & managing secure platforms in both Windows & Linux environments.Automating infrastructure to improve efficiency.Building CI / CD pipelines (Jenkins, Git or similar).Deploy and maintain monitoring and observability solutions.Working closely with customer teams to ensure projects are on-schedule & to agreed specification.Required Skills:Windows & Linux Systems Administration.AWS/AZURE Experience neededAutomation tools: Ansible, Terraform, Puppet (or similar).Scripting: PowerShell, Bash, Python (or similar).CI / CD tools: Jenkins, Git (or similar).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "464_DevOps Engineer": "Pallavi,\nnss\nrec1@nityainc.com\nReply to: rec1@nityainc.com\nAbbott is a global healthcare leader that helps people live more fully at all stages of life. Our portfolio of life-changing technologies spans the spectrum of healthcare, with leading businesses and products in diagnostics, medical devices, nutritionals and branded generic medicines. Our 114,000 colleagues serve people in more than 160 countries. Working at Abbott At Abbott, you can do work that matters, grow, and learn, care for yourself and your family, be your true self, and live a full life. You ll also have access to:Career development with an international company where you can grow the career you dream of.Free medical coverage for employees* via the Health Investment Plan (HIP) PPOAn excellent retirement savings plan with a high employer contributionTuition reimbursement, the Freedom 2 Save student debt program, and FreeU education benefit - an affordable and convenient path to getting a bachelor s degree.A company recognized as a great place to work in dozens of countries worldwide and named one of the most admired companies in the world by Fortune.A company that is recognized as one of the best big companies to work for as well as the best place to work for diversity, working mothers, female executives, and scientists. The Opportunity The DevOps Engineer is a creative operations professional who defines and implements Software Development Life Cycle (SDLC) technical solutions to support teams using Agile methodologies. Key areas include code repository management, database object synchronization, environment configuration management, release management, and continuous integration/delivery. The DevOps Engineer strives to create efficiency, enforce accuracy, and use technological innovation to streamline processes and systems. The DevOps team's focus is to implement and maintain best practices and constant pursuit of integrated, transparent, fluid, valuable, practical, and systematic enhancements to the eScreen SDLC process. What You ll Work OnCode repository management, including code merging, branching, permissions, and maintenanceManages the promotion of database objects and synchronization across database environments.Manage multiple environment configurations.Release management to the Development, QA, and Production environments.Researches and implements new technology and practices.Documents processes, systems, and workflows.Actively pursues collaboration, open communication, and cross-functional participation.Works with Operations Support to improve monitoring mechanisms to react to application failures before they are reported.Educates team members on DevOps processes, systems, and workflows.May be required to work nights and weekends as needed to support releases for production deployments during scheduled maintenance windows.May assume technical lead role on projects and may provide technical direction to application development teams.Travel is rarely expected for this position. Required QualificationsBachelor s degree in Computer Science, or related degree, or equivalent work experience.Experience with Azure DevOps, MS-Build, or Release Management Tools is required.Experience in IIS Configuration Management is required.Experience with Continuous Integration / Continuous Delivery (CI/CD) PREFERRED QUALIFICATIONS:Experience with Windows Server Configuration Management is preferred.Knowledge of MS SQL Server and RedGate SQL Tools preferred.Demonstrate the ability to set and meet tight deadlines and function well under pressure.Ability to work in a dynamic and fast-paced environment.Strong problem-solving skills.Working knowledge of Windows operating systems and Microsoft Office applications including Outlook, Word, Excel, Visio, PowerPoint, etc. Participants who complete a short wellness assessment qualify for FREE coverage in our HIP PPO medical plan. Free coverage applies in the next calendar year.Learn more about our health and wellness benefits, which provide the security to help you and your family live full lives: www.abbottbenefits.comFollow your career aspirations to Abbott for diverse opportunities with a company that can help you build your future and live your best life. Abbott is an Equal Opportunity Employer, committed to employee diversity.Connect with us at www.abbott.com, on Facebook at www.facebook.com/Abbott, and on Twitter AbbottNews. The base pay for this position is $57,300.00 $114,700.00. In specific locations, the pay range may vary from the range posted.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "465_Azure Cloud Architect  - Rockville, MD - W2 Contract": "Raghuteja,\nW3Global\nraghu.miriyala@w3global.com\nReply to: raghu.miriyala@w3global.com\nJob : Azure Cloud Architect Location : Rockville, MDClient : Montgomery County Government Job Description: Scope of Work: Under the guidance/direction/supervision of the Cloud Manager, and others as assigned, the Contractor will: Establish a practical approach to supporting an enterprise architecture that starts and ends with a focus on delivering signature-ready, actionable recommendations to business and IT leaders, so they can adjust policies, products, processes, and projects to achieve target business outcomes. \u2022 Develop a Reference Architecture that includes all major functions of infrastructure and how it aligns with the mission and vision of the organization. \u2022 Design and develop a comprehensive cloud architecture strategy with a focus on Microsoft Azure to support the organization's large-scale digital transformation. \u2022 Lead the development and implementation of cloud policies, procedures, and governance models to ensure effective management and security of cloud resources. \u2022 Work closely with IT security teams to establish robust cloud security frameworks that align with industry best practices and regulatory compliance requirements.\u2022 Guide the migration of existing applications and infrastructure to Azure, ensuring minimal disruption to business operations. \u2022 Develop cost management strategies to optimize and control cloud expenses without compromising on service delivery. \u2022 Collaborate with various departments and IT teams to ensure cloud solutions meet functional and technical requirements. \u2022 Establish continuous integration/continuous deployment (CI/CD) pipelines and automation practices to enhance development processes and operational efficiency. \u2022 Mentor and train team members and stakeholders on Azure cloud technologies and best practices. \u2022 Stay abreast of new Azure features and other cloud innovations to keep the organization at the cutting edge of technology advancement. Requirements: \u2022 Minimum of 15 years in IT development/infrastructure, with at least 8 years as a Cloud Architect focused on Microsoft Azure. \u2022 Proven experience in designing and implementing enterprise-scale cloud solutions. \u2022 Deep understanding of Azure architecture, cloud security, governance, and compliance frameworks. \u2022 Strong leadership skills and experience in developing policies and procedures for large-scale organizations. \u2022 Certifications such as Azure Solutions Architect Expert or similar are highly preferred. \u2022 Excellent communication, analytical, and project management skills. Education: Education, including relevant credentials and/or certifications. (NOTE: Documentation/evidence of any job-related credential or certification must be provided) Interviews: Please note that both remote and in-person interviews may be required for this opportunity. Location: This is an on-site/Hybrid position based in Montgomery County, MD.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "466_Azure Cloud Dev Tempe, AZ or Fort Worth, TX": "Pooja,\nICS Globalsoft\npooja@icsglobalsoft.com\nReply to: pooja@icsglobalsoft.com\nJob DescriptionJob Title: Azure Cloud DeveloperLocation: Tempe, AZ (preferred) or Fort Worth, TXVisa: Any VisaDocumentation required to verify all certifications - You will not be subitted to the end client without providing proof of current active CertificationsOverview: We are seeking a talented Azure Cloud Developer to join our team in building and supporting custom-developed applications for payroll and benefits. As we expand our in-house capabilities, we are looking for a mid to senior-level professional who can lead the charge in Azure development and architecture, taking ownership of application support, developing new features, and collaborating closely with our existing team. The ideal candidate will be responsible for designing, developing, and implementing solutions on the Microsoft Azure platform. They will work closely with our Architect and cross-functional teams to understand project requirements and translate them into scalable and secure cloud solutions. Responsibilities:Lead the development and maintenance of custom applications in the Azure cloud environment, ensuring high performance, reliability, and scalability.Collaborate with junior developers to mentor and guide them in Azure development best practices and methodologies.Take ownership of application support, troubleshooting issues, and implementing solutions in a timely manner.Design and develop new features and enhancements based on business requirements, ensuring alignment with overall architecture and best practices.Work closely with cross-functional teams to understand business requirements and translate them into technical solutions.Utilize DevOps toolchains, including Azure DevOps, GitHub, and GitHub Actions, to streamline development processes and ensure continuous integration and delivery.Implement security patterns such as OAuth, SAML, and PingFederate to ensure data protection and compliance with security standards.Utilize monitoring and alerting techniques, including tools such as Dynatrace, Azure Insights, and Azure App Insights, to proactively monitor application performance and identify potential issues.Collaborate with network engineers to implement cloud networking concepts and patterns, such as ExpressRoute and hub-spoke topology, to optimize performance and reliability.Design, develop, and deploy scalable cloud-based solutions using Microsoft Azure services.Collaborate with architects and developers to implement Azure cloud solutions that meet project requirements.Write clean, maintainable, and efficient code adhering to best practices.Configure and manage Azure services such as Azure App Service, Azure Functions, Azure SQL Database, Azure Storage, Azure Kubernetes Service (AKS), etc.Implement security measures to protect cloud environments and data.Monitor and optimize the performance of Azure applications and services.Develop automation scripts for provisioning and managing Azure resources.Troubleshoot issues and provide timely resolutions.Stay updated with the latest Azure features, services, and best practices.Participate in code reviews and provide constructive feedback to team members. Qualifications:Bachelor's degree in Computer Science, Information Technology, or related field.5+ years of experience in Azure cloud development, with expertise in developing and maintaining custom applications.Strong knowledge of Azure DevOps and other DevOps toolchains, with experience in implementing CI/CD pipelines and automation workflows.Experience with monitoring and alerting tools such as Dynatrace, Azure Insights, and Azure App Insights.Proficiency in security patterns such as OAuth, SAML, and PingFederate, with a focus on data protection and compliance.Familiarity with cloud networking concepts and patterns, including ExpressRoute, hub-spoke topology, and VNET integration.Excellent problem-solving skills and attention to detail, with a commitment to delivering high-quality solutions.Strong communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams.Azure Dev and Arch certifications preferred.Preferred Qualifications:Azure certifications such as Microsoft Certified: Azure Developer Associate or Microsoft Certified: Azure Solutions Architect Expert - Documentation required to verify all certificationsExperience with containerization technologies such as Docker and Kubernetes.Familiarity with infrastructure as code (IaC) tools such as Terraform or ARM templates.Experience working in Agile/Scrum environments. Required Skills:Extensive experience in Azure Cloud Development, demonstrating proficiency in building and maintaining custom applications.Proficiency with DevOps toolchains, including Azure DevOps, GitHub, and GitHub Actions, with a proven track record of implementing CI/CD pipelines and automation workflows.Familiarity with Dynatrace Monitoring Tool and Azure Insights for monitoring and alerting purposes.Strong understanding and implementation experience with security patterns such as OAuth, SAML, and PingFederate to ensure data protection and compliance.Hands-on experience with cloud networking concepts and patterns, including ExpressRoute, hub-spoke topology, VNET integration, and Service endpoints.Ability to implement monitoring and alerting techniques using tools such as Dynatrace, Mezmo/LogDNA, Log Analytics, Azure App Insights, Moogsoft, xMatters.Excellent problem-solving skills and attention to detail, with a commitment to delivering high-quality solutions.Effective communication and collaboration skills, with the ability to work closely with cross-functional teams.Azure Dev and Arch certifications preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "467_Hybrid: Lead Cloud Network Engineer:NY+12": "Om verma,\nTek Inspirations LLC\nom.verma@tekinspirations.com\nReply to: om.verma@tekinspirations.com\nJob Description -Local candidates are highly preferred, Please do not mention any exp in the resume that candidate cannot handle by himselfHybrid: Lead Cloud Network Engineer Location: NYC, NY, Mostly Remote with some onsite workDuration: 6+ monthsClient: TD BankExp: more than 12 years of exp, Lead is required but Architect level will be preferred if submitted. A network expert with Developement and cloud GCP experience. someone Comining from Enterprise, Hands-on architecture and automation experience required. Note: please do not copy paste skills and experience from JD into the resume. Job DescriptionRole and ResponsibilitiesDevelop custom code in Terraform and PowerShell to automate build and testing for the cloud network platform.Collaborate with developers in our cloud engineering teams to implement and continuously improve the framework and tools to support self-service automation of the platform.Work with control partners to understand and accommodate the network security requirements for Cloud Network architecture.Collaborate with cloud network engineering to help deploy the Cloud Network architecture through code.Automate the existing code components and integrate technologies to eliminate manual deployment steps.Develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.The engineer will require understanding of both cloud networking and coding to help execute the intended cloud architecture in a secure and maintainable way on existing cloud platforms. Qualifications7+ years of automation and IT experience. 3+ years in DevOps and cloud experience.Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell, and Shell Scripting languages.Experience with automating and integrating Serverless cloud provided PaaS solutions.Ability to troubleshoot code and logic errors for cloud-based network services.Understanding of deployment platforms and databases via CI/CD pipeline.Develop APIs and Webhook for multi-directional integration of cloud orchestration platform with system management systems, DevOps tools, and Cloud platforms.Understanding of agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus.Proficiency in cloud automation using cloud native CLI/API.Demonstrable experience deploying enterprise workloads to Azure/AWS/GCP.Must have working experience with Cloud Native Networking technologies.Must have experience using PowerShell to configure Cloud Networking components.Knowledgeable of cloud and hybrid-cloud implementations including IaaS, PaaS, and SaaS.Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes.A critical thinker with strong research and analytics skillsSelf-motivated with a positive attitude and an ability to work independently and or in a team.Able to work under tight timeline and deliver on complex problems.Bachelor's degree in computer science, engineering or a related field, or equivalent work experienceCertifications:CISSP, CCSP, Microsoft MCSE Azure \u2013 400 or 500 Om VermaTech. Recruitment specialistEmail: om.verma@tekinspirations.comTEK Inspirations LLC : 13573 Tabasco Cat Trail, Frisco, TX 75035\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "468_Position: Cloud Enterprise Security Architect : Frisco, TX : $85hr : Onite": "Amol,\nChabeztech\namol@chabeztech.com\nReply to: amol@chabeztech.com\nHello, Hope you are doing well Below is the Job Description Title: Cloud Enterprise Security ArchitectLocation: Frisco, TX Job/Responsibility Profile:\u00b7 Design and develop multi-tenant solutions for enabling cloud platform as service\u00b7 Deploy and Operate multi-cloud security solutions/platforms at Enterprise scale\u00b7 Develop end-to-end technical solutions in security space\u00b7 Develop self-service solutions to onboard customers and manage users on the platforms\u00b7 Assess the customers' security architecture, requirements and provide guidance\u00b7 Design and develop policies to improve security posture and prevent threat exposure\u00b7 Identify and adapt modern tools, principles and technologies to improve security across cloud landscape\u00b7 Support cloud customers through cloud-native architecture guidance, security architecture guidance, policy remediations, etc.\u00b7 Work with ITSM functions (Change management, Incident management, Problem management, Request management) as they apply to tools and platforms used by the teamTechnical Skills/Experience:\u00b7 Experience working in DevOps/GitOps teams\u00b7 Experience developing Infrastructure and Operations code, Platforms, and Automations\u00b7 Experience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate\u00b7 Understanding of all the basic services provided by CSPs (AWS, Azure and GCP)\u00b7 Knowledge and hands-on experience of interacting with CSP APIs\u00b7 Deep knowledge of IAM, Policies, Network and other security servicesAuthoring IAM policiesAuthoring Organization PoliciesDeveloping private network based applications (using private endpoints, Vnet integrations, IPSec)\u00b7 Developing Git Pipelines for managing platforms and operations\u00b7 Experience in Java Springboot/Python/GoLang development\u00b7 Experience in developing SAML, OAuth based applications\u00b7 Experience working with IaC tools such as Terraform, CloudFormation, or ARM templates.\u00b7 Experience in K8s development\u00b7 General experience working within ITSM processes (Change, Incident, Problem, Request management) in an Enterprise context\u00b7 Certifications such as AWS Certified Security Specialty, Azure Security Engineer Associate, or GCP Professional Cloud Security Engineer are a plus\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "469_AWS Cloud Engineer - Local To Waltham, MA": "Swati Gupta,\nTechnocraft Solutions LLC\nswati.gupta@technocraftsol.com\nReply to: swati.gupta@technocraftsol.com\nHello,Hope you are doing well!My name is Swati and I'm a Talent Acquisition Executive at Technocraft solutions. We provide IT Consulting Services to our customers\u2019 immediate and long-term resource needs. I am contacting you either because your resume has been posted to one of the internet job sites to which we subscribe or you had previously submitted your resume to Technocraft solutions Job Title: Cloud EngineerLocation: Waltham, MA 02451Type: Long Term Contract Job Description: \u2022 Create documentation, including technical guides and procedures, to ensure knowledge transfer and assist in training to team members on automation processes.\u2022 Troubleshoot and resolve issues related to cloud automation scripts, tools, and deployments.\u2022 Ansible, and Terraform to automate network configuration, provisioning, and management tasks.\u2022 Collaborate with customer engineering teams to gather requirements and automation tasks align with business objectives and operational needs.\u2022 Create Ansible playbooks to ensure consistent and repeatable deployments.\u2022 Project related work and help to operations support team.\u2022 cloud monitoring and alerting services and capabilities\u2022 Ability to solve complex technical problems and accomplish tasks with minimal supervision.\u2022 Ability to support project team to prioritize activities and provide deliverables as planned.\u2022 Monitor and optimize application performance, ensuring scalability as user demands grow.\u2022 Experience in Microservices architecture, Docker, and AWS EKS.\u2022 Expertise in AWS Cloud architecture, including EC2, S3, LAMBDA, ACM, SSM, EFS, FSx etc.\u2022 emerging technologies and industry trends, providing recommendations for adopting new tools and technologies to improve our technology stack.\u2022 Excellent communication skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "470_AWS Developer - Need Only local": "Ravi Duvey,\nTek Inspirations LLC\nravi.duvey@tekinspirations.com\nReply to: ravi.duvey@tekinspirations.com\nJob Description - Title : AWS Developer - JavaType : HybridLocation : Malvern , PADuration : 3 yearsMOI : VirtualNote : PV will take technical video screening call.Strong experience in Capital Markets, Fixed income, Equity and DerivativesProject : Portfolio management system - front office - investments & trading - auto generation - maintaining existing system - someone who understands ETFYears of exp : 5+ Qualifications :\u2022 Java / Spring \u2022 Typescript / JavaScript \u2022 Appsync \u2022 Lambda \u2022 Backend / Datastores (RDS / DynamoDB / SQL / PostgreSQL) \u2022 Eventing (Steams, SNS/SQS) \u2022 CloudWatch \u2022 Apache Flink \u2022 AWS Kinesis \u2022 AWS Event Bridge \u2022 AWS Simple WorkflowResponsibilities : Provides senior level system analysis, design, development, and implementation of applications and databases. Integrates third party products. Translates technical specifications into code for complex new or enhancement projects for internal clients. Writes programs, develops code, tests artefacts, and produces reports. Employs software development techniques to ensure tests are implemented in a way that support automation. Elevates code into the development, test, and production environments on schedule. Provides follow up production support. Submits change control requests and documents. Thoroughly understands software development methodology. Thoroughly understands development architecture standards. Trains and mentors staff with less experience. Resolves elevated issues. Participates in design, code, and test inspections throughout the life cycle to identify issues. Explains technical considerations at related meetings, including those with internal clients. Performs systems analysis activities. Thoroughly understands client business functions and technology needs. Has a broad understanding of Vanguards technologies, tools, and applications, including those that interface with business area and systems. Interfaces with cross functional team members and communicates systems issues at the appropriate technical level for each audienceThoroughly understands and complies with Information Technology and Information Security policies and procedures and verifies that deliverables meet requirements. Participates in special projects and performs other duties as assigned.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "471_AWS DEVOPS ENGINEER": "Angel.s,\nconcorditsystems\nangel.s@concorditsystems.com\nReply to: angel.s@concorditsystems.com\nJob DescriptionClient: Freddie MacClient Location: Mclean, VA100% RemoteHeavy on python coding and scripting. Hands-On Keyboarding Build CICD pipeline for creating Docker images and deploying Kubernetes pods in HELM Chart.Provide thought leadership on topics such as: Observability, Tooling, Cloud Native Services, and DevOps Methodology Observability: ELK, Grafana, Prometheus, Dynatrace Expectation: Design & Present solution for how to create standardized dashboards with Grafana and Prometheus. Present solution approach to Executive leaders. Once solution is approved, hands-on creation of standardized dashboards for Grafana/Prometheus. Create Dashboards using Grafana and Prometheus Splunk for SIEM (Cloud Trail Logs, Event Monitoring/Logging)Well-rounded experience with AWS services required: S3, EC2, API Gateway, Cloud Formation, Auto Scaling, Route53, SQS / SNS, Route 53, Step Functions, Elastic Load BalancersProvide access to S3 files, Upload/Store files in S3 bucket in Spring Boot Application, Send request to S3. CI/CD & IaC Build CI/CD integrations using Jenkins to deploy AWS services. Modify, Configure and Deploy Terraform or CloudFormation templates for IaC in AWSConfigure Lambda\u2019s with Terraform or CloudFormation and CI/CD pipelines for end-to-end automation Python Scripting, Microservices & Rest API\u2019s Integrate API\u2019s with AWS services, Secure API\u2019s, Invoke Lambda Functions to give API\u2019s permission to access AWS resources Write Python automation scripts to execute, trigger Lambda\u2019s or other automated deployments for AWS \u2013 build that processWrite Python scripts using Boto3 library to create Lambda\u2019s. Must be well-versed in Observability/Resiliency topics like:Multi-Region, Automated Failover/Failback, Resiliency Assist in the deploying/configuring Production Support tools for the application teams, for example: button for restarting pods or switching AWS regionsBuild automated configuration management capabilities utilizing AWS DevOps services to improve operational efficiency of applications in production.Implement observability tools for monitoring and analysis: This includes Grafana, Prometheus, Dynatrace, and ELKProvide AWS best practices and continue to develop AWS DevOps tools as teams mature\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "472_Urhgent opening for Senior Devops Lead Engineer-Remote": "Munish,\nSspearhead INC\nmunish.patodia1@sspearhead.com\nReply to: munish.patodia1@sspearhead.com\nLead cloud architecture and design sessions for CI/CD pipelines.Design and develop automation scripts and CI/CD pipelines across various Azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Configure and manage cloud security and policy components.Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments. MANDATORY SKILLS/EXPERIENCE Minimum 12 years\u2019 experience overseeing medium to large scale projects.Experienced in managing DevOps engineering and software development teams.Hands-on expertise in supporting and developing Java applications and Microservices.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Self-motivated with a proven track record as a team player Desirable skills/experience:Proficient in Azure AI Services and Azure Machine Learning.Skilled in monitoring tools like Elastic, Dynatrace, and similar platforms.Experienced in building .NET applications and using Microsoft Dataverse.Familiar with Web Content Management Systems and Salesforce.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "473_Cloud Network Engineer with AWS - Hybrid in GA :: NO H1B or CPT - Local and Onsite Interview Only": "Sonali Gupta,\nKPG99\nsgupta@kpgtech.com\nReply to: sgupta@kpgtech.com\nHey,Please Read the below Requirement and share your resume and also please let me know your visa status and current Location if interested.Cloud Network EngineerLocation- Hybrid in Atlanta GA - Local Candidates Only.Duration-6+Months Visa-No H1B/CPT*1 round - In person Interview.Must be a Cloud Network Engineer with strong AWS and also be a Palo Alto SME.Notes:3-4 years ago, they implemented standard AWS architecture.They're now migrating everything to the most modern AWS architecture as well as migrating systems from Azure and on-prem to AWS.Must Haves: -Using the most up to date AWS architecture.Has to be working as a Cloud Networking Engineer with AWS in the last 1-2 years, ideally the last four so they understand old arch and new.Palo Alto SME is a must. Palo Alto is their security partner of choice.Need experience migrating old tools to Palo Alto and deploying Palo Alto to AWS.They are doing some POCs with their security stack and Palo Alto Cloud Firewalls that may come to fruition.We are looking for an Engineer resource who is SME with Palo Alto Platform.Critical experience we are looking for as below.* Having experience in Cloud PA deployment in AWS and Azure platforms with latest deployment architecture - GWLBe and auto scale design* Having experience with Checkpoint to Palo Alto migration.* Will be working with Carrier Firewall engineers on standards / support and optimization of current security footprint.* Will be responsible for all documentation and knowledge transfer.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "474_C2C Opportunity for AWS Architect in Jersey City, NJ, USA (Hybrid mode of working  3 days a week from office)": "Kiran,\nsapphiresoftwaresolutions\nkirankumar@sapphiresoftwaresolutions.com\nReply to: kirankumar@sapphiresoftwaresolutions.com\nAWS Architect Location \u2013 Jersey City, NJ, USA (Hybrid mode of working \u2013 3 days a week from office)Experience level : 10 + Years Job description Responsibilities:Design and develop Step Functions workflows to automate multi-step processes across various AWS services.Integrate Step Functions with AWS Lambda functions, SQS queues, SNS notifications, and other relevant services.Implement error handling, retries, and timeouts within Step Functions workflows.Monitor and troubleshoot Step Functions workflows to ensure smooth execution.Document and explain complex workflows for technical and non-technical audiences.Stay up-to-date on the latest features and best practices for AWS Step Functions.Collaborate with engineers and stakeholders to identify opportunities for workflow automation. Qualifications:Proven experience designing and implementing serverless workflows using AWS Step Functions.Strong understanding of AWS services, including Lambda, SQS, SNS, and CloudWatch.Experience with scripting languages like Python or Node.js (a plus).Experience with infrastructure as code (IaC) tools like Terraform or CloudFormation (a plus).Excellent problem-solving and analytical skills.Strong communication and collaboration skills. Preferrable:Experience with CI/CD pipelines and integrating Step Functions workflows.Experience with workflow visualization tools like AWS Step Functions Workflow Studio.Certifications: AWS Certified Solutions Architect - Associate or Professional\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "475_Senior DevOps Engineer at Albany NY with 15 Plus Years of Exp GC OR US Only": "Srikanth Vanam,\nSligo Software Solutions Inc\nsrikanth.vanam@sligosoft.com\nReply to: srikanth.vanam@sligosoft.com\nGreetings from Sligo Software Solutions, Inc. !!SLIGO is a Software Development & IT Consulting company. We have around 100+ IT associates working with our esteemed clients across several domains such as IT, Banking, Healthcare, and Government. We have an urgent requirement for Senior DevOps Engineer position with our client. Role: Senior DevOps EngineerLocation: N/A, New YorkDuration: 12 MonthsJob ID: SLG - 19 - 9686 Mandatory RequirementsExperience ( 15+Years)Duties: Develop automation scripts and CI/CD pipelines across various Azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.Participate in cloud architecture and design sessions for CI/CD pipelines.Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Update build agents, extend Terraform and Ansible scripts.Configure and manage cloud security and policy components.Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments. Mandatory Qualifications: At least 12 years of experience in software development and DevOps tools.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Hands-on expertise in supporting and developing Java applications and Microservices.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, Python, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Knowledge of Terraform, Ansible, Helm, Kubernetes.Self-motivated with a proven track record as a team player.Please have EDUCATION details on all resumes.NO LAYERS PLEASE \u2013 U.S. Citizens, GC holders or YOUR H1-B candidate that can produce a 797A document.All candidates must be first screened by me then be screened Desirable Qualifications: \u2022 Knowledge of AWS and Azure Networking. Thank you,Srikanth V\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "476_Opening for AWS Architect  Location: Jersey City, NJ, USA (Hybrid mode of working  3 days a week from office)": "Rahman,\nWebster Tech Solutions Inc\nrahman@webstertechsolutions.com\nReply to: rahman@webstertechsolutions.com\nPosition: AWS Architect Location: Jersey City, NJ, USA (Hybrid mode of working \u2013 3 days a week from office) Job Description: Responsibilities:Design and develop Step Functions workflows to automate multi-step processes across various AWS services.Integrate Step Functions with AWS Lambda functions, SQS queues, SNS notifications, and other relevant services.Implement error handling, retries, and timeouts within Step Functions workflows.Monitor and troubleshoot Step Functions workflows to ensure smooth execution.Document and explain complex workflows for technical and non-technical audiences.Stay up-to-date on the latest features and best practices for AWS Step Functions.Collaborate with engineers and stakeholders to identify opportunities for workflow automation. Qualifications:Proven experience designing and implementing serverless workflows using AWS Step Functions.Strong understanding of AWS services, including Lambda, SQS, SNS, and CloudWatch.Experience with scripting languages like Python or Node.js (a plus).Experience with infrastructure as code (IaC) tools like Terraform or CloudFormation (a plus).Excellent problem-solving and analytical skills.Strong communication and collaboration skills. Preferable:Experience with CI/CD pipelines and integrating Step Functions workflows.Experience with workflow visualization tools like AWS Step Functions Workflow Studio.Certifications: AWS Certified Solutions Architect - Associate or Professional\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "477_SLG - 19 - 9689 - Senior DevOps Lead Engineer - SLG-1319": "Srikanth Vanam,\nSligosoft\nsrikanth.vanam@sligosoft.com\nReply to: srikanth.vanam@sligosoft.com\nGreetings from Sligo Software Solutions, Inc. !!SLIGO is a Software Development & IT Consulting company. We have around 100+ IT associates working with our esteemed clients across several domains such as IT, Banking, Healthcare, and Government. We have an urgent requirement for Senior DevOps Lead Engineer position with our client. Role: Senior DevOps Lead EngineerLocation: N/A, New YorkDuration: 12 MonthsJob ID: SLG - 19 - 9689Client: NY StateRespond by Date: 07/22/24 2:00 PM Mandatory RequirementsExperience (16+ Years)Minimum 12 years\u2019 experience overseeing medium to large scale projects.Experienced in managing DevOps engineering and software development teams.Hands-on expertise in supporting and developing Java applications and Microservices.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Self-motivated with a proven track record as a team player.Please have EDUCATION details on all resumes.NO LAYERS PLEASE \u2013 U.S. Citizens, GC holders or YOUR H1-B candidate that can produce a 797A document.All candidates must be first screened by me then be screened.Key Responsibilities:This is an 100% RemoteDaily activities include, but not limited to:Lead cloud architecture and design sessions for CI/CD pipelines.Design and develop automation scripts and CI/CD pipelines across various azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Configure and manage cloud security and policy components.Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments.Desirable Qualifications:Proficient in Azure AI Services and Azure Machine Learning.Skilled in monitoring tools like Elastic, Dynatrace, and similar platforms.Experienced in building .NET applications and using Microsoft Dataverse.Familiar with Web Content Management Systems and Salesforce\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "478_Senior DevOps Engineer at Albany NY with 15 Plus Years of Exp GC OR US Only": "Katta Dasharatham,\nSligo Software Solutions Inc\ndasharatham.katta@sligosoft.com\nReply to: dasharatham.katta@sligosoft.com\nGreetings from Sligo Software Solutions, Inc. !! SLIGO is a Software Development & IT Consulting company. We have around 100+ IT associates working with our esteemed clients across several domains such as IT, Banking, Healthcare, and Government. We have an urgent requirement for Senior DevOps Engineer position with our client. Role: Senior DevOps EngineerLocation: N/A, New YorkDuration: 12 MonthsJob ID: SLG - 19 - 9686 Mandatory RequirementsExperience ( 15+Years)Duties: Develop automation scripts and CI/CD pipelines across various Azure services including IaaS, PaaS, SaaS, ACA, APIM, Azure Functions, Azure Logic Apps, databases, secrets, and storage services.Enhance and maintain existing CI/CD pipelines for Java applications hosted on AWS using Azure DevOps.Participate in cloud architecture and design sessions for CI/CD pipelines.Provide feedback on proposed system architectures to teams involved in the MyCity Program workstreams.Implement and support highly scalable, flexible, and resilient cloud architectures to address business challenges and expedite cloud service adoption.Define requirements for integrating multiple cloud capabilities and scenarios, such as service availability, quality, usage correlation, charges, and cost-effective architecture.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Update build agents, extend Terraform and Ansible scripts.Configure and manage cloud security and policy components.Establish cloud consumption targets for multi-tenant/hybrid cloud deployments.Deploy, configure, and support IaaS/PaaS components in Azure and AWS environments.Design, plan, and implement Business Continuity and Disaster Recovery strategies for cloud environments. Mandatory Qualifications: At least 12 years of experience in software development and DevOps tools.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Hands-on expertise in supporting and developing Java applications and Microservices.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, Python, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Knowledge of Terraform, Ansible, Helm, Kubernetes.Self-motivated with a proven track record as a team player.Please have EDUCATION details on all resumes.NO LAYERS PLEASE \u2013 U.S. Citizens, GC holders or YOUR H1-B candidate that can produce a 797A document.All candidates must be first screened by me then be screened Desirable Qualifications: \u2022 Knowledge of AWS and Azure Networking. Thank you,Katta DasharathamSr Technical RecruiterSligo Software Solutions Inc.dasharatham.katta@sligosoft.comwww.sligosoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "479_AWS DevOps Engineer            Remote, mostly ES        USC or GC only": "Jahnvi Singh,\nPivotal technologies\njahnvi.singh@pivotal-technologies.com\nReply to: jahnvi.singh@pivotal-technologies.com\nHello, Candidates have failed with their Palo Alto and Panorama experience, they must have this with strong AWS. Location: Remote, mostly ES, but could be all 3Duration: 14+ weeks MUST be a US Citizen or Green Card and show Passport or proof of residence There will be an extensive background and employment verification please make sure you trust the candidate before you send Candidate MUST haves:AWS Infrastructure Strong experience on AWS Infrastructure services (EC2, ELB, RDS, Route53, S3, vpc, vpn, tgw, cloudwatch, cloudtrail, eventbridge, etc. Strong Networking with Palo Alto Networking with Panorama Automation with TerraformHands on experience in Terraform IaC deployments. Strong Experience in building CI/CD (Gitlab, Jenkins) and deployment automation Title: AWS DevOps Engineer with PaloAlto Firewall Skillset Description Of Services:AWS Cloud Engineer with PaloAlto Firewall skillset What You will need (Qualifications) AAt least 3+ years of experience in AWS with hands-on contribution in DevOps engineering. Strong track record of implementing AWS services in a variety of distributed computing environments (Design and implementation experience and Cloud Networking). Strong experience on AWS Infrastructure services (EC2, ELB, RDS, Route53, S3, vpc, vpn, tgw, cloudwatch, cloudtrail, eventbridge, etc. Large scale implementation and migration experience of Data Center to Cloud Infrastructure automation through DevOps scripting (Ansible, Shell, Python, Ruby, PowerShell) Hands on experience in Terraform IaC deployments. Strong Experience in building CI/CD (Gitlab, Jenkins) and deployment automation. Good practical Linux / Windows-based systems administration skills in a Cloud or Virtualized environmentExperience working on FedRamp compliant projects is a plus. Must be a GC or US Citizen. Palo Alto & Panaroma Skillset requirements:Design and architect Palo Alto firewall solutions for AWS cloud-based infrastructures based on business requirements and security best practices.Ability to independently deploy Palo Alto firewalls & Panorama in cloud environments using IaC automation, ensuring proper network segmentation and security zones. Strong hands-on experience using Panorama to configure security policies, NAT rules, VPN tunnels, and other firewall settings to protect cloud assets.Implement high availability and failover configurations for firewall devices. Develop and enforce security policies, including threat prevention, URL filtering, application control, and user identification.Ability to Integrate Panorama with other security tools and systems, such as SIEM platforms, to enhance threat detection and response capabilities. Palo Alto Networks certifications (e.g., PCNSA, PCNSE) are highly desirable. What You Bring To The Team Can work autonomously, deliver with minimal supervision from a set of requirements Demonstrated ability to think strategically about business, product, and technical challenges Has excellent communication skills to work as a member of a team Ability to function in an agile-based environment and provide good daily feedback on team stand-up call Participate in code release and production deployment, have an aggressive approach to fixing bugs and defects Deliverables:-Process Flows -Mentor and Knowledge transfer to client project team members -Participate as primary, co and/or contributing author on any and all project deliverables associated with their assigned areas of responsibility -Participate in data conversion and data maintenance -Provide best practice and industry specific solutions -Advise on and provide alternative (out of the box) solutions -Provide thought leadership as well as hands on technical configuration/development as needed. -Participate as a team member of the functional team -Perform other duties as assigned. Acceptance Criteria:must be US Citizen or Green Card Holder Thanks & Regards,Jahnvi SinghUS IT RecruiterOffice: (703) 570-8775 (Ext. 229)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "480_Please share local candidates-Cloud Engineer-Gov Cloud   Seattle Washington - Onsite-Job Description": "pallavi,\nyochana\npallavi@yochana.com\nReply to: pallavi@yochana.com\nHello,I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider with 14+ years of experience delivering value to leading companies across the U.S.A. and Canada,I'm currently staffing for Cloud Engineer-Gov Cloud \u2013 Seattle, Washington - Onsite, Below you will find the job description, if you are qualified and interested, please send me your Updated Word Document Resume.I'm very sorry if this position is not an ideal fit, We'll keep you in mind for other suitable positions and referrals would be appreciated.Thank you so much for your attention and participation.Job Role: Cloud Engineer-Gov CloudJob Location: Seattle, Washington - OnsiteProject Duration: 6-12 MonthsJob Description:Mandatory: \u2022 Experience with network onboarding and provisioning in cloud environments.\u2022 Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). Key ResponsibilitiesOversee the network onboarding process for new users and systems into Cloud environmentProvision and configure network resources in Cloud, ensuring compliance with security policies and government regulations.Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms.Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways.Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats.Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure.Conduct regular network assessments and audits to ensure compliance with internal and external requirements.Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards.Provide technical support and troubleshooting for Cloud network-related issuesStay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms QualificationsExperience with network onboarding and provisioning in cloud environments.Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA).In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud.Familiarity with security regulations, standards, and compliance requirements for cloud network environments.Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers.Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud.Strong communication and documentation skills for collaborating with cross-functional teams.Experience with automation tools (e.g., Python, Terraform) for network configuration and managementRelevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred. Regards, Pallavi BoolaRESOURCE SPECIALIST Contact- 2482373189Yochana IT Solutions Inc. 23000 Commerce Dr, Farmington hills, MI-48335 pallavi@yochana.com || www.yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "481_Need AWS Devops with Infrastructure, Plano, Tx (Need Locals Only)": "Srinath,\nIT Strategies\nsrinath@itstrategiesinc.com\nReply to: srinath@itstrategiesinc.com\nRole: AWS Devops with InfrastructureLocation: Plano, TX (Onsite)Duration: Contract\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "482_Job Opportunity - AWS CLOUD NATIVE DEVELOPER - Atlanta GA (Hybrid)": "srujan,\nThoughtwave software solutions\nsrujan@thoughtwavesoft.com\nReply to: srujan@thoughtwavesoft.com\nHi,This is Srujan from Thoughtwave Software and Solutions.I hope you are doing Well,Job title: AWS CLOUD NATIVE DEVELOPERLocation: Atlanta GA (Hybrid)Job ID: AWS CLOUD NATIVE DEVELOPER (742293)Complete Description:Position Overview: As an AWS Cloud Developer at GTA you will play a crucial role in designing, developing, and maintaining scalable cloud solutions on the AWS platform. You will collaborate closely with cross-functional teams in a SCRUM Agile environment to deliver high-quality software solutions that meet business objectives. The ideal candidate will have extensive experience with SOAP-based web services,custom header implementation, and handling MTOM (Message Transmission Optimization Mechanism) attachmentsKey Responsibilities:Design, develop, and deploy cloud-native applications on AWS using services such as Lambda, API Gateway, DynamoDB, S3, and others as needed.Implement serverless architectures using AWS Lambda functions with Python.Build and orchestrate workflows using AWS Step Functions and AWS State Machines.Design, develop, and implement SOAP-based web services using services technologies.Create and manage custom headers for web services to ensure security, authentication, and data integrity.Implement MTOM attachments such as PDF for efficient transmission of binary data in web services.Collaborate with Product Owners, Scrum Masters, and other team members to refine user stories and deliver solutions iteratively.Ensure code quality, performance, and scalability through automated testing, code reviews, and adherence to best practices.Troubleshoot and resolve issues in development, testing, and production environments.Stay current with AWS services, tools, and best practices and share your knowledge within the team.Qualifications:Bachelor's degree in Computer Science, Engineering, or a related field (or equivalent practical experience).Proven experience with XML, XSD, WSDL, and other related technologiesProven experience as a software developer with a strong understanding of cloud computing principles and practices.Hands-on experience designing and developing applications on AWS cloud services, particularly Lambda, API Gateway, DynamoDB, and S3.Proficiency in Python programming language; familiarity with other languages is a plus.Experience with AWS Step Functions and State Machines is highly desirable.Familiarity with Agile methodologies and SCRUM framework.Strong problem-solving skills and ability to work effectively in a team environment.Excellent verbal and written communication skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "483_Hot Requirements :AWS Engineer with IoT Remote": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nRole: AWS Engineer with IoTLocation: RemoteDuration: ContractClient: Value Labs/CSC Service Works JD:-Roles:Writing Lambda functions, building APIs, processing message streams from IoT devices, and implementing AWS cloud services such as EKS, Cognito, DynamoDB, and API Gateway for a door-unlocking feature as part of the project. Primary Skills Required:AWS services (EKS, Cognito, DynamoDB, API Gateway), programming in Node.js, understanding of MQTT and CoAP, Lambda functions, handling IoT data streams, serverless architecture, and RESTful APIs.Technical proficiency in AWS and serverless computing, ability to develop and integrate APIs, experience with modern authentication mechanisms (like Cognito), expertise in handling real-time data and loT communications, and innovative problem-solving skills in cloud architectures. Secondary skills:IAM (Identity and Access Management) CloudFormation/Terraform CI/CD Pipelines Monitoring and Logging Security Best Practices Collaboration Tools Communication Skills Problem Solving and Debugging Must have:Strong experience with AWS cloud services, particularly in serverless environments, experience with loT platforms and protocols, software development skills particularly in JavaScript/Nodejs, and familiarity with constructing and managing secure and scalable cloud-based architectures. Responsibilities:Provision and Manage AWS EKS Cluster: Design, provision, and manage AWS EKS clusters to support containerized applications. Ensure high availability and scalability of EKS clusters. Create and Manage Cognito User Pools: Set up and manage AWS Cognito User Pools for user authentication and authorization. Implement Cognito Lambda Triggers to customize authentication workflows. Develop and Manage AWS Lambda Functions: Create Lambda functions to handle various backend processes. Implement Lambda Authorizers to secure API Gateway endpoints. Integrate Lambda functions with DynamoDB, Cognito, and other AWS services. Design and Manage DynamoDB Tables: Design, create, and manage DynamoDB tables to support application data needs. Optimize DynamoDB performance through proper indexing and data modeling. Set Up and Manage API Gateway: Design and configure API Gateway endpoints for various microservices. Implement security measures using Lambda Authorizers and other API Gateway features. Develop RESTful APIs Using Node.js: Build and maintain backend services using Node.js. Ensure APIs follow RESTful principles and are secure, scalable, and efficient. Implement IoT Communication Protocols: Use MQTT and CoAP protocols for IoT communication and data transfer. Ensure reliable and efficient messaging between IoT devices and backend services. Ensure Security and Compliance: Implement AWS security best practices across all services. Ensure compliance with relevant data protection and privacy regulations. Automate Infrastructure Provisioning: Use infrastructure as code tools like CloudFormation or Terraform to automate resource provisioning. Maintain version control and documentation for infrastructure changes. Set Up CI/CD Pipelines: Implement continuous integration and continuous deployment pipelines for automated testing and deployment. Ensure fast and reliable deployment of code changes. Monitor and Troubleshoot Applications: Set up monitoring and logging using AWS CloudWatch and other tools. Troubleshoot and resolve performance and reliability issues in a timely manner. Collaborate with Cross-Functional Teams: Work closely with development, operations, and security teams to ensure seamless integration and deployment of services. Communicate effectively with stakeholders to understand requirements and provide updates.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "484_Senior DevOps Engineer || Remote": "Akanksha Yadav,\nTek Inspirations LLC\nakanksha.yadav@tekinspirations.com\nReply to: akanksha.yadav@tekinspirations.com\nJob Description -Senior DevOps EngineerWork Setting: RemoteLocation: Brooklyn, New York, 11201Start Date: August 12, 2024 Need To be localRequirementsMANDATORY MINIMUM QUALIFICATIONSThe successful candidate must have minimally achieved the following level of experience: At least 12 years of experience in software development and DevOps tools.Expertise in Azure DevOps including GIT, CI/CD build, and release pipelines.Proficient in operating Azure Services such as ACA/AKS, Azure VMs, Azure Functions, Azure Logic Apps, APIM, Azure Application Gateway, etc.Hands-on expertise in supporting and developing Java applications and Microservices.Familiarity with Microsoft Entra ID (Azure Active Directory).Strong understanding of HTTPS, SAML, OpenID, and OAuth protocols.Skilled in using Java project build tools like Maven, Gradle, and ANT.Extensive hands-on experience with Spring Boot, Python, JPA, Hibernate, and JSP.Excellent understanding of Relational Database technologies: Microsoft SQL Server, Oracle, and/or PostgreSQL.Proficient in monitoring Azure services and custom applications using Azure monitoring and observability services and similar tools.Effective oral and written communication skills.Knowledge of Terraform, Ansible, Helm, Kubernetes.Self-motivated with a proven track record as a team playerPREFERABLE QUALIFICATIONSKnowledge of AWS and Azure Networking\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "485_Hybrid Role || Aws Architect || Jersey City, NJ 07302": "Prashant,\nIntellectt INC\nprashant@intellectt.com\nReply to: prashant@intellectt.com\nAws Architect Hybrid Contract RoleJersey City, NJ 07302LOCAL WILL HIGHLY PREFER Responsibilities:Design and develop Step Functions workflows to automate multi-step processes across various AWS services.Integrate Step Functions with AWS Lambda functions, SQS queues, SNS notifications, and other relevant services.Implement error handling, retries, and timeouts within Step Functions workflows.Monitor and troubleshoot Step Functions workflows to ensure smooth execution.Document and explain complex workflows for technical and non-technical audiences.Stay up-to-date on the latest features and best practices for AWS Step Functions.Collaborate with engineers and stakeholders to identify opportunities for workflow automation.Qualifications:Proven experience designing and implementing serverless workflows using AWS Step Functions.Strong understanding of AWS services, including Lambda, SQS, SNS, and CloudWatch.Experience with scripting languages like Python or Node.js (a plus).Experience with infrastructure as code (IaC) tools like Terraform or CloudFormation (a plus).Excellent problem-solving and analytical skills.Strong communication and collaboration skills.Preferrable:Experience with CI/CD pipelines and integrating Step Functions workflows.Experience with workflow visualization tools like AWS Step Functions Workflow Studio.Certifications: AWS Certified Solutions Architect - Associate or Professional AWS Architect skill Matrix \u2013Skill MatrixMandatoryRatingsAWS step function/workflowYes AWS LambdaYes AWS SQS/SNSYes PythonYes CI CD pipelineDesirable CommunicationStrong\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "486_Lead Azure Cloud Engineer": "Harsh,\nAtlascloud\nharsh@acloudinc.com\nReply to: harsh@acloudinc.com\nHi.Hope you are doing great,We have requirement with our client let us know if you have suitable profile. Client : AetnaRole : Lead Azure Cloud EngineerLocation : Location: Tampa Hybrid \u2013 3 Days onsiteExp: 7+ years Job Description : Platforms \u2013 Work with development teams to stand up desired cloud environments.Requirements Elaboration \u2013 Identify the needs for build automation and implement appropriate solutions.System Performance \u2013 Contribute to solutions that satisfy performance requirements, tune application performance issues, ensure service uptime and response time meet SLAs.Standards \u2013 Be aware of CICD standards and best practices and utilize them in solutions effectively.Documentation \u2013Develop and maintain system documentation.Support team in managing client expectations and resolving issues on time. Talents Needed for Success:Bachelor\u2019s degree in computer science, Applied Computer Science, or related field.5+ yePassion for technology innovation, a curious mind, and an entrepreneur mindset.Abars or related experience.ility to present technical information clearly to different management levels.Azure fundamentals, Azure Security, and Azure Administrator experience a MUST. Azure certifications, knowledge of Bicep scripting a plusOther experience to include the following technologies: CICD patterns, Terraform.Experience using the following tools: GIT, Bit Bucket, Jira, Confluence, Jenkins.Experience with Python scripting preferred.Working knowledge of AWS is a plus.Knowledge of different software development methodologies (Waterfall, Agile, Scrum, Kanban)Platforms \u2013 Work with development teams to stand up desire Harsh,Delivery Lead,Atlas Cloud, solution for your IT.P: +1 4193797060E: harsh@acloudinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "487_AWS ETL Architect || Columbus, OH || Onsite": "Prashanth B,\nTekgence\nprashanth.b@tekgence.com\nReply to: prashanth.b@tekgence.com\nTitle: Sr. ArchitectLocation: Columbus, OHDuration: Long term Required:\u2022 Should have BANKlNG domain experience - Mandatory\u2022 Lead and execute the migration of on-premises systems to AWS, including cutover planning and post-migration reconciliation.\u2022 Architect and deploy scalable Spark clusters on Kubernetes within AWS, ensuring optimal performance and integration.\u2022 Design and develop efficient ETL pipelines for data extraction, transformation, and loading, maintaining high data quality and integrity.\u2022 Establish and enforce data standards across pipelines, data lakes, and data warehouses to ensure consistency and quality.\u2022 Oversee data ingestion, placement, and transformation processes, ensuring compliance with data regulations and best practices.\u2022 Utilize AWS services such as Glue Catalog, Lake Formation, and EMR to enhance data workflows and storage solutions.\u2022 Worked extensively on Data Migration projects from On prem to Cloud. --Regards,Prashanth B | Tekgence Inc.Talent Acquisition Specialist LinkedIn: https://www.linkedin.com/in/prashanth-goud-burra-535238207/Email: prashanth.b@tekgence.com | Website: www.tekgence.com6655 Deseo Dr \u2022 Suite 104 \u2022 Irving, TX \u2022 75039\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "488_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "489_DevOps Engineer": "Serisha,\nMapout Digitals\nserisha.k@mapoutinc.com\nReply to: serisha.k@mapoutinc.com\nHi,Hope you are doing great.Please let me know if you have any suitable profiles.No GC and OPTJob Title: CI/CD DevOps Release Engineer Location: Framingham, MA (First Priority to locals) \u2013 onsite is must Note: 3 rounds of interview \u2013 2 live coding interviews. Job Description:We are seeking a skilled CICD DevOps Release Engineer with expertise in Azure, Kubernetes, and Python to join our team. As a pivotal member of our DevOps group, you will play a crucial role in enhancing our software delivery pipeline, automating deployment processes, and ensuring the reliability and scalability of our applications.Key Responsibilities:Design, develop, and implement CI/CD pipelines using Jenkins and groovy libraries.Manage and maintain infrastructure as code (IaC) using Packer and Terraform in Azure.Migrate microservices to AKS.Collaborate closely with development, QA, and operations teams to foster a DevOps culture.Monitor and optimize the performance of CI/CD tools and pipelines.Troubleshoot build and deployment issues and provide timely resolution.Implement security best practices throughout all aspects of CI/CD pipelines.Required Skills and Experience:Strong experience with CI/CD tools such as Jenkins, GitLab CI, etc.Strong proficiency in Azure services and managing Azure environments.Strong understanding of Linux systemsProficiency in scripting and automation with Python and bash.Solid understanding of version control systems (e.g., Git) and branching strategies.Experience with infrastructure as code (IaC) tools like Terraform.Familiarity with Docker and containerization concepts.Knowledge of Agile methodologies and working in Agile teams.Excellent problem-solving skills and attention to detail.Strong communication and collaboration skills.Basic experience with Kubernetes for container orchestration and management.Preferred Skills:Certification in Azure or Kubernetes (e.g., Azure Administrator Associate, Certified Kubernetes Administrator).Experience with other cloud platforms (AWS, Google Cloud).Familiarity with monitoring and logging tools like Splunk/NewrelicExperience with Ansible.Education:Bachelor\u2019s degree in Computer Science, Engineering, or a related field (or equivalent experience).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "490_Infrastructure Engineers with Devops at Alpharetta, GA for Contract": "Mayur,\nKlaxontech\nmayur.s@klaxontech.com\nReply to: mayur.s@klaxontech.com\nPosition: Infrastructure Engineers with DevopsLocation: Alpharetta, GAClient: CapgeminiType: Contract Note: Need genuine candidates with active LinkedIn We are looking for folks who are reliable, fast learners, efficient system engineers / developers with hands-on experience in building custom infrastructure solutions, with a focus on automation, and are proficient in Python, Linux, and core infrastructure concepts. Required Skills: \u00b7 Proficiency in Python and Unix shell scripting and Dataiku\u00b7 Understanding of core infrastructure fundamentals (OS, networking, storage, virtualization, AuthN/AuthZ, APIs, etc.)\u00b7 Comfortable in following Agile/DevOps practices with relevant experience in using tools such as Git, Jira, and Bitbucket\u00b7 Software developer experience and understanding of fundamentals in distributed system design, development, and deployment.\u00b7 Hands-on experience with at least one major DBMS, data analytical, and/or messaging products (Sybase ASE, DB2 UDB, MSSQL, PostgreSQL, MongoDB, Greenplum, Redis, Hadoop, Databricks, MQSeries, Kafka)\u00b7 A self-starter with the ability to work effectively in teams.\u00b7 Excellent verbal and written communication skills\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "491_Sr Cloud Engineer": "Vivek,\nCieltech Inc\nvivek@swifttechinc.com\nReply to: vivek@swifttechinc.com\nHi Professional,Please share relevent profile. Client is looking for consultant only based out of Atlanta,GA.Sr Cloud Engineer:Altanta,GA-Only:(Hybrid)Rate:$65/hr on C2CJob Description:Qualifications:- 4+ years of experience in design and implementation of large enterprise public cloud environments - AWS Certified Professional preferred - 2+ years of hands-on experience integrating mission critical applications into AWS - Deep understanding of AWS services (IAM, VPCs, Landing Zone, EC2, ECS, KMS, CloudWatch, CloudTrail, Systems Manager, S3, RDS, Route53, Lambda, AWS Config, etc.) - Experience building and securing infrastructure as code using CloudFormation, Ansible, SAM and/or similar tools - Fluency with one or more scripting/coding languages such as Java, Javascript, REST, JSON, Python, bash - Experience implementing and leveraging the logging and monitoring solutions - Experience in cloud native architectures and micro-services design - Understanding of the shared responsibility model in AWS - Proven record of accomplishments in major enterprise level projects - Experience in design of complex distributed systems environments - Demonstrated ability to think strategically about business, product, and technical challenges - Ability to clearly communicate and present to various levels of the organization - Strong organizational and analytical skills with attention to detail - Independent and self-motivated and very thorough worker WHAT ELSE? - Candidate must have excellent communication skills (both written and verbal), demonstrated teamwork skills, be innovative, creative and have strong problem-solving abilities.Responsibilities:- Provide technical leadership and design expertise for AWS Foundation integration- Define standards and patterns for when and how to efficiently leverage AWS services- Lead automation of AWS integration processes and tasks- Aid development of Public Cloud Usage Statistics and Reporting dashboards for account owners- Develop frameworks and tools to enable dev teams to consume authorized AWS services- Partner with the DevTools team to develop and deploy a CI/CD pipeline leveraging tools such as Gitlab, Maven and Jenkins- Lead automation development using Ansible or CloudFormation to support acceleration of application delivery into AWS- Ensure Security-by-Design is a foundational component of every app deployed into AWS- Ensure enterprise monitoring standards are applied to each AWS app deployment- Contribute to creation, management and upkeep of Intranet FAQs, User guides and Knowledgebase, standards and build documents for AWS Foundations integration- Provide leadership in identifying new concepts, ideas, techniques, and technology assistance (best practices, guidelines and design patterns for supported technologies)- Perform cost benefit analysis to determine best system architecture via software, hardware, internal cloud, externally hosted or other defined model- Mentor engineering and development staff on public cloud integration design and hygiene, driving quality, consistency, resiliency, security and supportability into designs- Partner with peer Infrastructure teams to provide enterprise class AWS integration- Design and maintain standard templates, reference architectures, and design patterns that aid other Cloud Engineers in development of standards-based designsComments/Special InstructionsAWS , CFT , AnsibleManager\u2019s input:-I\u2019m looking for AWS experience in general and focused experience on Automation, (Ansible preferred), CFTs, service Catlogs., some knowledge on messaging tools.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "492_Re: Hybrid &amp; Local Only || Sr Cloud Security Architect || Alpharetta GA | Frisco, TX | Kansas City, Kansas | Bellevue, WA": "Sameer Raj,\nTanisha Systems Inc\nsameer@tanishasystems.com\nReply to: sameer@tanishasystems.com\n\u201cGreetings from Tanisha Systems Inc.\u201dPlease have a look at the below opportunity and reply to me if you are interested. Job Title: Sr Cloud Security ArchitectLocation: Alpharetta, GA / Frisco, TX / Kansas City, Kansas / Bellevue, WA (Hybrid, 3 days/week)Long term contract Technical SkillsExperience with onboarding and provisioning in cloud environmentsProficiency in scripting and automation tools (e.g., Python, Terraform)Knowledge of cloud security services, tools, and best practicesUnderstanding of security regulations, standards (e.g., FedRAMP, NIST), compliance requirements for cloud environmentsHands-on experience with cloud security services (IAM, CloudTrail, Config, GuardDuty, Security Hub)Cloud security certifications (e.g., AWS Certified Security - Specialty, Azure Security Engineer Associate)Problem-solving and analytical skills for identifying and mitigating security risks Key ResponsibilitiesDevelop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.Implement and enforce security controls to protect sensitive data and systems.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. QualificationsExperience with onboarding and provisioning in cloud environments.Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.Proficiency in scripting and automation tools (e.g., Python, Terraform).Strong communication and documentation skills for collaborating with cross-functional teams. Regards, Sameer RajTanisha Systems IncEmail: sameer@tanishasystems.com Address: 99 Wood Ave South, Suite # 308, Iselin, NJ 08830\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "493_Onsite Need--------------Infrastructure Azure Security IaaS (L4) Architect withAzure Governance &amp; Cost Management": "Rehan,\nResource Logistics\nrehan@resource-logistics.com\nReply to: rehan@resource-logistics.com\nTitle : Infrastructure Azure Security IaaS (L4) Architect with Azure Governance & Cost ManagementLocation : Dallas, Texas - HybridContract Experience - A minimum of 8-12 years of experience in a consulting or an architecture position with a software and/or services company.Roles & Responsibilities\u2022 Understand customer's IT estate, LOB applications, IT and business priorities and success measures to design implementation architectures and cloud native solutions\u2022 Collaborate with network, security, AD, backup, tools and application architects and SME's to develop enterprise grade solutions. \u2022 Develop deep relationships with key customer IT decision makers.\u2022 Define long term cloud strategy and mentor operations teams to implement the solutions\u2022 Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge\u2022 Advocate new features and solutions to bring operational efficiency and cost reduction.\u2022 Act as a subject-matter expert on Microsoft Azure and assist Sales/Pre-Sales in RFP activities.Requirements\u2022 Subject matter level expertise on Azure Governance & Cost Management, Azure Network & Security, Azure Active Directory, Compute & Storage, Backup & DR, Monitoring\u2022 Extensive experience in conducting design workshops, documentation of HLD (High Level Design) documents.\u2022 Rich experience in Migration projects and hand-on experience with ASR/Azure Migrate\u2022 Good Knowledge of Azure DevOps, ARM Templates & PowerShell scripting\u2022 Experience in PaaS services and Containers is preferred.\u2022 Proven track record of building deep technical relationships with senior IT executives and growing data services in large or highly strategic accounts.\u2022 Demonstrated ability to adapt to new technologies and learn quickly.\u2022 Proven track record of driving decisions collaboratively.\u2022 Resolving conflicts and ensuring follow through with exceptional verbal and written communication skills.\u2022 Presentation skills with a high degree of comfort with both large and small audiences (Senior Executives, IT management and developers)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "494_Hybrid: Urgent need Looking for Azure Cloud Architect in Montgomery Country, MD": "Sridhar,\nStellarIT Solutions\nsridhar@stellarit.com\nReply to: sridhar@stellarit.com\nOur Fortune 500 client is looking for a for Azure Cloud Architect in on their project based out Montgomery Country, MDJob Title: Azure Cloud ArchitectLocation: Montgomery Country, MD -HybridDuration: Long term*********************Need Local candidates only*************Job Description:Develop a Reference Architecture that includes all major functions of infrastructure and how it aligns with the mission and vision of the organization.Design and develop a comprehensive cloud architecture strategy with a focus on Microsoft Azure to support the organization's large-scale digital transformation.Lead the development and implementation of cloud policies, procedures, and governance models to ensure effective management and security of cloud resources.Work closely with IT security teams to establish robust cloud security frameworks that align with industry best practices and regulatory compliance requirements.Guide the migration of existing applications and infrastructure to Azure, ensuring minimal disruption to business operations.Develop cost management strategies to optimize and control cloud expenses without compromising on service delivery.Collaborate with various departments and IT teams to ensure cloud solutions meet functional and technical requirements.Establish continuous integration/continuous deployment (CI/CD) pipelines and automation practices to enhance development processes and operational efficiency.Mentor and train team members and stakeholders on Azure cloud technologies and best practices.Stay abreast of new Azure features and other cloud innovations to keep the organization at the cutting edge of technology advancement. Requirements:Minimum of 15 years in IT development/infrastructure, with at least 8 years as a Cloud Architect focused on Microsoft Azure.Proven experience in designing and implementing enterprise-scale cloud solutions.Deep understanding of Azure architecture, cloud security, governance, and compliance frameworks.Strong leadership skills and experience in developing policies and procedures for large-scale organizations.Certifications such as Azure Solutions Architect Expert or similar are highly preferred.Excellent communication, analytical, and project management skills.Please send your updated word format resume along with your best contact details to sridhar@stellarit.comStellar IT Solutions is a Global IT Solution provider headquartered in Rockville, MD with operations in the US and India. Stellar IT Solutions has over 15 years of IT and consulting experience to give cost effective solutions to many Fortune 500 companies.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "495_DevOps Release Engineer": "alice,\nrwaltz\nalice@rwaltz.com\nReply to: alice@rwaltz.com\nJob Description:\u00b7 We are seeking a skilled CICD DevOps Release Engineer with expertise in Azure, Kubernetes, and Python to join our team. As a pivotal member of our DevOps group, you will play a crucial role in enhancing our software delivery pipeline, automating deployment processes, and ensuring the reliability and scalability of our applications.Key Responsibilities:\u00b7 Design, develop, and implement CI/CD pipelines using Jenkins and groovy libraries.\u00b7 Manage and maintain infrastructure as code (IaC) using Packer and Terraform in Azure.\u00b7 Migrate microservices to AKS.\u00b7 Collaborate closely with development, QA, and operations teams to foster a DevOps culture.\u00b7 Monitor and optimize the performance of CI/CD tools and pipelines.\u00b7 Troubleshoot build and deployment issues and provide timely resolution.\u00b7 Implement security best practices throughout all aspects of CI/CD pipelines.Required Skills and Experience:\u00b7 Strong experience with CI/CD tools such as Jenkins, GitLab CI, etc.\u00b7 Strong proficiency in Azure services and managing Azure environments.\u00b7 Strong understanding of Linux systems\u00b7 Proficiency in scripting and automation with Python and bash.\u00b7 Solid understanding of version control systems (e.g., Git) and branching strategies.\u00b7 Experience with infrastructure as code (IaC) tools like Terraform.\u00b7 Familiarity with Docker and containerization concepts.\u00b7 Knowledge of Agile methodologies and working in Agile teams.\u00b7 Excellent problem-solving skills and attention to detail.\u00b7 Strong communication and collaboration skills.\u00b7 Basic experience with Kubernetes for container orchestration and management.Preferred Skills:\u00b7 Certification in Azure or Kubernetes (e.g., Azure Administrator Associate, Certified Kubernetes Administrator).\u00b7 Experience with other cloud platforms (AWS, Google Cloud).\u00b7 Familiarity with monitoring and logging tools like Splunk/Newrelic\u00b7 Experience with Ansible.Education:\u00b7 Bachelor\u2019s degree in Computer Science, Engineering, or a related field (or equivalent experience).\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "496_Azure Cloud Architect Montgomery County, DE - Hybrid  Must be local": "prashanth,\nAcuBahn Inc.\nprashanth@acubahn.com\nReply to: prashanth@acubahn.com\nAzure Cloud ArchitectMontgomery County, DE - Hybrid Must be local to DMV - Scope of Work:Under the guidance/direction/supervision of the Cloud Manager, and others as assigned, the Contractor will:Establish a practical approach to supporting an enterprise architecture that starts and ends with a focus on delivering signature-ready, actionable recommendations to business and IT leaders, so they can adjust policies, products, processes, and projects to achieve target business outcomes.Develop a Reference Architecture that includes all major functions of infrastructure and how it aligns with the mission and vision of the organization.Design and develop a comprehensive cloud architecture strategy with a focus on Microsoft Azure to support the organization's large-scale digital transformation.Lead the development and implementation of cloud policies, procedures, and governance models to ensure effective management and security of cloud resources.Work closely with IT security teams to establish robust cloud security frameworks that align with industry best practices and regulatory compliance requirements.Guide the migration of existing applications and infrastructure to Azure, ensuring minimal disruption to business operations.Develop cost management strategies to optimize and control cloud expenses without compromising on service delivery.Collaborate with various departments and IT teams to ensure cloud solutions meet functional and technical requirements.Establish continuous integration/continuous deployment (CI/CD) pipelines and automation practices to enhance development processes and operational efficiency.Mentor and train team members and stakeholders on Azure cloud technologies and best practices.Stay abreast of new Azure features and other cloud innovations to keep the organization at the cutting edge of technology advancement. Requirements:Minimum of 15 years in IT development/infrastructure, with at least 8 years as a Cloud Architect focused on Microsoft Azure.Proven experience in designing and implementing enterprise-scale cloud solutions.Deep understanding of Azure architecture, cloud security, governance, and compliance frameworks.Strong leadership skills and experience in developing policies and procedures for large-scale organizations.Certifications such as Azure Solutions Architect Expert or similar are highly preferred.Excellent communication, analytical, and project management skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "497_SRE - Site Reliability Engineer (Azure) - Onsite - Hybrid - PP no must": "Prashanth,\nBrillius\nprashanthn@brillius.com\nReply to: prashanthn@brillius.com\nPosition : SRE - Site Reliability Engineer (Azure) Location : O Fallon, MO Onsite/Hybrid roleDuration : Long Term Contract C2C Job Description :BizOps Provide technical leadership to BizOps team members with supporting our on-prem +Technical or team leadership experience ina 24/7 \u00abDatabase familiarity of experienceApplication and public cloud deployments environmentPerform daily monitoring and maintenance of all hardware/software infrastructureSupport, Support a highly available, highly-secure, hybrid production environment running on sLinux/Unix experience +Ansible/Chef experience iAzure experience Skill at automation (Ex. Orchestration,Linux systems in public cloud and on-prem deployments using various open source applicationsSet standards and define detailed processes for effectively managing a production environment with minimal downtime7 years of Virtualization experience experience|5+ years of Java application support experience Docker or equivalent container-based5+ years of Cloud technologies experience technology experience \u2019Be proactive and predictive, solve problems before they happenEmbrace DevOps practices & methodologiesProvide application and product support and troubleshooting of the Ethoca platformUse your experience to shape the future of our production infrastructureBe willing to learn new technologies and assist with architecture and designCommunicate clearly and effectively to all levels of the organization2+ years networking experience (Ex. TCP/IP, UDP, \u00abDesire to automate everything & anything Routing, Firewalls, load-balancers) *Software development education orComputer Science or Engineering degree or experience\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "498_Azure Cloud Systems Engineer || Must have experience with Azure OpenAI": "Ishavdeep Singh,\nCloud Think Technologies\nishavdeep@cloudthinktech.com\nReply to: ishavdeep@cloudthinktech.com\nJob Description Our client is seeking a talented and self-motivated Senior DevOps to support Azure OpenAI. You'll play a crucial role in designing, implementing, and maintaining the infrastructure and deployment processes for AI models. Your focus will be on ensuring seamless integration, continuous delivery, and efficient management of OpenAI services within an Azure environment. You will be assisting in developing our core team and helping to deliver a highly available platform for software solutions while collaborating with their larger Technology team globally. This is an exciting opportunity to play a leading role in changing the way that our client's products are delivered within the company and to their customers, devising and implementing a modern approach to infrastructure engineering that enables continuous delivery and releases features on-demand.The role holder will have excellent technical and project management skills as you'll be part of the Technology Infrastructure and Operations (TIO) team supporting, designing, migrating, and implementing products globally deployed within the Microsoft Azure Cloud Qualifications\" 5+ years of Systems Engineering experience\" BS Engineering/Computer Science or equivalent experience required Key Tasks Collaborate with software engineers, testers, and system administrators to ensure smooth integration of OpenAI services into applications. Foster a culture of teamwork and continuous improvement Set up monitoring, logging, and alerting for OpenAI services. Optimize resource utilization, scalability, and performance Develop and maintain infrastructure patterns supporting the promotion and deprecation of OpenAI and other LLM models within an API configuration Building and maintaining a set of tools that enable automation for creating and supporting Azure subscriptions in a large-scale tenant Improving existing Continuous integration across multiple product pipelines Oversee the day-to-day remediation of critical issues and building processes and efficiencies to eliminate the recurrence of the issue Maintain the health and security of infrastructure by monitoring and patching Ability to deliver TIO strategy in partnership with the business platform needs Participate in off-hours on-call support schedule Key Skills / Experience 3 5 years of experience with administering Microsoft Azure subscriptions in an Enterprise environment Understand Azure deployment processes such as Azure Blueprints, ARM templates, and PowerShell Modules Proficiency with the Azure Command Line (CLI) interface Deep knowledge of Azure Active Directory, IAM, and Roles Practiced in Azure infrastructure Virtual Machines, Azure Storage, Azure App Service and Database offerings Understand Azure VNETS, Express route, Internet networking, Virtual Private Networks and DNS Familiarity with Azure Cognitive Services and its component offerings Ability to use Terraform in an enterprise environment, including module creation and use Advanced Linux and Windows server administration experience Docker and Containerization, Kubernetes a plus Git and source control procedures An understanding of coding practices and DRY principles Authoring scripts with Bash and Python Executing and tracking tasks in an Agile environment Continuous Integration systems such as Jenkins and GitHub Actions Ability to work on several work streams with excellent time management Able to formulate and execute solutions to take into consideration the needs of multiple stakeholders A positive, constructive approach with an emphasis on collaboration and good execution Able to mentor and lead other members of the team TOP 3 must-have skills:a.) Experience with Microsoft Azure in an Enterprise setting. Support for multiple subscriptions and tenantsb.) Terraformc.) Familiarity of OpenAI or other Large Language Model (LLM) systemsCertifications:AZ-104 Azure Administrator Associate (Strong preference) AI-900 Azure AI Fundamentals (Nice to have) AI-102 Designing and Implementing a Microsoft Azure AI Solution (Nice to have)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "499_GCP Devops Lead - New jersey locals only - No GC": "Rakesh,\nBlue ocean venture\nrakesh.d@blue-oceanventures.com\nReply to: rakesh.d@blue-oceanventures.com\nRole : GCP Devops LeadLocation :NJ US Hybrid (Locals Only)ResponsibilitiesSuper solid into Terraform(Coding and writing modules)Design cloud infrastructure that is secure, scalable, and highly available on any AWS/GCP/Azure.,Work collaboratively with software engineering to define infrastructure and deployment requirements.Provision, configure and maintain AWS/GCP/Azure cloud infrastructure defined as code.Ensure configuration and compliance with configuration management tools,Administer and troubleshoot Cloud services, Build pipelines and application availability metrics.Build and maintain operational tools for deployment, monitoring, and analysis of AWS/GCP/Azure infrastructure and systemsWork on a collaborative cross-functional team including Cloud ops, Infrastructure provisioning, product management, UX design, Technology management, UI and back-end engineers, quality assurance, and operations\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "500_Looking for Azure Data Engineer with client Consumers Energy": "Manish,\nTask software solutions LLC\nrecruiter23@tasksoftwaresolutions.com\nReply to: recruiter23@tasksoftwaresolutions.com\nHi The role necessitates a deep understanding of data modeling, pipelines, orchestration, warehousing, schemas, and the ability to work independently. Client: Consumers EnergyInternal Job Title: Azure Data EngineerC2C Rate: $60.00 HRWork Location: Jackson, MI - Hybrid (Remote/On-Site) with weekly in office work days in Jackson, MI from day one of the assignment ongoing, subject to change in the future).Visa Status \u2013 OpenLocal to Michigan Only your role involves designing, implementing, monitoring, and maintaining data and analytical solutions on Microsoft Azure.Data Pipeline Design and Implementation:\u2022 Create and maintain data pipelines that facilitate data movement, transformation, and integration.\u2022 Ensure efficient and reliable data flow from source systems to target destinations. Data Storage Solutions:\u2022 Design and manage data storage solutions, including:\u2022 Azure Data Lake: Storing large volumes of raw data.\u2022 Azure SQL Database: Providing structured data storage.\u2022 Azure Blob Storage: Handling unstructured data.\u2022 Relational Databases: SingleStore, Microsoft SQL Server, Oracle Data Processing and Transformation:\u2022 Use tools like Databricks and dbt to process and transform data.\u2022 Leverage Azure Databricks, on-premise solutions, or Data Orchestration tools for running data transformations in production Data Integration:\u2022 Collaborate with data analysts and scientists to understand data requirements.\u2022 Design effective data workflows that enable data-driven decision-making. Performance Optimization:\u2022 Optimize SQL queries for better performance.\u2022 Ensure consistency between development and production environments. Security and Compliance:\u2022 Implement secure and compliant data processing pipelines.\u2022 Use Azure services and frameworks to produce cleansed and enhanced datasets for analysis Skills:\u2022 Microsoft Azure, Azure Databricks, and DBT\u2022 Knowledge of data catalog tools (e.g., Microsoft Purview)\u2022 Proficiency in data processing languages, including SQL and Python\u2022 Familiarity with Data Orchestration Tools\u2022 Excellent problem-solving skills and ability to work in a collaborative environment Education:\u2022 Bachelor\u2019s degree in computer science, Engineering, or related field.\u2022 Experience as a Data Engineer or similar role. Skills and Experience:Required Skills:\u2022 Microsoft Azure\u2022 Problem-solving\u2022 Data pipelines\u2022 Python\u2022 SQL Additional Skills:\u2022 Datasets\u2022 Performance optimization\u2022 SQL queries\u2022 Data integration Certifications & Licenses:\u2022 Proficiency in data processing languages, including SQL and Python\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "501_Excellent Opportunity for AWS Connect Developer at Washington, DC": "Muthukumaran K,\nSecurekloud\nmuthukumaran.krishnamoorthy@securekloud.com\nReply to: muthukumaran.krishnamoorthy@securekloud.com\nJob description:AWS Cloud with hands-on experience in Contact Centre technologiesHands on experience on developing Amazon Connect flows, queues, routing profiles etc.Hands on experience on integrating the lambda functions, Lex Bot in AWS Connect.Should have good experience on how end-to-end IVR works and integration of different AWS services.Should have good knowledge on Agile process.Should be a passionate learner and are motivated to take up new challenges\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "502_AWS Cloud Engineer (or) Application Support - C2C - Remote": "Subathraa,\nVysystems\nsubathraa@vysystems.com\nReply to: subathraa@vysystems.com\nAWS. Windows. Linux. ETL. Octopus. Twistlock.DNS mngmnt. certificates mngmnt\u2022 General knowledge of Data center designs and architecture and Basic Business contingency scenarios.\u2022 Industry best practices for building Cloud infrastructure as a code.\u2022 Various 3rd party tools/products available for monitoring and security (Splunk/AppD would be a +)\u2022 Deployment software experience (Octopus/Jenkins)\u2022 GIT experience\u2022 Networking (TCP, HTTP, DNS) and security (firewalls IP table, VPN, SSH)\u2022 Windows and Linux administration experience\u2022 Ability to troubleshoot complex issues solo/team\u2022 AWS Cloud services:\u2022 VPC's\u2022 ALB/NLB\u2022 EC2\u2022 ECS\u2022 S3\u2022 Storage Gateway\u2022 API Gateway\u2022 Lamda\u2022 SNS\u2022 SES\u2022 Route53 and Cloudfront\u2022 Encryption/KMS/CloudHSM\u2022 Inspector\u2022 IAM policies/roles\u2022 Shield\u2022 WAF\u2022 Cloudformation\u2022 Softskills\u2022 Self-starter\u2022 Ability to adapt in a dynamic environment.\u2022 Time management\u2022 Dependable & punctualExperience:\u2022 7+ years of Information Technology (IT) experience.\u2022 3+ years of Cloud Technology experience, preferably AWS.\u2022 AWS sysops or architect associate certification a +\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "503_Hot Requirements Node.Js Backened Developer + AWS only H1B And Us Citizens only and local from  Bay Area, CA.": "Princy Jain,\nMaintec\nprincy@maintec.com\nReply to: princy@maintec.com\nHI, Greetings for the day, we are hiring Node.Js Backened Developer + AWS for one of our client, Please share H1B And Us Citizens only and local fromBay Area, CA, as soon as possible. Role \u2013 Node.js Backend Developer + AWSLocation \u2013 Bay Area, CA (Hybrid at Mountain View office) 5+ years experience developing web, software applications 5+ years of experience in mid-tier and Backend APIs with NodeJs BS/MS in computer science or equivalent work experience 5+ years experience in the Software design/architecture process Experience with the entire Software Development Life Cycle (SDLC) 5+ years experience with web services (consuming or creating) with REST or SOAP Solid communication skills: Demonstrated ability to explain complex technical issues to both technical and non-technical audiences Strong understanding of the Software design/architecture process Experience with unit testing & Test Driven Development (TDD) Experience developing, maintaining, and innovating large scale, consumer facing web or mobile applications Experience with social, mobile, cloud/SaaS, big data, or analytics Experience designing and developing distributed scalable and highly reliable applications in Cloud. Experience with AWS or equivalent services is required. Familiar with the development challenges inherent with highly scalable and available web applications Always Be Learning: Experience with open source technologies (if no practical work experience w/ Big Data, or cutting edge front-end technology\u2014you\u2019re prototyping and/or researching the up and coming technology and solutionsExperience with various, modern web framework Thanks & Regards Princy Jain Maintec Technologies Inc.Email is the best way to reach: princy@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "504_DevOps - SRE Engineer with experience in ROS or NixOS - Fully Remote": "Justin Davis,\nGAC Solutions\njustin@gacsol.com\nReply to: justin@gacsol.com\nHi, I hope you're doing well! Looking for DevOps/SRE Engineer with experience in ROS or NixOS extremely important. Role: DevOps/SRE EngineerWork Location: Remote6-12 months JOB DESCRIPTION:Looking for DevOps/SRE Engineer with experience in ROS or NixOS extremely important. Graduate degree in engineering/CS or a related field with minimum 6+ years of experienceHands-on experience as DevOps/SRE with significant part of it in build/release engineeringExpert in developing complex multi-stage CI/CD pipelines in Jenkins/Gitlab-CI used acrossmultiple teamsStrong experience in building Infrastructure as a Code, self-serve pipelines using GitOpsmodelExperience using build/package tooling (eg. cmake, npm, nuget, maven/gradle, debian etc..)and fixing build dependency issuesProduction level experience in docker, Kubernetes and managing Kubernetes clusterStrong experience in provisioning/orchestration using Ansible, Terraform, HelmKnowledge of observability stack like Grafana, Prometheus, ElasticSearchComfortable with Linux, network troubleshooting and familiar with basic administrationExperienced in developing efficient, scalable code in popular languages likepython/Java/Go/bashKnowledge of VMware or other virtualization productsExperience with NixOS, building and packaging software in Nix or ROS is a mustSource Control software (Git), branching strategies and knowledge of best practicesWorking in a collaborative virtual environment, interfacing with other team members regularly,sharing knowledge via diagram, discussion, and exampleExcellent verbal and written communication skills.Thanks,Justin DavisSr Technical RecruiterE: justin@gacsol.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "505_We are looking for Kubernetes expert in Remote.": "JAY,\nSN Cloud Solutions LLC\njay@sncloudsolutions.com\nReply to: jay@sncloudsolutions.com\nTitle: Kubernetes expertLocation: RemoteDuration: 12 Month(s) Key Responsibilities:Kubernetes and EKS Management:Design, deploy, and manage Kubernetes clusters on AWS EKS.Implement best practices for Kubernetes architecture, including cluster setup, configuration, and management.Ensure high availability, scalability, and resilience of Kubernetes clusters.Container Orchestration:Develop and maintain Helm charts for Kubernetes resource management.Optimize Kubernetes deployments for resource efficiency and performance.Automate the deployment process using CI/CD pipelines and infrastructure-as-code tools like Terraform.Security and Compliance:Implement robust security practices within Kubernetes clusters, including RBAC, network policies, and pod security policies.Ensure compliance with organizational security standards and regulatory requirements.Manage secrets and sensitive data using Kubernetes secrets and AWS Secrets Manager.Monitoring and Logging:Set up and maintain monitoring and logging solutions for Kubernetes clusters using tools like Prometheus, Grafana, and AWS CloudWatch.Implement alerting mechanisms to proactively identify and resolve issues.Performance Optimization:Analyze and optimize the performance of Kubernetes clusters and containerized applications.Implement auto-scaling policies to dynamically adjust resources based on workload demands.Collaboration and Support:Collaborate with development teams to design and implement scalable and efficient containerized solutions.Provide technical support and guidance on Kubernetes-related issues.Conduct training sessions and knowledge sharing with team members to promote best practices.Spark Workloads (Good to Have):Conceptual understanding of Apache Spark workloads, including job execution, resource allocation, and performance optimization.Experience with running Spark jobs on Kubernetes is a plus.Required Skills and Experience:Technical Expertise:Extensive experience with Kubernetes and managing Kubernetes clusters in production environments.In-depth knowledge of AWS EKS and AWS services integration.Proficiency in containerization technologies such as Docker.Strong understanding of Helm, Terraform, and other infrastructure-as-code tools.Security and Networking:Expertise in implementing security best practices within Kubernetes environments.Knowledge of Kubernetes networking, including network policies and service meshes.Monitoring and Automation:Experience with monitoring and logging tools such as Prometheus, Grafana, and AWS CloudWatch.Strong automation skills using CI/CD tools and scripting languages (e.g., Python, Bash).Problem-Solving and Communication:Excellent problem-solving skills and the ability to troubleshoot complex issues.Strong communication and collaboration skills, with the ability to work effectively in a team environment.Preferred Qualifications:Certifications:AWS Certified Solutions Architect, AWS Certified DevOps Engineer, or similar certifications.Certified Kubernetes Administrator (CKA) or Certified Kubernetes Application Developer (CKAD). Thank you,JaySN Cloud Solutions LLC USA +1 972 327 7306 Email: Jay@sncloudsolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "506_Looking to hire for the Position of for the position of Azure DevOps Engineer Contract at Glastonbury, Connecticut": "Rakesh,\nAvance\nrakesh.g@avanceservices.us\nReply to: rakesh.g@avanceservices.us\nHI,Hope you are doing well. Azure DevOps EngineerContractCharlotte, North Carolina | Glastonbury, Connecticut Min 6+ yrs of hands on experience in managing complex release cycles and coordinating cross-functional teams. Strong understanding of CI/CD principles, including hands-on experience with tools such as Azure DevOps, Jenkins, GitLab CI/CD, MSBUILD, Sonar or others. Hands-on expertise with version control systems (GIT) and branching strategies. Ability to work closely with development teams, understanding their requirements and aligning pipeline processes. Effective communication skills to bridge the gap between development, operations, and other stakeholders. Familiarity with infrastructure as code (IaC) tools like Terraform, Azure Bicep, and scripting using PowerShell. Solid grasp of Agile methodologies and their application in release management. Demonstrated ability to coordinate cross-functional work teams toward task completion. Proactive identification and resolution of pipeline issues. Handling deployment failures, rollbacks, and incident management. Monitoring pipeline performance and addressing bottlenecks. Rakesh GuduruTeam Leadrakesh.g@avanceservices.us\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "507_Remote - Senior Devops Engineer with ADO exp (Healthcare domain)": "Bharath,\nThemesoft Inc.\nbharath.k@themesoft.com\nReply to: bharath.k@themesoft.com\nHello, Please find the below job opening and let me know your interest Role: Senior Devops EngineerLocation: Remote Job Description:Job Summary: The ideal candidate will have expertise in DevOps, ADO.NET, YAML, and Azure. This role involves analyzing business processes, identifying areas for improvement, and implementing solutions that align with our company's goals and objectives. Candidate must:- Possess a strong background in DevOps practices and tools, with hands-on experience in continuous integration and delivery. - Demonstrate proficiency in ADO.NET for efficient data access and manipulation. - Have extensive experience with YAML for configuration management and automation. - Show expertise in Azure services, including design, deployment, and management of cloud solutions. Required Skills: ADO based yaml templates for Java, python, terraform, Liquibase, vue js and javascript Top 3 Must Haves:1. DevOps processes understanding2. Hands on experience in working with Yaml pipelines in ADO3. Hands on experience in integrating ADO pipelines with different quality tools Nice to have:Terraform knowledge will be helpful. Roles & Responsibilities: Work with different teams to understand the requirement for creating ADO yaml pipelinesWork on documenting the needs in a fully understandable and readable formatCreate pipeline templates based on the requirementsReport on progress of template creation- Analyze business processes and identify areas for improvement to enhance efficiency and effectiveness. - Collaborate with stakeholders to gather and document business requirements and translate them into technical specifications. - Utilize DevOps practices to streamline development and deployment processes, ensuring continuous integration and delivery. - Implement and manage ADO.NET for data access and manipulation, ensuring seamless integration with various applications. - Develop and maintain YAML configurations for automated workflows and infrastructure as code. - Leverage Azure services to design, deploy, and manage scalable and secure cloud solutions. - Provide technical guidance and support to development teams, ensuring adherence to best practices and standards. - Conduct thorough testing and validation of implemented solutions to ensure they meet business requirements and perform optimally. - Monitor and analyze system performance, identifying and resolving issues to maintain high availability and reliability. - Prepare and present detailed reports and documentation to stakeholders, highlighting key findings and recommendations. - Stay updated with the latest industry trends and technologies, continuously enhancing skills and knowledge. - Foster a collaborative and innovative work environment, encouraging team members to share ideas and solutions. - Contribute to the company's overall success by driving impactful projects that align with strategic objectives.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "508_Devops or SRE - C2C - Whitehouse Station, NJ (Hybrid)": "mohan.t,\nVysystems\nmohan.t@vysystems.com\nReply to: mohan.t@vysystems.com\nJob Description:(Adoption and Support) DevOps/SRE to work for/with Infra(DTO) for filebeat onboarding of all Onprem VMs.(Automation) While taking care of Filebeat, He/she will also work for/with infra(DTO) for solution and build of automating the processInstall FilebeatUninstall FilebeatMonitor FilebeatUpgrade Filebeat\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "509_Immediate Hiring on AWS Architect  in NJ": "Haritha,\nAvance\nharitha.t@avanceservices.com\nReply to: haritha.t@avanceservices.com\nHi,Hope you are doing good. Please find the below JD and revert me iwth the resumes if you have anyRole: AWS Architect Location \u2013 Jersey City, NJ, USA (Hybrid mode of working \u2013 3 days a week from office)JD:Responsibilities:Design and develop Step Functions workflows to automate multi-step processes across various AWS services.Integrate Step Functions with AWS Lambda functions, SQS queues, SNS notifications, and other relevant services.Implement error handling, retries, and timeouts within Step Functions workflows.Monitor and troubleshoot Step Functions workflows to ensure smooth execution.Document and explain complex workflows for technical and non-technical audiences.Stay up-to-date on the latest features and best practices for AWS Step Functions.Collaborate with engineers and stakeholders to identify opportunities for workflow automation. Qualifications:Proven experience designing and implementing serverless workflows using AWS Step Functions.Strong understanding of AWS services, including Lambda, SQS, SNS, and CloudWatch.Experience with scripting languages like Python or Node.js (a plus).Experience with infrastructure as code (IaC) tools like Terraform or CloudFormation (a plus).Excellent problem-solving and analytical skills.Strong communication and collaboration skills. Preferrable:Experience with CI/CD pipelines and integrating Step Functions workflows.Experience with workflow visualization tools like AWS Step Functions Workflow Studio.Certifications: AWS Certified Solutions Architect - Associate or Professional\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "510_GCP Devops Lead - New jersey locals only - No GC": "Rakesh,\nBlue ocean venture\nrakesh.d@blue-oceanventures.com\nReply to: rakesh.d@blue-oceanventures.com\nRole : GCP Devops LeadLocation :NJ US Hybrid (Locals Only)ResponsibilitiesSuper solid into Terraform(Coding and writing modules)Design cloud infrastructure that is secure, scalable, and highly available on any AWS/GCP/Azure.,Work collaboratively with software engineering to define infrastructure and deployment requirements.Provision, configure and maintain AWS/GCP/Azure cloud infrastructure defined as code.Ensure configuration and compliance with configuration management tools,Administer and troubleshoot Cloud services, Build pipelines and application availability metrics.Build and maintain operational tools for deployment, monitoring, and analysis of AWS/GCP/Azure infrastructure and systemsWork on a collaborative cross-functional team including Cloud ops, Infrastructure provisioning, product management, UX design, Technology management, UI and back-end engineers, quality assurance, and operations\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "511_SRE or Devops local in San Jose, California only USC or GC": "Badal kanojia,\nStellentit\nbadal@stellentit.com\nReply to: badal@stellentit.com\nSRE Product ReliabilitySan Jose, CaliforniaPhone + SkypeJob Description:Key Responsibilities:Design and Implementation: Develop and implement observability solutions forKubernetes based applications using Fluentbit, Cloud Watch, StackDriver, Grafana Loki,Grafana Tempo, Prometheus, Envoy Health Probes, Open Telemetry, and ArgoCD.Monitoring and Logging: Configure and maintain logging pipelines using Fluentbit tocollect, process, and route logs for storage and analysis.Metrics and Tracing: Set up Prometheus for metrics collection and Grafana Tempo fordistributed tracing. Integrate these with Grafana for real-time monitoring and alerting viaopen telemetry.Telemetry: Utilize Open Telemetry to instrument applications for better traceability andobservability.CI/CD: Use ArgoCD for continuous deployment and ensure observability tools areintegrated into the CI/CD pipeline to deploy the observability suite.Observability Optimization: Analyze and optimize the performance of the observabilitystack to ensure minimal overhead and maximum efficiency.Troubleshooting: Proactively identify and resolve issues related to the observabilityinfrastructure. Collaborate with development and operations teams to troubleshoot andresolve incidents.Documentation and Training: Document observability processes and best practices.Provide training and support to other team members on the observability tools andtechniques.Required Skills and Qualifications:Experience: Proven experience as an SRE or in a similar role, with a strong focus onobservability in Kubernetes environments supporting applications in EKS in AWS.Technologies: Hands-on experience with Fluentbit, Cloud Watch, StackDriver, GrafanaLoki, Grafana Tempo, Prometheus, Envoy Health Probes, Open Telemetry, and ArgoCD.Kubernetes: In-depth knowledge of Kubernetes and container orchestration.Scripting and Automation: Proficiency in scripting languages such as Python, Bash, orsimilar for automation tasks.Monitoring and Logging: Strong understanding of monitoring, logging, and tracingconcepts and best practices.Problem Solving: Excellent analytical and problem-solving skills.Collaboration: Strong communication skills and the ability to work effectively in a teamenvironment.Continuous Improvement: A proactive attitude towards identifying opportunities forimprovement and implementing solutions.Preferred Qualifications:Certifications: Relevant certifications such as Certified Kubernetes Administrator(CKA) or Certified Kubernetes Application Developer (CKAD)Cloud Platforms: Experience with cloud platforms such as AWS and EKS.DevOps Practices: Familiarity with DevOps practices and tools.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "512_Onsite Contract Position for Kore.ai DevOps Engineer in Malvern, PA or Charlotte, NC": "Gaurav Sharma,\nUSG Inc.\ngaurav.ks@usgrpinc.com\nReply to: gaurav.ks@usgrpinc.com\nHi, Hope you are doing great.Please go through the job description given below and if you are interested do share an updated word copy of your resume and best time to reach you over the phone. Position: Kore.ai DevOps EngineerLocations: Malvern, PA or Charlotte, NCDuration: Contract Job Description:DevOps Engineer \u2013 Kore.ai (4-6 years of industry experience with 2-3 years hands on experience on the following)\u2022 Proven experience as a DevOps Engineer, particularly with Kore.ai platform.\u2022 Experience in deploying and maintaining contact center solutions and chatbots.\u2022 Proficiency with CI/CD tools like Jenkins and GitLab CI.\u2022 Strong understanding of cloud platforms, especially AWS and Azure.\u2022 Expertise in scripting languages such as Python and Bash.\u2022 Familiarity with containerization technologies like AWS ECS, ECR, Docker and Kubernetes.\u2022 Experience with monitoring tools like Splunk, Honeycomb, and Prometheus.\u2022 Strong problem-solving skills and attention to detail.\u2022 Excellent communication and collaboration abilities.\u2022 Relevant certifications in DevOps, cloud platforms, or chatbot development are a plus. Key Responsibilities:1. Deployment and Management:\u2022 Should have experience of managing a team of 6-8 members, working in the similar skills to guide and help them as and when needed.\u2022 Deploy Kore.ai contact center solutions and chatbots across various environments.\u2022 Automate deployment processes using CI/CD pipelines with tools like Jenkins and GitHub Actions CI/CD solutions.\u2022 Ensure high availability, scalability, and reliability of the deployed solutions on AWS platforms.\u2022 Monitor the performance and health of Kore.ai chatbots and contact center solutions using tools like Splunk, Honeycomb, Prometheus etc.\u2022 Implement proactive measures to maintain system health and performance.\u2022 Troubleshoot and resolve issues related to chatbot functionality and performance.2 Collaboration and Support:\u2022 Work closely with development, QA, and customer support teams to ensure smooth deployment and operation of chatbots.\u2022 Provide technical support and guidance to internal teams and external clients as needed.\u2022 Collaborate with stakeholders to gather requirements and implement new features using Agile methodologies.3 Security and Compliance:\u2022 Implement security best practices to protect chatbot and contact center data.\u2022 Ensure compliance with relevant regulations and industry standards.\u2022 Conduct regular security audits and vulnerability assessments.4 Continuous Improvement:\u2022 Identify areas for improvement in deployment and maintenance processes.\u2022 Stay updated with the latest industry trends and best practices in DevOps and chatbot technologies.\u2022 Develop and maintain documentation for deployment processes and maintenance procedures. Qualifications:\u2022 Education: Bachelor\u2019s degree in Computer Science, Information Technology, or related field.\u2022 Experience:\u2022 Proven experience as a DevOps Engineer, particularly with Kore.ai platform.\u2022 Experience in deploying and maintaining contact center solutions and chatbots. Skills:\u2022 Proficiency with CI/CD tools like Jenkins and GitLab CI.\u2022 Strong understanding of cloud platforms, especially AWS and Azure.\u2022 Expertise in scripting languages such as Python and Bash.\u2022 Familiarity with containerization technologies like AWS ECS, ECR, Docker and Kubernetes.\u2022 Experience with monitoring tools like Splunk, Honeycomb, and Prometheus.\u2022 Strong problem-solving skills and attention to detail.\u2022 Excellent communication and collaboration abilities.\u2022 Certifications: Relevant certifications in DevOps, cloud platforms, or chatbot development are a plus. Thanks with regards,Gaurav Sharma | Sr. Technical RecruiterEmail ID: gaurav.ks@usgrpinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "513_Cloud ETL Developer": "Anusha,\nHanker Systems Inc\nanushaj@hankersystems.com\nReply to: anushaj@hankersystems.com\nHello Everyone,Hope Your doing good please find the below requriment below,Job Title:Cloud ETL DeveloperRichmond, VA6+ MonthsWeb Interview We Need Local to VirginiaSkillsAmount UsedLast UsedDesigns and develops systems for the maintenance of the Data Asset Program, ETL processes, and business intelligence. Design and supports the DW database and table schemas for new and existent data sources for the data hub and warehouse. Design and development of Data Work closely with data analysts, data scientists, and other data consumers within the business in an attempt to gather and populate data hub and data Advanced understanding of data integrations.\u00b7 Strong knowledge of database architectures, strong understanding of ingesting spatial data Ability to negotiate and resolve conflicts,\u00b7 Ability to effectively prioritize and handle multiple tasks and projects Excellent computer skills and be highly proficient in the use of Ms Word, PowerPoint, Ms Excel, MS Project, MS Visio, and MS Team Foundation Server Experience with key data warehousing architectures including Kimball and Inmon, and has a broad experience designing solutions using a broad set of da expertise in Data Factory v2,Data Lake Store, Data Lake Analytics, Azure Analysis Services, Azure Synapse IBM Datastage, Erwin, SQL Server (SSIS, SSRS, SSAS), ORACLE, T-SQL, Azure SQL Database, Azure SQL Datawarehouse. Operating System Environments (Windows, Unix, etc.).\u00b7 Scripting experience with Windows and/or Python, Linux Shell scripting Experience in AZURE Cloud engineering Job Description: *Local Richmond, VA candidates ONLY required due to onsite requirement**This position requires onsite 3 days a week with 2 remote**Contractor will be responsible for purchasing parking through VDOT\u2019s Parking Management Office or procuring their own parkingJob Description: Data Analyst/SR ETL DeveloperThe Virginia Department of Transportation (VDOT)Information Technology Division (ITD) is seeking a Master Data Analyst with demonstrated experience in data analytics to work as a key member of Enterprise Data Asset team. This analyst will support teams working in Agile (Sprint) to analyze datasets to be made available in a cloud-based data management platform that will support the agency to produce master data with data governance.Responsibilities include analyzing source systems which contain a spatial component for candidate datasets; documenting business processes and data lifecycle; developing data requirements, user stories and acceptance criteria; and testing strategies. Develop ETL to extract business data and spatial data and load it into a data warehousing environment. Design and test the performance of the system. Consult with various teams to understand the company\u2019s data storage needs and develop data warehousing options. Deep knowledge of coding languages, such as python, Java, XML, and SQL. Well-versed in warehousing architecture techniques such as MOLAP, ROLAP, ODS, DM, and EDW.VDOT is a fast-paced organization with very high standards for work quality and efficiency. This position is expected to handle multiple projects, and remain flexible and productive, despite changing priorities and processes. Ongoing improvement and efficiency are a part of our culture, and each team member is expected to proactively contribute to process improvements.Responsibilities:\u2022 Work with the Project team members and business stakeholders to understand business processes and pain points\u2022 Develop expertise in source system datasets and data lifecycle\u2022 Profile source data which may contain a spatial component; review source data and compare content and structure to dataset requirements; identify conflicts and determine recommendations for resolution\u2022 Conduct entity resolution to identify matching and merging and semantic conflicts\u2022 Elicit, record, and manage metadata\u2022 Diagram current processes and proposed modifications using process flows, context diagrams and data flow diagrams\u2022 Decompose requirements into Epics and Features and create clear and concise user stories that are easy to understand and implement by technical staff\u2022 Utilize progressive elaboration; map stories to data models and architectures to be used by internal staff to facilitate master data management\u2022 Identify and group related user stories into themes, document dependencies and associated business processesDiscover and document requirements and user stories with a focus on improving both business and technical processing\u2022 Assist Product Owner in maintaining the product backlog\u2022 Create conceptual prototypes and mock-ups\u2022 Collaborate with staff, vendors, consultants, and contractors as they are engaged on tasks to formulate, detail and test potential and implemented solutions\u2022 Perform Quality Analyst functions such as defining test objectives, test plans and test cases, and executing test cases\u2022 Coordinate and Facilitate User Acceptance Testing with Business and ensure Project Managers/Scrum Masters are informed of the progress\u00b7Designs and develops systems for the maintenance of the Data Asset Program(Data Hub), ETL processes, ETL processes for spatial data, and business intelligence.\u00b7Develop a new data engineering process that leverage a new cloud architecture and will extend or migrate our existing data pipelines to this architecture as needed.\u00b7Design and supports the DW database and table schemas for new and existent data sources for the data hub and warehouse. Design and development of Data Marts.\u00b7Work closely with data analysts, data scientists, and other data consumers within the business in an attempt together and populate data hub and data warehouse table structure, which is optimized for reporting.\u00b7The Data developers partners with Data modeler and Data architect in an attempt to refine the business\u2019s data requirements, which must be met for building and maintaining Data Assets.\u00b7QualificationsRequired:The candidate must have a minimum of 10 years of experience delivering business data analysis artifacts5+ years of experience as an Agile Business Analyst; strong understanding of Scrum concepts and methodologyExperience organizing and maintaining Product and Sprint backlogsExperience translating client and product strategy requirements into dataset requirements and user storiesProficient with defining acceptance criteria and managing acceptance process\u00b7Exceptional experience writing complex sql queries for Sql Server and Oracle\u00b7Experience with Azure Databricks\u00b7Experience with ESRI ArcGISExperience with enterprise data managementExpertise with Microsoft Office products (Word, Excel, Access, Outlook, Visio, PowerPoint, Project Server)Experience with reporting systems \u2013 operational data stores, data warehouses, data lakes, data marts\u00b7The candidate must have exceptional written and oral communications skills and have the proven ability to work well with a diverse set of peers and customers Preferred Skills:\u00b7 Advanced understanding of data integrations.\u00b7Strong knowledge of database architectures\u00b7Strong analytical and problem solving skills\u00b7Ability to build strong relationships both internally and externally\u00b7Ability to negotiate and resolve conflicts\u00b7Ability to effectively prioritize and handle multiple tasks and projects\u00b7Strong written and verbal communication skills\u00b7Desire to learn, innovate and evolve technologyComputer Skills/MS Office/Software:\u00b7Excellent computer skills and be highly proficient in the use of MS Word, PowerPoint, MS Excel, MS Project, MS Visio, and MS Team Foundation Server, which will all be necessary in the creation of visually and verbally engaging ETL, data designs and tables as well as the communication of documentation and reporting.\u00b7Deep passion for data analytics technologies as well as analytical and dimensional modeling. The candidate must be extensively familiar with ETL(Extraction, Transformation & Load), data warehousing, and business intelligence tools such as business objects, PowerBI and Tableau.\u00b7The candidate must also have vast knowledge of database design and modeling in the context of data warehousing.\u00b7Experience with key data warehousing architectures including Kimball and Inmon, and has a broad experience designing solutions using a broad set of data stores (e.g., HDFS, Azure Data Lake Store, Azure Blob Storage, Azure SQL Data Warehouse, Azure Cosmos DBTechnologies Required:\u00b7Data Factory v2,Data Lake Store, Data Lake Analytics, Azure Analysis Services, AZURE Synapse\u00b7IBM Datastage, Erwin, SQL Server (SSIS, SSRS, SSAS), ORACLE, T-SQL, Azure SQL Database, Azure SQL Datawarehouse.\u00b7Operating System Environments (Windows, Unix, etc.).\u00b7Scripting experience with Windows and/or Python, Linux Shell scripting anushaj@hankersystems.com 813-438-3442\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "514_Job Description || Sr. Cloud Engineer With AWS Connect  ||Washington DC": "Prem krishna,\nTHEMESOFT\nprem@themesoft.com\nReply to: prem@themesoft.com\nHI,Good Day!!!I\u2019m Prem with Theme Soft. I came across your profile wanted to reach out to discuss this exciting opportunity I have with my client for Sr. Cloud Engineer With AWS Connect role. Your profile looks to be a great match for the requirement and would appreciate your thoughts once review the below requirement. Job Title: Sr. Cloud Engineer With AWS Connect Location: Washington DCDuration: Long term Job description:AWS Cloud with hands-on experience in Contact Centre technologiesHands on experience on developing Amazon Connect flows, queues, routing profiles etc.Hands on experience on integrating the lambda functions, Lex Bot in AWS Connect.Should have good experience on how end-to-end IVR works and integration of different AWS services.Should have good knowledge on Agile process.Should be a passionate learner and are motivated to take up new challenges Thanks & RegardsPrem Krishna _______________________ Technical RecruiterEmail:prem@themesoft.comLinked in ID : - linkedin.com/in/krishna-krish-859578216Web: themesoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "515_Azure DevOps Engineer at Glastonbury,CT - Onsite": "Neha Shedule,\nAvance Consulting\nneha.s@avanceservices.com\nReply to: neha.s@avanceservices.com\nHi Folks,Job DescriptionAzure Devops EngineerMin 6+ yrs of hands on experience in managing complex release cycles and coordinating cross-functional teams. Strong understanding of CI/CD principles, including hands-on experience with tools such as Azure DevOps, Jenkins, GitLab CI/CD, MSBUILD, Sonar or others. Hands-on expertise with version control systems (GIT) and branching strategies. Ability to work closely with development teams, understanding their requirements and aligning pipeline processes. Effective communication skills to bridge the gap between development, operations, and other stakeholders. Familiarity with infrastructure as code (IaC) tools like Terraform, Azure Bicep, and scripting using PowerShell. Solid grasp of Agile methodologies and their application in release management. Demonstrated ability to coordinate cross-functional work teams toward task completion. Proactive identification and resolution of pipeline issues. Handling deployment failures, rollbacks, and incident management. Monitoring pipeline performance and addressing bottlenecks. Regards,NehaEx: +1732 466 2855 Ext : 6241neha.s@avanceservices.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "516_Azure Infrastructure Engineer. Onsite TX locals": "Rakesh,\nRavin Software Inc\nrakesh.b@ravinsoft.com\nReply to: rakesh.b@ravinsoft.com\nRole: Azure Infrastructure Engineer.Location: Dallas, TX (Local only)Job Description: Azure Infrastructure L2 EngineerWe are seeking a highly experienced and skilled Azure Infrastructure L2 Senior Engineer to join our IT team. This role is crucial in ensuring the robust operation, maintenance, and optimization of our Azure cloud infrastructure. The ideal candidate will have extensive hands-on experience with Azure, strong technical skills, and the ability to provide advanced support and troubleshooting services. Proficiency in Terraform, ServiceNow, automation, and DevOps practices is essential.Roles and responsibilities:Advanced Infrastructure Monitoring: Proactively monitor Azure infrastructure to ensure optimal performance, availability, and security.Incident and Problem Management: Handle complex infrastructure-related incidents and problems, performing advanced troubleshooting and root cause analysis.ServiceNow Ticket Handling: Manage and resolve high-priority tickets using the ServiceNow platform, ensuring adherence to SLAs.Maintenance and Optimization: Perform advanced maintenance tasks, including system updates, patch management, performance tuning, and optimization.Automation and Scripting: Develop, implement, and maintain automation scripts and processes using tools such as PowerShell, Azure CLI, and Terraform.DevOps Practices: Collaborate with DevOps teams to integrate infrastructure management with CI/CD pipelines, promoting a seamless development and deployment process.Infrastructure as Code (IaC): Utilize Terraform to manage infrastructure as code, ensuring consistency, scalability, and reliability.Documentation and Knowledge Sharing: Maintain comprehensive documentation of configurations, processes, and procedures. Mentor and train junior engineers.Security and Compliance: Ensure the infrastructure complies with security policies, best practices, and regulatory requirements.Collaboration and Leadership: Work closely with other IT teams, providing guidance and leadership to ensure effective and efficient infrastructure management.Continuous Improvement: Identify and implement improvements to processes, tools, and technologies to enhance the overall performance and reliability of the Azure infrastructure.Required Mandatory experienceAzure Administration: Minimum of 5 years of hands-on experience managing and supporting Azure environments.Terraform Expertise: Extensive experience with Terraform for managing infrastructure as code, including creating, modifying, and managing infrastructure resources.ServiceNow Proficiency: Proficiency in using ServiceNow for advanced ticket handling, including creating, updating, resolving high-priority tickets, and managing workflows.Automation Skills: Strong experience in automation using PowerShell, Azure CLI, and other relevant tools.DevOps Practices: Proven experience in integrating infrastructure management with CI/CD pipelines and collaborating with DevOps teams.Troubleshooting and Problem Resolution: Demonstrated ability to diagnose and resolve complex technical issues efficiently. RakeshRavin Software Inc2600 K Ave, Suite#224,Plano, TX 75074Email Id : rakesh.b@ravinsoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "517_Azure Data Engineer :: 14+ years of Experience :: Need local to NYC or nearby location": "Manohar,\nSusweeinfo\nmanohar@susweeinfo.com\nReply to: manohar@susweeinfo.com\nPOSITION: Azure Data EngineerLOCATION: Hybrid in NYC Experience: 14+ years Need local to NYC or nearby location Passport number is mustCertified candidates only JOB DESCRIPTION1.Azure Platform Experience:The candidate should have hands-on experience with various Azure services including but not limited toAzure Data Factory, Databricks, Data Lake, and Power BI.They should be able to design, build, and maintain ETL pipelines, manage data lakes,and create insightful reports and visualizations.Experience with Azure SQL Database, Azure Synapse Analytics, Azure COSMOS DB , Azure ML, other Azure services is a plus.SQL skill is a must to have And .NET C# skills will be a great plus.2.Azure Data Engineer: Design, implement, and maintain data solutions using Azure cloud services. Develop ETL processes, optimize data pipelines, and ensure data quality and security. Collaborate with teams to deliver scalable data architecture supporting analytics and business intelligence.3.Security on Azure Platform:The candidate should have a strong understanding of security protocols within the Azure platform.This includes knowledge of identity and access management, network security,encryption methods, secure data communication, and securing serverless architectures.4.Data Governance:The candidate should have experience in implementing data governance strategies.This includes setting up data governance frameworks, defining data ownership,ensuring data quality, and implementing data privacy and compliance measures.Knowledge of data cataloging, data lineage, and metadata management is also important. 5.Certifications: Azure certifications such as Azure Security Engineer Associate, or Azure Solutions Architect Expert are highly desirable. Azure Databricks Data Engineer Professional Certificate is required. Databricks Machine Learning Certificate Preferable. TECHNICAL SKILLSAbility to intrepid, modify and write scripts and SQL queriesApps/.Net DeveloperAzure Cloud Data EngineerAzure DatabricksAzure Data FactoryAzure logic AppPowerBi Report ServerPython NICE TO HAVE.NET C# skillsSSIS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "518_AWS Cloud Engineer-Atlanta, GA(Locals only)": "santhosh,\nInfinity Quest\nsanthosh.a@i-q.co\nReply to: santhosh.a@i-q.co\nAWS Cloud EngineerAtlanta, GA Please see below the JD.* AWS Cloud engineer, hands-on exp on EC2, S3, ROS, EBS, VPC and EKS* Good exp in installing and managing the Kafalta permit* Hands-on exp Kube helm charts, Rabbiting helm chart* Troubleshoot the application deploy Kubernetes* Good knowledge of the Cassandra database installations and managing the databases on AWS cloud and on perm* Hands-on exp on the GitLab CI/CD pipelines\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "519_HiringCloud Security Engineerslocal from Texas and Washinton,": "Reeti,\nMaintec\nreeti@maintec.com\nReply to: reeti@maintec.com\nHI, Greetings for the day, we are hiring Cloud Security Engineers for one of our client, please share good match and local from Texas and Washinton, as soon as possible.Role: - Cloud Security Engineers Location: - Frisco, TX \u2013 Day 1 ONSITE/Bothell, WA \u2013 Day 1 ONSITEResponsible for planning, designing, testing and implementing computer and network security infrastructure design and implementation for project including Cloud-Native container as a service.Required skillsDeep understanding of cloud computing principles, including virtualization, containerization, microservices, and serverless computing; Risk Management, RHCOS security, container security, Kubernetes security, IAM security, network security, auditing, encryption, secrets management and data protection, securing CI/CD Analyze environments to identify both technical and operational challenges while making recommendations and developing solutions for improvement.Lead complex or high severity troubleshooting and incident/problem resolutions with other security or cloud teamsMaintain knowledge of current developments in cloud, CaaS and cybersecurity, maintaining of threats to It environmentsBachelor\u2019s degree in IT, Cybersecurity or related field or equivalent experience5+ years of experience in Information security with 4+ years of experience in cloud security3+ years of experience of cloud container security experience.Experience with cloud infrastructure as code tools such as Terraform, CloudFormation, and Azure Resource Manager. Observability: Tracing/Metrics/Logs and Dashboards for Platform and Application workloads (Promethius, Grafana, Vector Openshift logging) Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.(Github, Gitea, Gitguardian, ) Experience with static container scanning Trivy, Snyk. sBOM (Bill of Material): Syft/Grype Experience with runtime container security, Falco, Red Hat ACS Experience with Red Hat OpenShift and Openstack cloud platforms, Advanced cluster security, Advanced cluster management Experience with Policy/Regulation compliance: OPA, Red Hat ACS, Kyverno Strong knowledge of hybrid cloud, AWS, GCP, Azure and KurbenetesService Mesh isolation Vulnerability and threat managementApplication pen testingHands on experience with HashiCorp Vault, Cyberark or similar (PAM, secrets, certificate management platform) Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.Static Container Scanning: Trivy, Snyk. sBOM (Bill of Material): Syft/Grype Reporting/Observability: Grafana, Prometheus, Red Hat Advanced Cluster Security.Professional certifications CISSP, CEH, CDP (Certified DevSecOps Professional)Thanks & Regards Reetti Mnedratta Maintec Technologies Inc.Email is the best way to reach: reeti@maintec.comThanks & Regards Reetti Mendratta Maintec Technologies Inc.Email is the best way to reach: reeti@maintec.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "520_Enterprise Architect With Kafka &amp; AWS": "raj kumar,\nNITYA Software Solutions Inc\nrec2@nityainc.com\nReply to: rec2@nityainc.com\nHi,I hope you are doing well.I am Raj from NITYA Software Solutions Inc.I've included below the job description, please let me know if you are interested, and reply with one updated resume, which I've expected.Role: Enterprise Architect With Kafka & AWSDesign and implement Kafka architecture and data pipelines. Collaborate with business leaders, project teams, and application managers to create and integrate Kafka solutions. Develop architecture strategies and artifacts for technology initiatives. Ensure solutions adhere to corporate policies and industry best practices. Lead architecture efforts and provide guidance to project teams. Stay updated with emerging technologies to meet customer needs. Qualifications: Experience in systems, applications, or architecture planning. Experience with industry-standard database modeling languages. Experience with databases such as Oracle, DB2, SQL Server, or Teradata. Proficiency in data management, modeling, and movement. Experience with risk applications and credit risk technology systems. Desired Skills: Strong communication and interpersonal skills. Ability to influence and collaborate at all organizational levels. Knowledge of data governance, BI tools, and analytical methods. Experience with Big Data tools such as Hadoop, Spark, and Kafka. Agile methodology experience.Thank you,RajkumarEMAIL ID:rec2@nityainc.comNITYA Software Solutions Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "521_Urgent Need-- Senor DevOps Engineer, 100% Remote": "SAPNA,\nITECS\nsapna@itecsus.com\nReply to: sapna@itecsus.com\nPosition: Senior DevOps EngineerLocation: Nashville, TN. 100% Remote Description: Minimum of 10 years of hands-on experience working on DevOps tools and technologies in a complex environment. 3-5 years of hand-on experience working with GCP (Google Cloud Platform)Experience working with Terraform modulesHands-on coding skills in Golang and Python scripting Working experience on CI/CD pipelinesExperience working with GitLabWriting Unit Tests using GolangExcellent communication is must to have Bachelor\u2019s of higher degree in computer science or equivalent.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "522_GCP SRE lead with Linux or Kubernetes at Atlanta, GA (Hybrid) - Contract": "Deepank Bansal,\nZensar\ndeepank@tanishasystems.com\nReply to: deepank@tanishasystems.com\nGreetings for the day! My name is Deepank and I'm an IT recruiter at Tanisha Systems Inc. Our records show that you are an experienced IT professional. This experience is relevant to one of my current openings. Please take a look at the Job Description and let me know if you're interested. Job Title: GCP SRE lead with Linux/ KubernetesLocation: Atlanta, GA (Hybrid)Duration: Contract Job DescriptionWe need a GCP SRE lead with Linux/ Kubernetes Platform developmental experience who can work Onsite in Atlanta GA( 3 days Onsite and 2 days remote ). Responsibilities\u2022 Manage Cloud (GCP) & legacy (Linux/ Windows) infrastructure for the projects in the portfolio that includes production, QA, Dev and other environments\u2022 Assist in designing, deploying, and maintaining GCP & legacy infrastructure to support application workloads.\u2022 Assist in configuring and optimizing GCP services.\u2022 Build & manage pipelines using Jenkins.\u2022 Deployment using ArgoCD\u2022 Manage and administer Kubernetes clusters, including cluster provisioning, deployment, and troubleshooting.\u2022 Monitor and optimize cluster performance, capacity, and security under guidance.\u2022 Knowledge of cloud monitoring on GCP, splunk, grafana, prometheus\u2022 Fluency with one or more current generation scripting language used by SRE/DevOps professionals.\u2022 Troubleshoot and fix issues utilizing a systematic problem-solving approach\u2022 Ensure all operational activities adhere to industry regulations, compliance standards, and cybersecurity best practices.\u2022 Ability to work on legacy applications along with cloud native applications\u2022 Evaluate existing operational processes and identify areas for improvement, efficiency gains, and automation.\u2022 Implement best practices and standard operating procedures to streamline service delivery. Experience required\u2022 At least 5 years of experience in DevOps and SRE.\u2022 BE in Computer Science, Computer Engineering\u2022 Excellent communication and stakeholder management skills.\u2022 Enjoys taking product/project ownership and being accountable for results.\u2022 Ability to work a flexible schedule based on project needs\u2022 Should have experience on GCP and implemented DevOps & SRE strategies.\u2022 Knowledge of Linux & Windows is desired\u2022 Knowledge of K8S, Jenkins, ArgoCD, splunk, grafana, prometheus is required Thanks & best regards, Deepank BansalTanisha Systems, Inc.An Information Technology CompanyDesk: (732) 934-6047. Mob: (908)666-1508Email: deepank@tanishasystems.comWeb : www.tanishasystems.comTanisha Systems Inc. | 99 Wood Ave South | 350 5th Avenue | Suite # 308 | Iselin, NJ 08830\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "523_Sr Cloud Enterprise Security Architect - Hybrid": "satyesh,\nTanisha Systems\nsatyesh@tanishasystems.com\nReply to: satyesh@tanishasystems.com\nRole:- Sr Cloud Enterprise Security ArchitectLocation:- Frisco TX \u2013 Hybrid (3 days onsite , 2 days remote)ContractJob/Responsibility Profile: \u2022 Design and develop multi-tenant solutions for enabling cloud platform as service\u2022 Deploy and Operate multi-cloud security solutions/platforms at Enterprise scale\u2022 Develop end-to-end technical solutions in security space\u2022 Develop self-service solutions to onboard customers and manage users on the platforms\u2022 Assess the customers' security architecture, requirements and provide guidance\u2022 Design and develop policies to improve security posture and prevent threat exposure\u2022 Identify and adapt modern tools, principles and technologies to improve security across cloud landscape\u2022 Support cloud customers through cloud-native architecture guidance, security architecture guidance, policy remediations, etc.\u2022 Work with ITSM functions (Change management, Incident management, Problem management, Request management) as they apply to tools and platforms used by the team Technical Skills/Experience:\u2022 Experience working in DevOps/GitOps teams\u2022 Experience developing Infrastructure and Operations code, Platforms, and Automations\u2022 Experience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate \u2022 Understanding of all the basic services provided by CSPs (AWS, Azure and GCP)\u2022 Knowledge and hands-on experience of interacting with CSP APIs\u2022 Deep knowledge of IAM, Policies, Network and other security services:-o Authoring IAM policieso Authoring Organization Policies o Developing private network based applications (using private endpoints, Vnet integrations, IPSec)\u2022 Developing Git Pipelines for managing platforms and operations\u2022 Experience in Java Springboot/Python/GoLang development\u2022 Experience in developing SAML, OAuth based applications\u2022 Experience working with IaC tools such as Terraform, CloudFormation, or ARM templates.\u2022 Experience in K8s development\u2022 General experience working within ITSM processes (Change, Incident, Problem, Request management) in an Enterprise context\u2022 Certifications such as AWS Certified Security Specialty, Azure Security Engineer Associate, or GCP Professional Cloud Security Engineer are a plus Skills Years of Experience Rating out of 10 Experience working in DevOps/GitOps teams Experience developing Infrastructure and Operations code, Platforms, and Automations Experience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate Understanding of basic services provided by CSPs (AWS, Azure, and GCP) Knowledge and hands-on experience of interacting with CSP APIs Deep knowledge of IAM, Policies, Network, and other security services (Authoring IAM policies, Organization Policies, Private network-based applications) Experience in Java Springboot, Python, GoLang development Developing Git Pipelines for managing platforms and operations Experience in developing SAML, OAuth based applications Experience working with IaC tools such as Terraform, CloudFormation, or ARM templates Experience in Kubernetes (K8s) development General experience working within ITSM processes (Change, Incident, Problem, Request management) in an Enterprise context Certifications such as AWS Certified Security Specialty, Azure Security Engineer Associate, or GCP Professional Cloud Security Engineer Thanks & Regards, SatyeshASM- Talent AcquisitionTanisha Systems Inc.Address: 99 Wood Ave South, Suite # 308, Iselin, NJ 08830Web: www.tanishasystems.comEmail: satyesh@tanishasystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "524_Senior AWS Connect Developer": "Rahul Singh,\nTek Inspirations LLC\nrahul.singh@tekinspirations.com\nReply to: rahul.singh@tekinspirations.com\nJob Description -Title: Senior AWS Connect DeveloperLocation: Baltimore - Hybrid (Local)Clearance Type: Public Trusted ClearanceThese candidates can work remote but will have to come on site to Baltimroe MD 2 times in the first week to do badging and receive their laptop.Basic Qualifications:\u00b7 Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field\u00b7 7 years of experience working with large enterprise framework\u00b7 7 years of hands-on experience working with AWS Cloud platform\u00b7 3 years of hands-on experience working with Amazon Connect\u00b7 3 years of experience using Node.js, Python or Java for development\u00b7 3 years of experience in designing call routing & messaging\u00b7 Excellent interpersonal and communication skillsRequired SkillsKnowledge or experience with CCaaS with Artificial Intelligence (AI) strategies and solutions.Knowledge and experience with integrating Pega workflows with CCaaS.Knowledge and experience with building reports and dashboards preferably with Tableau.Solid AWS Cloud and AWS Connect Developer experienceExtensive experience and knowledge with IVR, Automatic Call Distribution (ACD), Skill-Based Routing, Live ChatExperience with process flow documentation creation.Strong verbal and written communication skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "525_Sr Cloud Network Engineer || Bellevue ||": "Rupa Jha,\nChabeztech\nrupa@chabeztech.com\nReply to: rupa@chabeztech.com\nTitle: Sr Cloud Network Engineer Location: Bellevue Key Responsibilities \u2022 Oversee the network onboarding process for new users and systems into Cloud environment \u2022 Provision and configure network resources in Cloud, ensuring compliance with security policies and government regulations. \u2022 Implement secure network architectures, including Virtual Private Clouds (VPCs), subnets, routing tables, and network access control lists across AWS, Azure, and other cloud platforms. \u2022 Configure and manage cloud networking services for secure connectivity between on-premises and cloud environments, such as AWS Direct Connect, Azure VPN Gateway, and transit gateways. \u2022 Implement network security controls, such as security groups, network firewalls, and web application firewalls to protect against unauthorized access and cyber threats. \u2022 Monitor network traffic and security logs using cloud services for flow logs, activity trails, and threat detection to identify and respond to potential security incidents. \u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud network infrastructure. \u2022 Conduct regular network assessments and audits to ensure compliance with internal and external requirements. \u2022 Develop and maintain comprehensive network security policies, procedures, and documentation in compliance with security standards. \u2022 Provide technical support and troubleshooting for Cloud network-related issues \u2022 Stay up-to-date with the latest cloud networking services, security features, and best practices across multiple platforms Qualifications \u2022 Experience with network onboarding and provisioning in cloud environments. \u2022 Knowledge of government network security standards and compliance requirements (e.g., FedRAMP, FISMA). \u2022 Familiar with NIST - 171 security framework, Azure Defender, AWS security hub, Guarduty, Macie \u2022 In-depth knowledge of network security principles, protocols, and best practices for secure network design and implementation in the cloud. \u2022 Familiarity with security regulations, standards, and compliance requirements for cloud network environments. \u2022 Hands-on experience with cloud networking services like VPCs, Direct Connect, VPN gateways, transit gateways, network firewalls, and web application firewalls across AWS, Azure, and other major cloud providers. \u2022 Strong understanding of network security controls, firewalls, intrusion detection/prevention systems, and network monitoring tools in the cloud. \u2022 Strong communication and documentation skills for collaborating with cross-functional teams. \u2022 Experience with automation tools (e.g., Python, Terraform) for network configuration and management \u2022 Relevant certifications such as AWS Certified Advanced Networking - Specialty, Azure Network Engineer Associate, or similar are preferred.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "526_DevOps Engineer (Mobile)": "Nick,\nCompusoft Integrated Solutions\nnick@compusoft-is.com\nReply to: nick@compusoft-is.com\nHi, Greetings from Compusoft, Please review the following requirement and let me know if you\u2019re interested. Please send your updated profile with contact details Title: DevOps Engineer (Mobile)Work Location: Detroit, MI / Charlotte, NC (Hybrid)Duration: Long Term Mandatory Skills:\u2022 Experience maintaining large code repositories contributed to by multiple teams\u2022 Experience creating and maintaining CI/CD pipelines with tools like self-hosted Gitlab CI/CD or Github Actions\u2022 Experience building and releasing compiled applications in cloud environments\u2022 Strong skills in working with Docker containers\u2022 Strong skills in scripting languages like Bash, Python, and Go<\u2022 Experience designing and delivering applications using cloud platforms like AWS, GCP, or Azure \u2022 Experience using infrastructure-as-code tools like Terraform for managing cloud infrastructure\u2022 Able to work with engineering teams on projects with deep dives into code, networking, and systems administration in cloud environments Desired Skills:\u2022 Experience working with Apple App Store and Google Play Store, and the deployment processes associated with them\u2022 Experience with configuration of Android applications, familiarity with Gradle files and the Android SDK\u2022 Experience with configuration of iOS applications and iOS SDK\u2022 Experience with configuration of Javascript applications, Typescript, bundlers, testing frameworks \u2022 Experience in application-layer tools for the above platforms like Jetpack Compose, SwiftUI, and/or React Native ThanksNick BCompuSoft Integrated Solutions, Inc.31500 W.13 Mile Road Suite #200Farmington Hills, MI- 48334nick@compusoft-is.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "527_AWS Architect || Columbus, OH || Onsite": "Prashanth B,\nTekgence\nprashanth.b@tekgence.com\nReply to: prashanth.b@tekgence.com\nTitle: Sr. ArchitectLocation: Columbus, OHDuration: Long term Required:\u2022 Should have BANKlNG domain experience - Mandatory\u2022 Lead and execute the migration of on-premises systems to AWS, including cutover planning and post-migration reconciliation.\u2022 Architect and deploy scalable Spark clusters on Kubernetes within AWS, ensuring optimal performance and integration.\u2022 Design and develop efficient ETL pipelines for data extraction, transformation, and loading, maintaining high data quality and integrity.\u2022 Establish and enforce data standards across pipelines, data lakes, and data warehouses to ensure consistency and quality.\u2022 Oversee data ingestion, placement, and transformation processes, ensuring compliance with data regulations and best practices.\u2022 Utilize AWS services such as Glue Catalog, Lake Formation, and EMR to enhance data workflows and storage solutions.\u2022 Worked extensively on Data Migration projects from On prem to Cloud. --Regards,Prashanth B | Tekgence Inc.Talent Acquisition Specialist LinkedIn: https://www.linkedin.com/in/prashanth-goud-burra-535238207/Email: prashanth.b@tekgence.com | Website: www.tekgence.com6655 Deseo Dr \u2022 Suite 104 \u2022 Irving, TX \u2022 75039\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "528_Senior AWS Connect Developer": "Kuldeep Sharma,\nVizonInc\nkuldeep@vizoninc.com\nReply to: kuldeep@vizoninc.com\nJob Description -Senior AWS Connect Developer These candidates can work remote but will have to come on site to Baltimroe MD 2 times in the first week to do badging and receive their laptop.Key Required Skills: The successful candidate will have fluency in AWS Connect design and development principles and work with stakeholders to develop enterprise business solutions that leverages industry processes and best practicesBachelor's Degree in Computer Science, Mathematics, Engineering, or a related field with 7+ years of experienceMust be able to obtain and maintain a Public Trust. Contract requirement. ***************************************************************** Position Description:Design, develop and implement AWS Connect processes to include new or modifications to applications, forms, workflow, policies, actions, access control, interfaces, and any other configurations required to support client processesProvide recommendations for enhancing existing operations and provide recommendations for new updates or modificationsSupport and enhance existing processes based on new requirements or upgradesFacilitate requirements gathering, review and document requirementsDesign and develop solutions within the ServiceNow environment Basic Qualifications:Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field7 years of experience working with large enterprise framework7 years of hands-on experience working with AWS Cloud platform3 years of hands-on experience working with Amazon Connect 3 years of experience using Node.js, Python or Java for development3 years of experience in designing call routing & messagingExcellent interpersonal and communication skills Required SkillsKnowledge or experience with CCaaS with Artificial Intelligence (AI) strategies and solutions.Knowledge and experience with integrating Pega workflows with CCaaS. Knowledge and experience with building reports and dashboards preferably with Tableau.Solid AWS Cloud and AWS Connect Developer experienceExtensive experience and knowledge with IVR, Automatic Call Distribution (ACD), Skill-Based Routing, Live ChatExperience with process flow documentation creation.Strong verbal and written communication skills. Desired SkillsAdept at working independently, but also in a team environmentDemonstrated experience delivering technology solutions in a fast-paced, deadline driven enterprise environmentExcellent understanding of change management, testing requirements, techniques, and tools to ensure high availability of systemsPrior Federal government experienceSelf-starter, highly motivated individual who adapts to a dynamic work environmentStrong attention to detail with an ability to operate effectively across multiple priorities\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "529_Full-Stack Developer with DevOps": "Vikrama,\nValiantiq\nvrao@valiantiq.com\nReply to: vrao@valiantiq.com\nTitle: Full-Stack Developer with DevOpsDuration: 6+ MonthsLocation: Remote 100%Client: WWT/KPMGVisa: ONLY GREEN CARD AND CITIZEN.Primary Responsibilities: 1.C# Development:Writing and maintaining efficient, reusable, and reliable C# code.Developing applications using .NET frameworks.2. UI Development:Creating user interfaces using NodeJS or React.Ensuring the responsiveness and performance of applications.3. Backend Development:Managing and interacting with backend SQL databases.Writing and optimizing SQL queries and stored procedures.4. API Development and Integration:Developing and maintaining APIs.Integrating third-party APIs into applications.5. Containerization:Creating and managing containerized applications using tools like Docker.6. Kubernetes:Deploying, scaling, and managing containerized applications using Kubernetes.Nice to Have:Python:Utilizing Python for scripting, automation, or additional backend services.Typical Work Scenarios:Collaborating with cross-functional teams to define, design, and ship new features.Ensuring the best possible performance, quality, and responsiveness of applications.Identifying and correcting bottlenecks and fixing bugs.Maintaining code quality, organization, and automation.Implementing DevOps practices for continuous integration and deployment. Thanks & Regards,Vikrama RaoRecruitment Executive- ValiantIQ Inc.\"Searching Best Minds \u25a0 Searching Best Minds\"Email: vrao@valiantiq.com P. 302-357-9010 F. (302) 482-3672Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line for automatic removal. And mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "530_Urgent Need -  AWS Cloud Engineer": "Jon,\nSmarttechlink\njon@smarttechlink.com\nReply to: jon@smarttechlink.com\nHi,We do have a priority requirement with one of our clients. Kindly review and let me know if you have any questions.AWS Cloud EngineerAtlanta, GA ( ONLY LOCALS )Please see below the JD.* AWS Cloud engineer, hands-on exp on EC2, S3, ROS, EBS, VPC and EKS* Good exp in installing and managing the Kafalta permit* Hands-on exp Kube helm charts, Rabbiting helm chart* Troubleshoot the application deploy Kubernetes* Good knowledge of the Cassandra database installations and managing the databases on AWS cloud and on perm* Hands-on exp on the GitLab CI/CD pipelinesThanks & Regards JonSr Technical Recruiterjon@smarttechlink.com | www.smarttechlink.com |\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "531_SalesforceCloud Architect (133158) - Need locals in Michigan - In Person Interview.": "Riyaz,\nPredica Inc.\nriyaz@predicaz.com\nReply to: riyaz@predicaz.com\nJob Title : Salesforce Cloud Architect (133158) - Need locals in Michigan - In Person Interview.Location : Lansing, MIClient : State of MichiganDuration : Long Term. Salesforce skillsets : \u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform\u2022 Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. Regards,Riyaz ShaikMail ID: riyaz@predicaz.comPredica Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "532_Only locals to Texas - DevOps Engineer with AWS - 13+years experience is preferred": "madhu,\nnexwave\nmadhu@nexwaveinc.com\nReply to: madhu@nexwaveinc.com\nRole : DevOps EngineerLocation : Plano, TX ( Need Locals / Onsite from Day 1 )Exp Req : 13+yearsDuration : 12 MonthsMust Have: AWS, Github actions, CI/CD, automation scripts, React Need Only H1B Profiles Need Active LinkedIn & Passport Number Need Locals / Should be flexible to go for In Person interview on client request Primary Skills:\u2022 Infrastructure Management:o Experience in provisioning and managing infrastructure in AWSo Strong understanding of AWS services and best practices CI/CD:o Proficiency in using GitHub Actions for CI/CD pipelineso Ability to design and orchestrate CI/CD workflows using GitHub Actions Secondary Skills: Scripting and Automation:o Experience with automation scripts for infrastructure managemento Knowledge of infrastructure as code tools (e.g., Terraform, CloudFormation) Monitoring and Logging:o Familiarity with monitoring tools and logging best practiceso Ability to set up alerts and dashboards for infrastructure monitoring Roles and Responsibilities:\u2022 Provision and manage AWS infrastructure to support application deployment\u2022 Design and implement CI/CD pipelines using GitHub Actions\u2022 Automate infrastructure provisioning and management tasks\u2022 Monitor and troubleshoot infrastructure issues\u2022 Collaborate with development teams to ensure seamless integration and deployment\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "533_Urgent hiring for Cloud Security Engineers": "padmavathi,\nYochana IT solutions\npadmavathi@yochana.com\nReply to: padmavathi@yochana.com\nHello,I hope you are doing great.This is Padmavathi from Yochana IT Solutions, we have an urgent requirement with one of our clients, please go through the requirement below and let me know your interest. You can forward this opportunity to your friends or colleagues; so that we can help someone who may be desperately looking for opportunities. I sincerely appreciate your timeRole: - Cloud Security Engineers Location: - Frisco, TX \u2013 Day 1 ONSITEFROM PANEL \u2013 FOCUS ON THESE AREAS / SKILLS : MUST HAVE - Cloud security certifications. Hands-on practical experience on cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring. Proficiency in scripting and automation tools (e.g., Python, Terraform).Cloud Security Engineer Key Responsibilities\u2022 Develop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.\u2022 Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.\u2022 Implement and enforce security controls to protect sensitive data and systems.\u2022 Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.\u2022 Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.\u2022 Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. Qualifications\u2022 Experience with onboarding and provisioning in cloud environments.\u2022 Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.\u2022 In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.\u2022 Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.\u2022 Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.\u2022 Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.\u2022 Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.\u2022 Proficiency in scripting and automation tools (e.g., Python, Terraform).\u2022 understands onboarding and will be doing onboarding provisioning work on AWS and Azure\u2022 Strong communication and documentation skills for collaborating with cross-functional teams.Thanks & Regards,Padmavathi Vindeepu,Resource Specialist, Farmington Hills, MI,Email Id: padmavathi@yochana.comhttps://www.linkedin.com/in/padmavathi-vindeepu-25b540261/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "534_Job Description ||10+ Years Only || Sr. AWS Cloud Engineer With Lambda ||Washington DC": "Prem krishna,\nTHEMESOFT\nprem@themesoft.com\nReply to: prem@themesoft.com\nHi, Good Day!!!I\u2019m Prem with Theme Soft. I came across your profile wanted to reach out to discuss this exciting opportunity I have with my client for Sr. AWS Connect developer role. Your profile looks to be a great match for the requirement and would appreciate your thoughts once review the below requirement. Job Title: Sr. AWS Cloud Engineer With LambdaLocation: Washington DCDuration: Long term Job description:AWS Cloud with hands-on experience in Contact Centre technologiesShould have good hands-on experience developing Lambda Functions in Node JS.API gateway experience is requiredShould have good knowledge on Agile process.Should be a passionate learner and are motivated to take up new challenges Thanks & RegardsPrem Krishna _______________________ Technical RecruiterEmail:prem@themesoft.comLinked in ID : - linkedin.com/in/krishna-krish-859578216Web: themesoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "535_AWS Connect Developer || Woodlawn, Maryland || Remote ((Onsite only first weeks then remote)": "Roopesh Sharma,\nVIZON INC\nroopesh@vizoninc.com\nReply to: roopesh@vizoninc.com\nHello, Hope you are doing great. Please response me on my official email - roopesh@vizoninc.com Job Title: AWS Connect DeveloperLocation: Woodlawn, Maryland (Onsite only first weeks then remote) Visa: IndependentThese candidates can work remote but will have to come on site to Baltimore MD 2 times in the first week to do badging and receive their laptop.Key Required Skills: The successful candidate will have fluency in AWS Connect design and development principles and work with stakeholders to develop enterprise business solutions that leverages industry processes and best practicesBachelor's Degree in Computer Science, Mathematics, Engineering, or a related field with 7+ years of experienceMust be able to obtain and maintain a Public Trust. Contract requirement. ***************************************************************** Position Description:Design, develop and implement AWS Connect processes to include new or modifications to applications, forms, workflow, policies, actions, access control, interfaces, and any other configurations required to support client processesProvide recommendations for enhancing existing operations and provide recommendations for new updates or modificationsSupport and enhance existing processes based on new requirements or upgradesFacilitate requirements gathering, review and document requirementsDesign and develop solutions within the ServiceNow environment Basic Qualifications:Bachelor's Degree in Computer Science, Mathematics, Engineering, or a related field7 years of experience working with large enterprise framework7 years of hands-on experience working with AWS Cloud platform3 years of hands-on experience working with Amazon Connect 3 years of experience using Node.js, Python or Java for development3 years of experience in designing call routing & messagingExcellent interpersonal and communication skills Required SkillsKnowledge or experience with CCaaS with Artificial Intelligence (AI) strategies and solutions.Knowledge and experience with integrating Pega workflows with CCaaS. Knowledge and experience with building reports and dashboards preferably with Tableau.Solid AWS Cloud and AWS Connect Developer experienceExtensive experience and knowledge with IVR, Automatic Call Distribution (ACD), Skill-Based Routing, Live ChatExperience with process flow documentation creation.Strong verbal and written communication skills. Desired SkillsAdept at working independently, but also in a team environmentDemonstrated experience delivering technology solutions in a fast-paced, deadline driven enterprise environmentExcellent understanding of change management, testing requirements, techniques, and tools to ensure high availability of systemsPrior Federal government experienceSelf-starter, highly motivated individual who adapts to a dynamic work environmentStrong attention to detail with an ability to operate effectively across multiple priorities\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "536_Remote Hiring - Enterprise architect AWS with Kafka experience or AWS Kafka Architect": "Vijay,\nWarrior Tech Solution LLC\nvijay@warriortechsolutions.com\nReply to: vijay@warriortechsolutions.com\nGreetings from Warrior Tech Solutions LLC! Job Title: Enterprise architect AWS with Kafka experience/ AWS Kafka ArchitectClient: Dell/Wells Fargo Location: Dallas, TX Hybrid(3 days a week) \u2013 Remote is also fineExperience:15+ years Rate: As per the Industry standards Duration:2+ years Visa: No OPT/CPT This role involves collaborating with business leaders, application managers, and other architects to ensure data solutions align with the company's architecture standards and policies. Key Responsibilities: Design and implement Kafka architecture and data pipelines. Collaborate with business leaders, project teams, and application managers to create and integrate Kafka solutions. Develop architecture strategies and artifacts for technology initiatives. Ensure solutions adhere to corporate policies and industry best practices. Lead architecture efforts and provide guidance to project teams. Stay updated with emerging technologies to meet customer needs. Qualifications: Experience in systems, applications, or architecture planning. Experience with industry-standard database modeling languages. Experience with databases such as Oracle, DB2, SQL Server, or Teradata. Proficiency in data management, modeling, and movement. Experience with risk applications and credit risk technology systems. Desired Skills: Strong communication and interpersonal skills. Ability to influence and collaborate at all organizational levels. Knowledge of data governance, BI tools, and analytical methods. Experience with Big Data tools such as Hadoop, Spark, and Kafka. Agile methodology experience. Additional Requirements: Willingness to work additional hours and travel as needed. Successful completion of a criminal background check. Thanks & Regards, Vijay | Lead Talent Acquisition Group +1 813 322 4583 | vijay@warriortechsolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "537_Hiring for AWS Public Cloud Developer - (Day 1 Onsite) Hartford, CT -Cognizant - The Hartford": "Gangadar Reddy,\nCentraprise\nedula.gangadar@centraprise.com\nReply to: edula.gangadar@centraprise.com\nRole: AWS Public Cloud Developer Location: Hartford, CT -Hire type: ContractJob description:Qualification and experience required: -Total 10 years of experience in Cloud infrastructure.5+ Years of Experience in AWS & 3 Years in Terraform. Minimum 2 Years in EKS & Ansible.Certification (Mandate):Terraform Associate CertificationSkills: Core AWS - EC2, S3, Lambda, AWS Backup, DR, CloudFormation- 10 years \u2013 Cand. should have extensive experienceTerraform- Min. 5 Years and should be expertEKS- 2-3 Years (Hands on)Ansible - 2 years \u2013 (Hands- on)Documentation / SOP's- 2 years \u2013 (Hands- on) RegardsSiva Gangadar reddyUS IT RecruiterCentraprise Corp33 Wood Avenue South, Suite 600, Iselin NJ 08830 Desk: 469-639-0369Email: edula.gangadar@centraprise.com We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "538_Azure DevOps": "LEELA,\nmeghaz.com\nleela.p@meghaz.com\nReply to: leela.p@meghaz.com\nAzure DevOpsLocation : New York , NYCDuration : 6 months contractGc or citizenMicrosoft Azure, Azure DevOps, Networking, Load balancers, DNS and Azure servicesExperience building CICD pipelines (Build and deployment of Azure Services and applications based on Nodejs and pythonPython and node\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "539_Immediate requirement | Salesforce Commerce Cloud (SFCC) Consultant | Indianapolis, IN 46204 | Onsite": "RK,\nSoftcom Systems Inc\nrk@softcomsystems.com\nReply to: rk@softcomsystems.com\nRole: Salesforce Commerce Cloud (SFCC) Consultant Location: Indianapolis, IN 46204 | OnsiteType: ContractMinimum years of experience: 8-10 yearsJob Details (Must Have Skills):\u2022 Experience in developing in the SFCC platform. Working knowledge with CI/CD, tracking tools, Jenkins, teamcity etc. \u2022 Experience with JSON, XML, HTML and CSS. \u2022 Good experience in relevant technologies. \u2022 Recent experience in upgrading SFCC from site genesis, SFRA to composable storefront. \u2022 Experience in creating custom API calls using hooks.\u2022 Experience with java script and libraries such as node, Express, Jquery, AngularJS, React, and VUE. \u2022 Experience with TDD, experience with consuming, RESTful Webservices.Detailed Job Description:\u2022 Good experience in relevant technologies. Recent Experience in upgrading SFCC from site genesis, SFRA to composable storefront. Experience in creating custom API calls using hooks.\u2022 Experience in developing in the SFCC platform. Working knowledge with CI CD, tracking tools, Jenkins, teamcity etc. Experience with JSON, XML, HTML and CSS.\u2022 Experience with java script and libraries such as node, Express, Jquery, AngularJS, react, and VUE.\u2022 Experience with TDD, experience with consuming, RESTful Webservices, knowledge with version control system such as Git or Bit bucket. Good experience on agile methodologies, knowledge of developer management tools such as Jira and confluence. Experience with one or more backend development languages such as Node Js, express, python, Golang, Java, and Ruby, PHP or other. Experience with client server technology as well as server side Java script.\u2022 Experience in creating and managing API integration and data feeds analysis and debugging skills to quickly identify the root cause per resolution. Strong problem solving, decision making and analytical abilities, organized detail oriented, level headed and flexible at all times. Desire to learn and to adapt to changing technology. Ability to manage multiple priorities and meet deadlines in a fast paced environment. Strong written and verbal communication skills.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "540_AWS Cloud Developer or Engineer- Hartford, CT  Day 1 Onsite": "Basavaraj,\nCentraprise\nbasavaraj@centraprise.com\nReply to: basavaraj@centraprise.com\nHi Team, Greetings from Centraprise,We have below urgent requirements with one of our prime clients, please go through the below job details and let me know your availability along with updated resume and contact details.Role: - AWS Cloud Developer or EngineerLocation \u2013 Hartford, CT \u2013 Day 1 OnsiteNOTE: LinkedIn & Passport Number Is Must for all visaTotal 10 years of experience in Cloud infrastructure.5+ Years of Experience in AWS & 3 Years in Terraform. Minimum 2 Years in EKS & Ansible.Certification (Mandate):Terraform Associate CertificationSkills:Core AWS - EC2, S3, Lambda, AWS Backup, Disaster Recovery (DR), CloudFormation- 10 years \u2013 Cand. should have extensive experienceTerraform- Min. 5 Years and should be expertEKS- 2-3 Years (Hands on)Ansible - 2 years \u2013 (Hands- on)Documentation / SOP's- 2 years \u2013 (Hands- onRoles & Responsibilities:Hands-On Experience & Understanding AWS Core Services, CloudFormation & Terraform.Ability to understand the scripts in CloudFormation & Re-write in Terraform.Understand the application infrastructure architecture.Provide day to-day infrastructure operations support for the applications in scope including os Support, Backup,Understand the clients process in performing the AWS Operations and applications in scope.Be Proactive to perform required automation using the client approved technology and tools.Considered as an SME Role and coordinate with onsite client stakeholders and offshore associates.To be considered as Mentor for the offshore associates. Should conduct onsite/offshore hand-over calls.Experience in working with onshore/offshore team.Should be available to perform On Call Support for any P1/P2 Critical/High IncidentsThanks & RegardsBasavaraj | Talent Acquisition AssociateCentraprise Corp33 Wood Avenue South, Suite 600, Iselin NJ 08830Desk : 848-209-8309Email: basavaraj@centraprise.comConnect me on LinkedIn: linkedin.com/in/basavaraj-n-methre-720b55223\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "541_Looking for Azure DevOps Engineer need 10+yrs need locals to NY and Princeton NJ": "Austin varma,\nEminencets\naustin@eminencets.com\nReply to: austin@eminencets.com\nRole: Azure DevOps EngineerDuration: Long TermLocation: Princeton NJ or NYC office (1166 Avenue ofAmericas, New York)Hybrid: 3 days WFORoles and Responsibilities:(5-6 Years Azure experience,Expertise in deploying .NET based apps, Docker/Kubernetes/GHActions/Azure DevOps) Thanks, and regards.Austin VarmaRecruiter Eminence Technology Solutions LLCEmail: Austin@eminencets.comWebsite: www.eminencets.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "542_C++ developer with some Python, AWS........Remote": "Vijay,\nTechrakers\nvijaym@techrakers.com\nReply to: vijaym@techrakers.com\nC++ developer with some Python/AWSRemoteLong Term C++, Linux, SQL, Python and some AWS \u2022 C++ for core engine, do a lot of coding python, shell scripting, SQL for workflow managementComputational expansive work Need someone who is very proficient in Linux - their program runs in LinuxWould be looking for someone who has good knowledge and hands on experience in C++Strong programming in python, R, SQL would help as well - a good programmer could pick things up easilyDoesn\u2019t expect to have much hardcore programming work for this project, however, there will be a lot of enhancements to make - need to understand existing code and make enhancementsNeed someone who is comfortable working with large volumes of data - producing terabytes of data each monthAWS experience would be highly desired, but they have a team for AWS infrastructure so they can support if neededCore model logic / business analytics development work is what they are hiring forVery strong problem solving mentality - aren't doing rocket science, don\u2019t need folks who have a PHD, but this person should be looking for solutions constantly and be able to communicate fluently with others in the team, express themselves to others and understand what they are saying, asking questions, participating, anticipating what other people will knowWants to hire someone who wants to stay on your team long term\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "543_AWS Cloud engineer with Terraform certificate || Hartford CT": "Akash,\nsmart it frame\nakash@smartitframe.com\nReply to: akash@smartitframe.com\nHi,Greetings from Smart IT Frame, Hope you are doing well!!!Smart IT Frame specializes in enabling you with your most critical line of resources. Whether it\u2019s for permanent staffing, contract staffing, contract-to-hire or executive search, we understand the importance of delivering the most suitable talent; on time and within budget. With our Core focus in emerging technologies, we have provided global technology workforce solutions in North America, Canada & India. We take pride in delivering specialized talent, superior performance, and seamless execution to meet the challenging business needs of customers worldwide. Role: AWS Public CloudLocation: Hartford CTExp : 12 Years Certifications: -\u2022 Terraform Associate Certification Skill Proficiency:\u2022 Core AWS \u2013 EC2, S3, Lambda, AWS Backup, DR, CloudFormation- 10 years- Expert level.\u2022 Terraform- 4 years- Expert level.\u2022 EKS- 2 years \u2013 Hands- on\u2022 Ansible- 2 years \u2013 Hands- on\u2022 Documentation / SOP\u2019s- 2 years \u2013 Hands- on Roles & Responsibilities:\u2022 Hands-On Experience & Understanding AWS Core Services, CloudFormation & Terraform.\u2022 Ability to understand the scripts in CloudFormation & Re-write in Terraform.\u2022 Understand the application infrastructure architecture.\u2022 Provide day to-day infrastructure operations support for the applications in scope including OS Support, Backup, DR & Security.\u2022 Understand the clients process in performing the AWS Operations and applications in scope.\u2022 Be Proactive to perform required automation using the client approved technology and tools.\u2022 Considered as an SME Role and coordinate with onsite client stakeholders and offshore associates.\u2022 To be considered as Mentor for the offshore associates. Should conduct onsite/offshore hand-over calls.\u2022 Experience in working with onshore/offshore team.\u2022 Should be available to perform On Call Support for any P1/P2 Critical/High Incidents.Qualification and experience required: -\u2022 A minimum of total 10 years of experience in Cloud infrastructure.\u2022 5+ Years of Experience in AWS & 3 Years in Terraform. 2 Years in EKS & Ansible.\u2022 AWS Associate \u2013 Developer ---Warm Regards,AkashSmart IT Frame LLCakash@smartitframe.comwww.smartitframe.comhttps://www.linkedin.com/in/akash-s-332905212/-----WBENC------\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "544_AWS Cloud Network Engineer - Palo Alto SME  6+ months  Atlanta, GA (Need Only Local Candidates)  Hybrid  Interview: 1st being video and 2nd being in person onsite": "Dev Soni,\nTek Inspirations LLC\ndev.soni@tekinspirations.com\nReply to: dev.soni@tekinspirations.com\nAWS Cloud Network Engineer - Palo Alto SME6+ monthsAtlanta, GA (Need Only Local Candidates) - HybridInterview: 1st being video and 2nd being in person onsite Must have :On prem and AWS migration to AWSAzure to AWS migrationDeployment of Palo Alto in AWSCheckpoint to Palo Alto migrationWe are looking for a Engineer resource who is SME with Palo Alto Platform. Critical experience we are looking for as below.1. Having experience in Cloud PA deployment in AWS and Azure platforms with latest deployment architecture - GWLBe and auto scale design2. Having experience with Checkpoint to Palo Alto migration.3. Will be working with Carrier Firewall engineers on standards / support and optimization of current security footprint.4. Will be responsible for all documentation and knowledge transfer.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "545_Remote - Cloud-Network Engineer - Marriott": "sunil kumar,\npransu tech solutions\nsunil@pransutechsolutions.com\nReply to: sunil@pransutechsolutions.com\nRemote - Cloud/Network Engineer - Marriott Kforce Routing Juniper SRX firewallsExpress Routes, VLAN Hubs in Azure REQUIREMENTS Tech \u2013 Must haveNetwork HeavyFirewalls: Strong in this area, JuniperF5 load balancerNetwork RoutingExpress Routing Cloud (2nd Priority, still required)Azure/AWS Background/Ideal Career HistoryVolume of Work: HighEnterprise: Complexity, Scale (Very Large), Publicly Traded, Multi-tiered OrgsSize Org: Small to Mid-CompaniesIndustry: N/A\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "546_Job Title: Sr. DevOps Engineer (Integration and Deployment)": "saloni chaurasia,\ntekinspirations\nsaloni.chaurasia@tekinspirations.com\nReply to: saloni.chaurasia@tekinspirations.com\nHello,I Hope you are doing great.Please find below position if you have any matching candidate as per requirment .Please send me updated resume with candidate information.Job Title: Sr. DevOps Engineer (Integration and Deployment)Location: VA, ManassasHYBRIDDuration: 1 YEAR PLUSNeed Local OnlyPlease Fill Skill Matrix DescriptionWe are seeking a versatile and skilled DevOps Engineer to join our dynamic team. The ideal candidate will have a strong background in continuous integration and continuous deployment (CI/CD) using Jenkins, infrastructure as code (IaC), Kubernetes, cloud, and container technologies. Additionally, programming skills in Python or Java, and deployment automation experience are crucial for this role. A good understanding of Red Hat Enterprise Linux (RHEL) and vCenter API experience is also required. This role involves product testing to ensure our solutions meet the highest quality standards.Key Responsibilities:======================Design, implement, and manage CI/CD pipelines using Jenkins to streamline development, testing, and deployment processes.Develop and maintain infrastructure as code using tools like Terraform, Ansible, or similar technologies.Deploy, manage, and scale applications using Kubernetes and other container orchestration platforms.Utilize cloud platforms (AWS, Azure, GCP) for infrastructure and application deployment.Automate deployment processes to ensure efficient and reliable releases.Write and maintain scripts in Python or Java for various automation tasks.Collaborate with development, QA, and operations teams to ensure seamless integration and delivery of solutions.Perform product testing to validate functionality, performance, and reliability of solutions.Troubleshoot and resolve issues related to application and infrastructure performance.Monitor system performance, identify bottlenecks, and propose optimizations.Ensure security and compliance standards are maintained throughout the development and deployment processes.Stay up-to-date with the latest industry trends and technologies to continually improve our DevOps practices.==================Qualifications:Proven experience as a DevOps Engineer or similar role.Strong background in CI/CD using Jenkins.Hands-on experience with infrastructure as code (IaC) tools such as Terraform or Ansible.Proficiency in container technologies like Docker and orchestration platforms like Kubernetes.Solid understanding of cloud platforms (AWS, Azure, GCP).Programming skills in Python or Java.Experience with deployment automation.Good understanding of Red Hat Enterprise Linux (RHEL).Familiarity with vCenter API.Experience in product testing and quality assurance practices.Strong problem-solving skills and attention to detail.Excellent communication and collaboration abilities.Ability to work independently and as part of a team in a fast-paced environment.SPECIFIC SKILLSAutomation and TestingProfessional (4-5) ( 5,00 years )Experience levels and years of experience Container Technologies on LinuxProfessional (4-5) ( 5,00 years )Experience levels and years of experience DEV-OPSSenior (6-9) ( 6,00 years )Experience levels and years of experience Infra As CodeProfessional (4-5) ( 5,00 years )Experience levels and years of experience Linux/RHEL OSSenior (6-9) ( 6,00 years )Experience levels and years of experience Python /JavaSenior (6-9) ( 6,00 years )Experience levels and years of experienceRegards,Saloni Chaurasia{ Sr.Technical Recruiter }TEK Inspirations LLC Pvt. Ltd. |13573 Tabasco Cat Trail, Frisco, TX 75035, United StatesE-Mail: saloni.chaurasia@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "547_Please share local candidates-IaC (AWS) Architect  Berkeley heights NJ - Onsite": "pallavi,\nyochana\npallavi@yochana.com\nReply to: pallavi@yochana.com\nHello,I\u2019m a Technical Recruiter with Yochana Solutions Inc., a national IT staffing solutions provider with 14+ years of experience delivering value to leading companies across the U.S.A. and Canada,I'm currently staffing for IaC (AWS) Architect \u2013 Berkeley heights, NJ - Onsite, Below you will find the job description, if you are qualified and interested, please send me your Updated Word Document Resume.I'm very sorry if this position is not an ideal fit, We'll keep you in mind for other suitable positions and referrals would be appreciated.Thank you so much for your attention and participation.Job Role: IaC (AWS) ArchitectJob Location: Berkeley heights, NJ - OnsiteProject Duration: 6-12 MonthsJob Description: HCL AWS Cloud Capability Delivery Unit is looking for experienced customer-focused AWS Cloud Consultant/Engineer with technical depth and architecture skills. In this role, he/she will be responsible for managing and developing the AWS cloud platform. This includes working with the development team to deploy applications, managing and scaling the AWS infrastructure, and ensuring high availability of the platform. The engineer will also work closely with the operations team to troubleshoot and resolve issues with the platform.Specifically, you will be part of customer engaging team of technical PMs and engineers responsible for implementing new custom deployment pipelines and execute POCs and pilot migration projects with modernized approach. Ideally candidates with exposure in Global markets across US, UK, Continental Europe, APAC including Indian market will be preferred.o Position AWS Cloud Engineer/Consultanto Experience - A minimum of 9 12 years hands-on experience in infrastructure delivery, Designing and implementing solutions with proficiency in automation and devOps tools.Roles & Responsibilities: Automate solutions for repeatable problems. Work with clients to transform manual processes and deployments into DevOps automated pipelines using AWS native services. Develop test plan and test cases to demonstrate application/database readiness post migration. Work closely with application teams to ensure business functionality and SLAs are met. Develop innovative solutions to complex business and technology problems. Keeping up to date with market trends and competitive insights and maintain technical skills and knowledge. Advocate new features and solutions to bring operational efficiency and cost reduction. Key focus on Infrastructure as code (IaC) and automation Work with development team to deploy applications to AWS Manage and scale AWS infrastructure Ensure high availability of AWS platform Troubleshoot and resolve issues with AWS platform and applications Identify, troubleshoot and implement solutions to performance, scaling and availability issues across the platform Work with security team to ensure compliance standards are met Assist in provisioning AWS resources - Assist DevMandatory Skills: Bachelor s degree in Information Science / Information Technology, Computer Science, Engineering, Mathematics, Physics, MCA, or a related field. Subject matter level expertise on AWS DevOps, AWS CloudFormation. AWS Compute (EC2) & Storage, Backup & DR, Monitoring Experience in any one programming languages like Python, Node.js etc. Infrastructure automation through DevOps scripting (E.g., shell, PowerShell, Ansible, Terraform, CloudFormation etc) Hands-on experience of automating the build and deployment of AWS resources, using Terraform, at both the platform level (e.g. Landing Zone) and across the compute, storage, network data and security services. CloudFormation deep expertise (cfn hup, cfn init, custom resources, stack sets etc.) From a development background, the individual will have experience of the Software Delivery Life Cycle (SDLC), preferably with hands-on experience working with Python, and be familiar with an agile delivery framework (e.g. Scrum) Strong hands-on experience on CI/CD tools like Jenkins, Team city etc. Configuration management using Ansible and/or Chef/Puppet. Strong verbal and written communication skills, with the ability to work effectively across internal and external organizations. Implementing experience with primary AWS services (EC2, ELB, ECS, RDS, Lambda, API Gateway, Route53 & S3) Experience in Kubernetes Regards, Pallavi BoolaRESOURCE SPECIALIST Contact- 2482373189Yochana IT Solutions Inc. 23000 Commerce Dr, Farmington hills, MI-48335 pallavi@yochana.com || www.yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "548_Lead Azure Data Engineer, Onsite Day1, Weehawken NJ": "Sunita Sharma,\nTechSource inc\nsunita@tsourceinc.net\nReply to: sunita@tsourceinc.net\nAzure Data AnalystLocation:-Weehawken NJOnsite Role from Day1, Experience required 12yrs to 15yrsKRA'sManage and analyze large datasets using Azure data services to inform business decisions. Build and maintain data pipeline architecture within Azure. Assemble complex data sets that meet functional business requirements. Identify, design, and implement internal process improvements.responsible for collecting, analyzing, and visualizing data from various sources using Azure services. They work with large amounts of data to identify trends, patterns, and insights that can help businesses make informed decisions.dissect complex data sets, identify patterns, and derive insights that can drive business decisions.analyze and interpret data within the Microsoft Azure cloud ecosystem\u2022Responsible for designing, building, and maintaining scalable data pipelines and infrastructure\u2022Hands on with Microsoft Azure Databricks, Microsoft Azure Data Factory, Microsoft Azure SQL, Microsoft Azure Cloud Services, Azure Data Lake ADLS Azure Data Lake Storage\u2022Day to day activities will include collaborating with cross functional teams, analyzing data requirements, and implementing data solutions using your expertise in Data Engineering\u2022Design and develop scalable data pipelines and infrastructure using Data Engineering expertise\u2022Ensure data quality and integrity by implementing data governance and best practices\u2022Optimize data processing and storage for performance and cost efficiency\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "549_Immediate Interview - AWS Data engineer with Data Modeler Expertise- Bethlehem, PA (Hybrid)": "Giridharan TM,\nLorven Technologies\ngiridharan@lorventech.com\nReply to: giridharan@lorventech.com\nHi Our client is a currently looking for Data engineer with Data Modeler Expertise with a Long Term project in Bethlehem, PA (Hybrid) below is the detailed requirement. Position: Data engineer with Data Modeler ExpertiseLocation: Bethlehem, PA (Hybrid)Duration: Long Term Contract Job Description:5+ Years of experience as a Data Engineer Strong technical expertise in Python and SQL Experience with Big Data Tools such as Hadoop and Apache Spark (Pyspark) Solid experience with AWS services such as Cloud Formation, S3, Athena, Glue, Glue Data Brew, EMR/Spark, RDS, Redshift, Data Sync, DMS, DynamoDB, Lambda, Step Functions, IAM, KMS, SM, Event Bridge, EC2, SQS, SNS, Lake Formation, Cloud Watch, Cloud Trail Responsible for building, test, QA & UAT environments using Cloud Formation. Build & implement CI/CD pipelines for the EDP Platform using Cloud Formation and Jenkins Key Skills: Implement high-velocity streaming solutions and orchestration using Amazon Kinesis, AWS Managed Airflow, and AWS Managed Kafka (preferred) Solid experience building solutions on AWS data lake/data warehouse Analyze, design, Development , and implement data ingestion pipeline in AWS Knowledge of implementing ETL/ELT for data solutions end to end ingest data from Rest APIs to AWS data lake (S3) and relational databases such as Amazon RDS, Aurora, and Redshift. Perform the Peer Code Review and, perform code quality analysis, and associated tools end-to-end for Prudential's platforms Create detailed, comprehensive, and well-structured test cases that follow best practices and techniques Understanding requirements, and data solutions (ingest, storage, integration, processing, access) on AWS Knowledge of implementing RBAC strategy/solutions using AWS IAM and Redshift RBAC model Knowledge of analyzing data using SQL Stored procedures Build automated data pipelines to ingest data from relational database systems, file system , and NAS shares to AWS relational databases such as Amazon RDS, Aurora, and Redshift Build Automated data pipelines to develop test plans, execute manual and automated test cases, help to identify the root causes, and articulate defects clearly Recreate production issues to help determine the issue and verify any fixes Conducting End to End verification and validation for the entire application Creating Jenkins CI pipelines to integrate Sonar/Security scans and test automation scripts Using Git/bitbucket for efficient remote team working, storing framework, and developing test scripts Part of DevOps QA and AWS team focusing on building CI/CD pipelinePart of the release/build team and mainly worked on release management, CI/CD pipeline Deploy multiple instances by using cloud formation templatesRegards Giridharan,Lorven Technologies, Inc. 101 Morgan Lane | Suite 209 | Plainsboro | NJ 08536Email: giridharan@lorventech.com | Web: www.lorventech.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "550_Immediate need of Lead AWS Data Engineer": "Kiran Terve,\nTechstar Group\nkirant@techstargroup.com\nReply to: kirant@techstargroup.com\nLead Data EngineerSenior \u2013 Bloomington, IL Note: (Work will be remote but may be requested to travel to onsite location.)Technical Skills:AWS cloud- s3, dynamodb, redshift, Iam, sqs, sns, glue, step functions, lambda, cloudwatch, API gateway, transfer family, Athena, Lake Formation.Experience with terraform, Scalr and GitOps.Experience with etl scripts(pyspark).Python programming with pytest, terratest and go language.Responsible for building/maintaining data pipelines in gitlab.Developing, maintaining and optimizing data work flows.Debugging skills for identifying various data patterns for business needs/requirements.Responsible for data migration from on-premises to data lake buckets.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "551_Job Description || Sr. AWS Connect developer ||Washington DC": "Prem krishna,\nTHEMESOFT\nprem@themesoft.com\nReply to: prem@themesoft.com\nHi, Good Day!!!I\u2019m Prem with Theme Soft. I came across your profile wanted to reach out to discuss this exciting opportunity I have with my client for Sr. AWS Connect developer role. Your profile looks to be a great match for the requirement and would appreciate your thoughts once review the below requirement. Job Title: Sr. AWS Connect developerLocation: Washington DCDuration: Long term Job description:o AWS Cloud tech lead with hands-on experience in Contact Centre technologieso Should have good hands-on experience developing Lex bots along with Lambda Functions in Node JS.o Hands on experience on developing Amazon Connect flows, queues, routing profiles etc.o Hands on experience on developing the lambda functions in Node JS.o Should have good experience on how end-to-end IVR works and integration of different AWS services.o Should have good knowledge on Agile process.o Should be a passionate learner and are motivated to take up new challenges Thanks & RegardsPrem Krishna _______________________ Technical RecruiterEmail:prem@themesoft.comLinked in ID : - linkedin.com/in/krishna-krish-859578216Web: themesoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "552_Salesforce Cloud Architect": "Harry,\nGSK Solutions\nharry@gsksolutions.com\nReply to: harry@gsksolutions.com\nJob Title: Salesforce Cloud Architect (Hybrid Onsite - Locals Only)Location: Lansing, MIDuration: 1 year with extension possibleClient : State of MichiganID : #133158Pay: $80/hrInterview Process: 1st round will be held via MS Teams. 2nd round interviews will be held in person. Candidates must be available for an in-person interview.Hybrid: Local candidates only. Candidates must be located within 2 hours of Lansing, MI. NO REMOTE ONLY OPTION. Will need to be onsite from day 1, two days a week. Resource will be working a hybrid schedule.Top Skills & Years of Experience:- 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning- (Full Job Description Attached)Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:\u2022 Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.\u2022 Work with the customer and end users to define technical requirements for new projects and any future enhancements.\u2022 Lead Technical workshops and design sessions with the Customer.\u2022 Propose/Articulate design/architecture options with pros/cons\u2022 Ensure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.\u2022 Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.\u2022 Review technical architecture deliverables throughout development to ensure quality and requirements traceability.\u2022 Has overall technical responsibility for the technical aspects of the project environments\u2022 Compile internal development guidelines/standards for the development team and ensure the team is following those standards.\u2022 Provide oversight on QA efforts to ensure adherence to quality assurance standards.\u2022 Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.\u2022 Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.\u2022 Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinement\u2022 Excellent written and verbal communication skills.Salesforce skillsets\u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform\u2022 Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "553_AWS Public Cloud || Hartford CT": "Akash,\nsmart it frame\nakash@smartitframe.com\nReply to: akash@smartitframe.com\nHi,Greetings from Smart IT Frame, Hope you are doing well!!!Smart IT Frame specializes in enabling you with your most critical line of resources. Whether it\u2019s for permanent staffing, contract staffing, contract-to-hire or executive search, we understand the importance of delivering the most suitable talent; on time and within budget. With our Core focus in emerging technologies, we have provided global technology workforce solutions in North America, Canada & India. We take pride in delivering specialized talent, superior performance, and seamless execution to meet the challenging business needs of customers worldwide. Role: AWS Public CloudLocation: Hartford CTExp : 12 YearsRoles & Responsibilities:\u2022 Hands-On Experience & Understanding AWS Core Services, CloudFormation & Terraform.\u2022 Ability to understand the scripts in CloudFormation & Re-write in Terraform.\u2022 Understand the application infrastructure architecture.\u2022 Provide day to-day infrastructure operations support for the applications in scope including OS Support, Backup, DR & Security.\u2022 Understand the clients process in performing the AWS Operations and applications in scope.\u2022 Be Proactive to perform required automation using the client approved technology and tools.\u2022 Considered as an SME Role and coordinate with onsite client stakeholders and offshore associates.\u2022 To be considered as Mentor for the offshore associates. Should conduct onsite/offshore hand-over calls.\u2022 Experience in working with onshore/offshore team.\u2022 Should be available to perform On Call Support for any P1/P2 Critical/High Incidents.Qualification and experience required: -\u2022 A minimum of total 10 years of experience in Cloud infrastructure.\u2022 5+ Years of Experience in AWS & 3 Years in Terraform. 2 Years in EKS & Ansible.\u2022 AWS Associate \u2013 Developer Certifications: -\u2022 Terraform Associate CertificationSkill Proficiency:\u2022 Core AWS \u2013 EC2, S3, Lambda, AWS Backup, DR, CloudFormation- 10 years- Expert level.\u2022 Terraform- 4 years- Expert level.\u2022 EKS- 2 years \u2013 Hands- on\u2022 Ansible- 2 years \u2013 Hands- on\u2022 Documentation / SOP\u2019s- 2 years \u2013 Hands- on ---Warm Regards,AkashSmart IT Frame LLCakash@smartitframe.comwww.smartitframe.comhttps://www.linkedin.com/in/akash-s-332905212/-----WBENC------\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "554_Devops with Gen AI : PA NC": "Chandra N,\nSiri Info\nchandra.n@siriinfo.com\nReply to: chandra.n@siriinfo.com\nRole name:DeveloperRole Description:? Provides subject matter expertise in building bots using Kore.ai across domains (voice, web agent assist).? Expertise in applying the build once/deploy multiple strategy for bots across chat, voice and agent assist.? Expertise in troubleshooting Kore.ai errors.? Expertise in supporting application platforms, troubleshooting issues.? Ability to help technical design sessions with cross-functional teams ? Willingness to provide guidance and support as developers, data scientists, ux designers are learning this new way of working.? Provides subject matter expertise for unit and integration testing for features developed on the Kore.ai platform.? Assistance in creating runbooks and documenting production support guidelines based on experience at other firms.? Serves as a Kore technical ambassador as the Vanguard team identifies Kore defects and / or Kore feature requests.? Participates in design, code, and test inspections throughout the life cycle to identify issues. ? Explains technical considerations at related meetings, including those with internal departments.? Writes and elevates code using Vanguard?s SDLC tools and process.? Consults on implementation of knowledge and call summarization features, inclusive of Gen AI? Support during production incidents involving the Kore platform and features running on the platform? Provide Subject matter expertise in monitoring and telemetry tools and provide guidance in defining SLO/SLA for each bot.? Other duties as agreed to by both parties? Will work in high traffic / real-time Enterprise apps? Will be a part of Agile Methodologies, Design Patterns, and Software Revision Control Systems? Will gain experience in scaling applications and database infrastructure? Able to lead the team from technical, managerial leadership perspective? Inspire & motivate team-mates and be a cross-functional go-to-person? Implementing proofs of concept, prototypes and production-grade softwareCompetencies:Digital : Cloud DevOps, Digital : PythonExperience (Years):4-6Essential Skills:Must have strong expertise in either Java, Python, Node.js? Skillset should have the following JSON, Redis / MongoDB, Web Sockets? Knowledge of AI, NLP, ML, Chat bots will be added advantage.? Knowledge on Linux / Unix flavours, NGinx? Experience in Web Services using REST, SOAP and RabbitMQ? Knowledge of different authentication and authorization techniques? Candidate must have hands on experience in server-side programming independent of technology? Strong knowledge of code documentation and handover? Experience building scalable, high-available mission-critical platforms? Exposure to any of the cloud platforms like AWS/GCP/Azure? UI-side development experience in any one of the technologies Java Script /Angular.JS /React.JS/Vue.JS? Strong leadership competencies including the ability to think and act strategically, drive for results, build a high-performance culture and inspire/teach others to perform at higherlevelsTechnology requirements? Min 7+ yrs hands on KORE.ai, SmartAssist, IVR Technologies, Java, Java Script, Python? Good experience on Conversational AI Platform? Exposure to Gen AI and its concepts with conversational AI Products? Good experience in understanding the business Experience managing, troubleshooting and supporting Secure, Highly Available cloud platforms? Cloud Engineering? Google, AWS, Azure? Application Maintenance? Webserver / Middleware? Rabbit MQ, Kafka, Node.JS, Nginx? Database platforms ? Mongo? Cache: Redis? Automation tools: Ansible, TerraformDesirable Skills:Technology requirements? Min 7+ yrs hands on KORE.ai, SmartAssist, IVR Technologies, Java, Java Script, Python? Good experience on Conversational AI Platform? Exposure to Gen AI and its concepts with conversational AI Products? Good experience in understanding the business Experience managing, troubleshooting and supporting Secure, Highly Available cloud platforms? Cloud Engineering? Google, AWS, Azure? Application Maintenance? Webserver / Middleware? Rabbit MQ, Kafka, Node.JS, Nginx? Database platforms ? Mongo? Cache: Redis? Automation tools: Ansible, TerraformCountry:United StatesBranch | City | Location:TCS - Raleigh, NC~TCS - Iselin, NJ(South)CHARLOTTE~MalvernCharlotte, NC~Malvern, PA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "555_Hot List :: AWS Engineer-Available for C2C": "martin,\nLambdanets\nmartin@lambdanetsllc.com\nReply to: martin@lambdanetsllc.com\nHai, Please find the H1B candidate resume & contact details and We can share PP & DOC Skill: AWS Engineer Legal Name: Maharishi KamaleshContact number: +17408033679Current Location: Austin - TexasLinkedIn: Maharishi Kamalesh SSN: 95086Passport Number: Z6364009Relocation: yesVisa: H1BAvailability: ImmediateSkype ID: maharishi.Kamalesh Thanks & Regards,Martin FordLambdanets2929 Kenny Rd, Suite 220,Columbus Ohio,43221Phone: +1 740 777 4473Email: martin@lambdanetsllc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "556_Urgent Role ::DevOps Engineer NET Migration and CI CD Specialist :: Local to Troy MI (Someone in Troy or very close to troy) :: Face to face interview": "Ravi singh,\nVYZE INC\nravi.singh@vyzeinc.com\nReply to: ravi.singh@vyzeinc.com\nNeed Someone For Face to face interview in Troy Michigan DevOps Engineer - .NET Migration and CI/CD SpecialistLocal to Troy, MI (Someone in Troy or very close to troy)Video/Phone (Client might call the candidate for F2F so they should be open)All visa6-12 MonthResponsibilities':1. Migrate .NET applications and other artifacts from Team Foundation Server to GitHub Enterprise 2. Create CI/CD workflow to deploy to on-prem servers and other targets 3. Assist development teams to adopt DevOps methodology 4. Lead the resolution of production deployment issuesQualifications:1. Strong experience of GitHub and GitHub Actions 2. Strong knowledge in .Net development 3. Scripting knowledge for automation is highly recommended.Regards, Ravi SinghSr. IT Recruiter | IT Healthcare & InformaticsEmail: ravi.singh@vyzeinc.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "557_Need: Sr Cloud Security Architect: Bellevue, WA or Frisco, TX (Need Only Locals)": "Satnam Singh,\nSPAR Information Systems\nsatnam.singh@sparinfosys.com\nReply to: satnam.singh@sparinfosys.com\nHello All,Hope you are doing great Please go through the job description and let me know your interest.Role: Sr Cloud Security ArchitectLocation: Bellevue, WA or Frisco, TX (Hybrid from Day 1) (Need Only Locals)Duration: Long Term ContractJob Description:Key ResponsibilitiesDevelop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards.Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection.Implement and enforce security controls to protect sensitive data and systems.Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments.Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences.Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. QualificationsExperience with onboarding and provisioning in cloud environments.Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms.In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers.Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments.Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.Strong problem-solving and analytical skills for identifying and mitigating security risks proactively.Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred.Proficiency in scripting and automation tools (e.g., Python, Terraform).Strong communication and documentation skills for collaborating with cross-functional teams.Thanks & Regards,Satnam SinghDirect: 201 623 3660Email : Satnam.singh@sparinfosys.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "558_AWS Data Enginee || Wilmington, DE ||": "Shivendra,\nSynkriom\nshivendra.singh@synkriom.com\nReply to: shivendra.singh@synkriom.com\nRole - AWS Data Enginee location - Wilmington, DE JOB DESCRIPTION : - Mandatory Skills: Spark, hands on experience in Java coding, AWS Glue, Lambda, S3, EMR Must Have:\u2022 BS/BA degree or equivalent experience\u2022 General: Strong organizational, problem-solving, and critical thinking skills; Strong documentation skills\u2022 Coding: Proficiency in Python\u2022 Cluster Computing frameworks: Proficiency in Spark and Spark SQL\u2022 AWS Data Services: Proficiency in Lake formation, Glue ETL (or) EMR, S3, Glue Catalog, Athena, Kinesis (or) MSK, Airflow (or) Lambda + Step Functions + Event Bridge\u2022 Data De/Serialization: Expertise in at least 2 of the formats: Parquet, Iceberg, AVRO, JSON-LD\u2022 AWS Data Security: Good Understanding of security concepts such as: Lake formation, IAM, Service roles, Encryption, KMS, Secrets Manager Good to Have:\u2022 Linux Scripting, Jenkins\u2022 DevOps: Git, CI/CD, JIRA, TDD\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "559_Salesforce Cloud Architect (F2F INTERVIEW)": "Prashant Jha,\nTek Inspirations LLC\nprashant.jha@tekinspirations.com\nReply to: prashant.jha@tekinspirations.com\nSalesforce Cloud Architect/ Hybrid Role Location:- Hybrid - Lansing, MI, 48993 \u2013 Need local candidate with address proof such as DL and IDDuration:- Long Term ContractInterview Mode :- Final interview will be in person. Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations Salesforce skillsets5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "560_ETL Development Lead - DataStage and Cloud DW Solutions": "Khayal Abbas,\nScalable- Systems\nkhayal.abbas@scalable-systems.com\nReply to: khayal.abbas@scalable-systems.com\nHi,My name is Khayal Abbas and I am a Staffing Specialist at Scalable Systems. I am reaching out to you on an exciting job opportunity with one of our clients.To Apply: Please submit your resume and cover letter detailing your relevant experience to khayal.abbas@scalable-systems.com.Job Summary:We are seeking an experienced ETL Development Lead with a strong background in designing and developing ETL workflows using DataStage, and experience with Python and cloud data warehouse implementations, to join our team at TCS in Culver City, CA. The ideal candidate will provide technical leadership, ensure quality deliverables, and manage large DW implementations in an agile onshore/offshore model.(Contract)Role: - ETL Development Lead - DataStage and Cloud DW SolutionsLocation: Culver City, CAClient is TCSTechnical/Functi006Fnal Skills Essential Skills:Extensive experience designing and developing ETL workflows using DataStage.Proficient in Python for ETL development.Experience with large DW implementations.Hands-on experience with cloud data warehouses such as Teradata, AWS, and Snowflake.Experience with other ETL and reporting tools (Business Objects, Tableau, etc.).Ability to understand business requirements and translate them into technical specifications.Strong familiarity with data quality assurance and testing.Excellent communication and leadership skills.Desirable Skills:Experience in the Media & Entertainment or Consumer Packaged Goods industry with Retail Execution.Hands-on experience working with AWS.Experience working with a large customer base across the globe with aggressive schedules and expectations.Experience in implementing and maintaining large DW solutions in an agile environment. Thanks, with Warm RegardsKhayal AbbasSr. Technical Recruiterkhayal.abbas@scalable-systems.comScalable Systems | Inspiring InnovationBig Data | Analytics | Integration | Intelligencewww.scalable-systems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "561_DevOps Kore AI Engineer for Charlotte, NC - Onsite": "Kiran Nagani,\nGAC Solutions\nkiran@gacsol.com\nReply to: kiran@gacsol.com\nThis is Kiran from \u201cGAC Solutions\u201d . I have an excellent job opportunity with one of our clients. Please find the job description below. Role: DevOps Kore AI EngineerLocation: Charlotte, NC \u2013 OnsiteDuration: 6 + Months contract Job description: DevOps Engineer \u2013 Kore.ai (4-6 years of industry experience with 2-3 years hands on experience on the following) \u2022 Proven experience as a DevOps Engineer, particularly with Kore.ai platform.\u2022 Experience in deploying and maintaining contact center solutions and chatbots.\u2022 Proficiency with CI/CD tools like Jenkins and GitLab CI.\u2022 Strong understanding of cloud platforms, especially AWS and Azure.\u2022 Expertise in scripting languages such as Python and Bash.\u2022 Familiarity with containerization technologies like AWS ECS, ECR, Docker and Kubernetes.\u2022 Experience with monitoring tools like Splunk, Honeycomb, and Prometheus.\u2022 Strong problem-solving skills and attention to detail.\u2022 Excellent communication and collaboration abilities.\u2022 Relevant certifications in DevOps, cloud platforms, or chatbot development are a plus. Key Responsibilities: Deployment and Management: \u2022 Should have experience of managing a team of 6-8 members, working in the similar skills to guide and help them as and when needed. \u2022 Deploy Kore.ai contact center solutions and chatbots across various environments.\u2022 Automate deployment processes using CI/CD pipelines with tools like Jenkins and GitHub Actions CI/CD solutions.\u2022 Ensure high availability, scalability, and reliability of the deployed solutions on AWS platforms.\u2022 Monitor the performance and health of Kore.ai chatbots and contact center solutions using tools like Splunk, Honeycomb, Prometheus etc.\u2022 Implement proactive measures to maintain system health and performance.\u2022 Troubleshoot and resolve issues related to chatbot functionality and performance.Collaboration and Support:\u2022 Work closely with development, QA, and customer support teams to ensure smooth deployment and operation of chatbots.\u2022 Provide technical support and guidance to internal teams and external clients as needed.\u2022 Collaborate with stakeholders to gather requirements and implement new features using Agile methodologies.Security and Compliance:\u2022 Implement security best practices to protect chatbot and contact center data.\u2022 Ensure compliance with relevant regulations and industry standards.\u2022 Conduct regular security audits and vulnerability assessments. Continuous Improvement:\u2022 Identify areas for improvement in deployment and maintenance processes.\u2022 Stay updated with the latest industry trends and best practices in DevOps and chatbot technologies.\u2022 Develop and maintain documentation for deployment processes and maintenance procedures. Qualifications: \u2022 Education: Bachelor\u2019s degree in Computer Science, Information Technology, or related field.\u2022 Experience:\u2022 Proven experience as a DevOps Engineer, particularly with Kore.ai platform.\u2022 Experience in deploying and maintaining contact center solutions and chatbots.\u2022 Skills:\u2022 Proficiency with CI/CD tools like Jenkins and GitLab CI.\u2022 Strong understanding of cloud platforms, especially AWS and Azure.\u2022 Expertise in scripting languages such as Python and Bash.\u2022 Familiarity with containerization technologies like AWS ECS, ECR, Docker and Kubernetes.\u2022 Experience with monitoring tools like Splunk, Honeycomb, and Prometheus.\u2022 Strong problem-solving skills and attention to detail.\u2022 Excellent communication and collaboration abilities.\u2022 Certifications: Relevant certifications in DevOps, cloud platforms, or chatbot development are a plus.Thanks,KiranSenior RecruiterE: kiran@gacsol.comwww.gacsol.com\u2018Experts in Digitalization and Engineering - Enterprise 4.0\u2019\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "562_Hiring for AWS Public Cloud Developer - (Day 1 Onsite) Hartford, CT -Cognizant - The Hartford": "Gangadar Reddy,\nCentraprise\nedula.gangadar@centraprise.com\nReply to: edula.gangadar@centraprise.com\nRole: AWS Public Cloud Developer Location: Hartford, CT -Hire type: ContractJob description:Qualification and experience required: -Total 10 years of experience in Cloud infrastructure.5+ Years of Experience in AWS & 3 Years in Terraform. Minimum 2 Years in EKS & Ansible.Certification (Mandate):Terraform Associate CertificationSkills: Core AWS - EC2, S3, Lambda, AWS Backup, DR, CloudFormation- 10 years \u2013 Cand. should have extensive experienceTerraform- Min. 5 Years and should be expertEKS- 2-3 Years (Hands on)Ansible - 2 years \u2013 (Hands- on)Documentation / SOP's- 2 years \u2013 (Hands- on) RegardsSiva Gangadar reddyUS IT RecruiterCentraprise Corp33 Wood Avenue South, Suite 600, Iselin NJ 08830 Desk: 469-639-0369Email: edula.gangadar@centraprise.com We respect your online privacy. If you would like to be removed from our mailing list please reply with \"Remove\" in the subject and we will comply immediately. We apologize for any inconvenience caused. Please let us know if you have more than one domain. The material in this e-mail is intended only for the use of the individual to whom it is addressed and may contain information that is confidential, privileged, and exempt from disclosure under applicable law. If you are not the intended recipient, be advised that the unauthorized use, disclosure, copying, distribution, or the taking of any action in reliance on this information is strictly prohibited. We are an equal opportunity employer with a diverse workforce.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "563_AWS Public Cloud - C2C- Hartford, CT(Onsite)": "Rajesh Kumar,\nVy Systems\nrajesh.kumar@vysystems.com\nReply to: rajesh.kumar@vysystems.com\nlet us know if he is Hands-On in Production environment using Infra As A code Terraform/CloudFormation. Roles & Responsibilities: \u2022 Hands-On Experience & Understanding AWS Core Services, CloudFormation & Terraform. \u2022 Ability to understand the scripts in CloudFormation & Re-write in Terraform. \u2022 Understand the application infrastructure architecture. \u2022 Provide day to-day infrastructure operations support for the applications in scope including OS Support, Backup, DR & Security. \u2022 Understand the clients process in performing the AWS Operations and applications in scope. \u2022 Be Proactive to perform required automation using the client approved technology and tools. \u2022 Considered as an SME Role and coordinate with onsite client stakeholders and offshore associates. \u2022 To be considered as Mentor for the offshore associates. Should conduct onsite/offshore hand-over calls. \u2022 Experience in working with onshore/offshore team. \u2022 Should be available to perform On Call Support for any P1/P2 Critical/High Incidents. Qualification and experience required: - \u2022 A minimum of total 10 years of experience in Cloud infrastructure. \u2022 5+ Years of Experience in AWS & 3 Years in Terraform. 2 Years in EKS & Ansible. \u2022 AWS Associate \u2013 Developer Certifications: - \u2022 Terraform Associate Certification Skill Proficiency: \u2022 Core AWS \u2013 EC2, S3, Lambda, AWS Backup, DR, CloudFormation- 10 years- Expert level. \u2022 Terraform- 4 years- Expert level. \u2022 EKS- 2 years \u2013 Hands- on \u2022 Ansible- 2 years \u2013 Hands- on \u2022 Documentation / SOP\u2019s- 2 years \u2013 Hands- on\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "564_Looking for Salesforce Cloud Architect Hybrid  Lansing MI  Local Only": "Sushmita Soni,\nSonitalent\nsushmita.soni@sonitalentcorp.com\nReply to: sushmita.soni@sonitalentcorp.com\nHope you are doing well , We are Hiring for Salesforce Cloud Architectand let me know if you are looking for this role and send me your updated Job DescriptionJob: Salesforce Cloud ArchitectLocation: Hybrid , Lansing, MI ( Local Only ) Duration: 12 months+Need LinkedIn Interview \u2013 1St Teams interview , 2 in person Interview Must have - 5+ years of working experience on Salesforce Sales Cloud, Service Cloud 2+ years with Public Sector Foundation for licensing/permitting Experience in a minimum of two full Salesforce.com lifecycle implementations Do not Send Developer CandidatesJob Description \u2013 Need consultant with 15+ year experience In-depth understanding of the capabilities and constraints of the Salesforce.Assess Salesforce architecture and provide secure, high-performance technical solutions on the Force.com platform.Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems.Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL.Experience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com.Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts.Backup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documents.Identification of risks and issues from a technical perspective. Thanks & RegardsSushmita SoniSr. Technical Recruiter| SoniTalent Corp.Desk | 859-659-1004 EXT 201Email-sushmita.soni@sonitalentcorp.comAddress - 5404 Merribrook Lane, Prospect, KY, USA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "565_Salesforce Cloud Architect (133158) - Need locals in Michigan - In Person Interview.": "Riyaz,\nPredica Inc.\nriyaz@predicaz.com\nReply to: riyaz@predicaz.com\nJob Title : Salesforce Cloud Architect (133158) - Need locals in Michigan - In Person Interview.Location : Lansing, MIClient : State of MichiganDuration : Long Term. Salesforce skillsets : \u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform\u2022 Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. Regards,Riyaz ShaikMail ID: riyaz@predicaz.comPredica Inc.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "566_Mainframe Developer (IMS DB Cloud) || Wilmington, DE  Hybrid (Relocation is fine)": "Simranjeet Kaur,\nTanisha Systems Inc\nsimranjeet@tanishasystems.com\nReply to: simranjeet@tanishasystems.com\nGreetings!!Please, let me know if you are available or interested in the following position by replying to me with your:1. Updated Resume2. Work Authorization3. Salary/Rate Expectations4. Present location Mainframe Developer (IMS DB Cloud)Wilmington, DE \u2013 Hybrid (Relocation is fine)Contract Job Description:Required qualifications, capabilities, and skills. \u2022 Hands-on practical experience in system design, application development, testing, and operational stability\u2022 Proficient in coding in one or more languages.\u2022 Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\u2022 Overall knowledge of the Software Development Life Cycle.\u2022 Solid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\u2022 Hands on experience in IBM-Z/OS COBOL, CICS,IMS DB/DC, DB2, VSAM, JCL, CA7, Changeman/Endeavor Mainframe application development suits and IBM WebSphere MQ messaging solution.\u2022 Experience in creating API using zO/S connect & Kafka integration is a plus.\u2022 Demonstrated knowledge and work experience on AWS is a plus. Preferred qualifications, capabilities, and skills\u2022 Familiarity with modern front-end technologies\u2022 Knowledge on banking domain & credit card is a plus.\u2022 Exposure to cloud technologies\u2022 Exposure to JAVA Thanks & RegardsSimranjeet KaurTechnical RecruiterTanisha Systems (IncE-Verify, Certified Minority Business Enterprise MBE, Dun & Bradstreet CREDIBILITY CORP Certified)Iselin, NJ 08830M: (732) 384-4408 and EXT - 719Email : simranjeet@tanishasystems.comWeb: http://www.tanishasystems.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "567_Salesforce Cloud Architect": "Vipin kumar,\nTek inspirations\nvipin.kumar@tekinspirations.com\nReply to: vipin.kumar@tekinspirations.com\nJob Description -Salesforce Cloud ArchitectTop Skills & Years of Experience:- 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.Work with the customer and end users to define technical requirements for new projects and any future enhancements.Lead Technical workshops and design sessions with the Customer.Propose/Articulate design/architecture options with pros/consEnsure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.Review technical architecture deliverables throughout development to ensure quality and requirements traceability.Has overall technical responsibility for the technical aspects of the project environments Compile internal development guidelines/standards for the development team and ensure the team is following those standards.Provide oversight on QA efforts to ensure adherence to quality assurance standards. Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinementExcellent written and verbal communication skills. Salesforce skillsets5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "568_Azure Data Engineer :: 14+ years of Experience :: Hybrid in NYC": "Manohar,\nTekRpo\nmanohartekrpo@gmail.com\nReply to: manohartekrpo@gmail.com\nPOSITION: Azure Data EngineerLOCATION: Hybrid in NYCExperience: 14+ years MustJOB DESCRIPTION1.Azure Platform Experience:The candidate should have hands-on experience with various Azure services including but not limited toAzure Data Factory, Databricks, Data Lake, and Power BI.They should be able to design, build, and maintain ETL pipelines, manage data lakes,and create insightful reports and visualizations.Experience with Azure SQL Database, Azure Synapse Analytics, Azure COSMOS DB , Azure ML, other Azure services is a plus.SQL skill is a must to have And .NET C# skills will be a great plus.2.Azure Data Engineer: Design, implement, and maintain data solutions using Azure cloud services. Develop ETL processes, optimize data pipelines, and ensure data quality and security. Collaborate with teams to deliver scalable data architecture supporting analytics and business intelligence.3.Security on Azure Platform:The candidate should have a strong understanding of security protocols within the Azure platform.This includes knowledge of identity and access management, network security,encryption methods, secure data communication, and securing serverless architectures.4.Data Governance:The candidate should have experience in implementing data governance strategies.This includes setting up data governance frameworks, defining data ownership,ensuring data quality, and implementing data privacy and compliance measures.Knowledge of data cataloging, data lineage, and metadata management is also important.5.Certifications: Azure certifications such as Azure Security Engineer Associate, or Azure Solutions Architect Expert are highly desirable. Azure Databricks Data Engineer Professional Certificate is required. Databricks Machine Learning Certificate Preferable. TECHNICAL SKILLSAbility to intrepid, modify and write scripts and SQL queriesApps/.Net DeveloperAzure Cloud Data EngineerAzure DatabricksAzure Data FactoryAzure logic AppPowerBi Report ServerPythonNICE TO HAVE.NET C# skillsSSIS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "569_Salesforce Cloud Architect at Location: Lansing, MI hybrid": "ambika gupta,\ntek inspirations LLC\nambika.gupta@tekinspirations.com\nReply to: ambika.gupta@tekinspirations.com\nJob Description -Salesforce Cloud ArchitectLocation: Lansing, MI Note: need candidate LinkedIn and address proof Salesforce skillsets5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "570_RE: Role: Salesforce Cloud Architect Hybrid in Lansing, MI": "Shubham Singh,\nTek Inspirations LLC\nshubham@tekinspirations.com\nReply to: shubham@tekinspirations.com\nHello,I Hope you are great in good health and sprit \ud83d\ude42 !!Note: Role: Salesforce Cloud ArchitectLocation: Lansing, MIHybrid Local candidates only. Candidates must be located within 2 hours of Lansing, MI. NO REMOTE ONLY OPTION. Will need to be onsite from day 1, two days a week. Resource will be working a hybrid schedule. Top Skills & Years of Experience: - 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning - (Full Job Description Attached) Interview Process: 1st round will be held via MS Teams. 2nd round interviews will be held in person. Candidates must be available for an in-person interview.Duration: 1 year with extension possible Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for allnew and existing SaaS (Salesforce) implementations for the client dept. Responsibilities for this position include but are not limited to:\u2022 Participate in vendor\u2019s solution design sessions for client projects providing designguidance in areas such as extensions/modifications, data conversion, environmentprovisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.\u2022 Work with the customer and end users to define technical requirements for new projectsand any future enhancements.\u2022 Lead Technical workshops and design sessions with the Customer.\u2022 Propose/Articulate design/architecture options with pros/cons\u2022 Ensure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.\u2022 Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.\u2022 Review technical architecture deliverables throughout development to ensure quality and requirements traceability.\u2022 Has overall technical responsibility for the technical aspects of the project environments\u2022 Compile internal development guidelines/standards for the development team and ensure the team is following those standards.\u2022 Provide oversight on QA efforts to ensure adherence to quality assurance standards.\u2022 Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.\u2022 Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice atthe decision-making table.\u2022 Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinement\u2022 Excellent written and verbal communication skills. Salesforce skillsets\u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform. Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning WebComponents (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications,AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective. The successful candidate will be a self-motivated individual, who can work under dynamicconditions and within deadlines. Regards!! Shubham SinghSr Technical Recruiter| IT Healthcare & InformaticsTEK Inspirations LLC | 13573 Tabasco Cat Trail, Frisco, TX 75035 || Email: shubham@tekinspirations.com II Phone: 469-498-0636LinkedIn-https://www.linkedin.com/in/shubham-singh-a95354315/Whats'up No. +91-(953-232-0892) Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line to remove@tekinspirations.com. And mention all the e-mail addresses to be removed with any e-mail addresses,which might be diverting the e-mails to you. We are sorry for the inconvenience\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "571_Direct Client Req:: Need   Salesforce Cloud Architect Lansing, MI- Hybrid- Onsite Interview": "Divya,\nDIA Software Solutions\ndivya@diasoftwaresolutions.com\nReply to: divya@diasoftwaresolutions.com\nHi,I hope this message finds you well! I am reaching out to you on an exciting Direct client opportunity with one of our clients. Can you please go through the requirements and let me know if you are interested in this position?Job Title: Salesforce Cloud Architect Location: Lansing, MI- HybridExperience: 8 yearsDuration: 1 year with extension possibleInterview Process: 1st round will be held via MS Teams. 2nd round interviews will be held in person. Candidates must be available for an in-person interview.Job Description:As a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.Work with the customer and end users to define technical requirements for new projects and any future enhancements.Lead Technical workshops and design sessions with the Customer.Propose/Articulate design/architecture options with pros/consEnsure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.Review technical architecture deliverables throughout development to ensure quality and requirements traceability.Has overall technical responsibility for the technical aspects of the project environments Compile internal development guidelines/standards for the development team and ensure the team is following those standards.Provide oversight on QA efforts to ensure adherence to quality assurance standards. Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinementExcellent written and verbal communication skills. Salesforce skillsets: 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clarity Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as Own Backup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective. The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. Top Skills & Years of Experience: - 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning THANKS & REGARDSDIVYA | DIA SOFTWARE SOLUTIONS LLC.Austin, TX ||divya@diasoftwaresolutions.com|Diasoftwaresolutions.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "572_AWS Engineer": "Janany,\nTeamware Solution\njanany.t@twsol.com\nReply to: janany.t@twsol.com\nHi, Hope you are doing good. This is Janany from Teamware Solutions. We have a Immediate Opening on AWS Engineer. If you are Interested please share me your Updated resume. Role: AWS EngineerLocation: Austin, TX (Onsite)Hiring: Contract Job Description:Amazon Web Service(AWS)Cloud Computing Machine Learning Kindest Personal Regards, JananyTalent Acquisition Executivejanany.t@twsol.com8951 Cypress Waters Blvd, Suite #1092, Dallas Texas 75019Teamware Solutions Incwww.teamwaresolutions.net\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "573_Contract role::: Cloud Security Engineers::: Frisco, TX (onsite role)": "Jasmine Kaur,\nResource Logistics\njasmine.kaur@resource-logistics.com\nReply to: jasmine.kaur@resource-logistics.com\nHi, Greetings of the day!!! We have a wonderful job opportunity of Cloud Security Engineers, located in Frisco, TX (onsite role). Please have a look on below mentioned detailed job description:: Role: - Cloud Security Engineers Location: - Frisco, TX (onsite role)Rate: $60/hr. on C2C Must have:cloud security certifications.Hands-on practical experience on cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring.Proficiency in scripting and automation tools (e.g., Python, Terraform). Key ResponsibilitiesDevelop and implement secure onboarding processes for new cloud accounts, users, and resources across AWS, Azure, and other cloud platforms in compliance with federal regulations and security standards. Provision and configure cloud security services like Identity and Access Management (IAM), logging, config management, threat detection, and security monitoring for continuous protection. Implement and enforce security controls to protect sensitive data and systems. Collaborate with cross-functional teams to ensure secure integration of applications and services into the cloud environments. Respond to security incidents, investigate root causes, and implement remediation measures to prevent future occurrences. Document and maintain comprehensive security policies, procedures, and configurations for cloud environments. QualificationsExperience with onboarding and provisioning in cloud environments. Proven experience as a Cloud Security Engineer or similar role, with a strong focus on secure cloud deployments across multiple platforms. In-depth knowledge of cloud security services, tools, and best practices for AWS, Azure, and other major cloud providers. Familiarity with security regulations, standards (e.g., FedRAMP, NIST), and compliance requirements for cloud environments. Hands-on experience with cloud security services like IAM, CloudTrail, Config, GuardDuty and Security Hub for threat detection and security monitoring. Strong problem-solving and analytical skills for identifying and mitigating security risks proactively. Cloud security certifications like AWS Certified Security - Specialty, Azure Security Engineer Associate, or similar are highly preferred. Proficiency in scripting and automation tools (e.g., Python, Terraform). understands onboarding and will be doing onboarding provisioning work on AWS and AzureStrong communication and documentation skills for collaborating with cross-functional teams.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "574_AWS Data Architect  :: Contract :: Torrance, CA (Day 1 Onsite)": "Jaya Verma,\nGtech llc\njverma@greattechglobal.com\nReply to: jverma@greattechglobal.com\nRole: AWS Data Architect Location : Torrance, CA (Day 1 Onsite)Job Type : Contract Job Description: AWS Data Architect would also develop and deliver AWS Cloud-based Big Data and Analytical Solutions.Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing robust, secure, and optimized AWS architectures, and the ability to be hands-on in delivering the target solution. AWS Architect will work with customers and data engineers in delivering big data solutions on cloud. - Minimum of 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)- Minimum of 10 years of experience in big data, database and data warehouse architecture and delivery- Minimum of 5 years of professional experience in 2 of the following areas:- Solution/technical architecture in the cloud- Big Data/analytics/information analysis/database management in the cloud- IoT/event-driven/microservices in the cloud- Experience with private and public cloud architectures, pros/cons, and migration considerations.Good to have Blue Yonder experience- Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.- Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc. Certifications- Certified AWS Solutions Architect - Associate Thanks and Regards, GTECH LLCJaya Verma | Senior Technical Recuriter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "575_Need AWS Data Engineer - Remote - 13+ Years": "Venkat Ram,\nNeo Tech Solutions, Inc.\nvenkat@neotechusa.com\nReply to: venkat@neotechusa.com\nHi ,Hope you are doing wellPlease find the below Job Description and let me know if you are interested in applying for this position. Position: AWS Data EngineerDuration: 12 MonthsLocation: Scottsdale, AZ - Remote Job Description:Must have : IAM , AWS , Glue , S3 Redshift , Kinesis , Python/Java , Scala RDS SQL ServerAWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Needs someone who can understand and work on AWS echo system - Glue , Kinensis, DynamoDB etc. ) and the access permissions and especially terraform infra as code.It is combination of skills AWS DE + some hands-on knowledge in AWS infra. Position Summary:We are seeking a highly skilled AWS Data Engineer with extensive experience in AWS technologies to join our team as a contractor. The ideal candidate will have a strong background in designing, building, and maintaining data pipelines, and must be capable of contributing immediately to our ongoing projects.Key Responsibilities:Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage.Implement data ingestion processes to handle streaming and batch data.Ensure data quality and integrity through robust ETL processes.Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS.Optimize and tune data processing workflows for performance and cost efficiency.Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability.Document data architecture, processes, and workflows.Qualifications:Bachelor's or master\u2019s degree in computer science, Engineering, or a related field.Minimum of 5 years of experience in data engineering, with a focus on AWS technologies.Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server.Strong proficiency in SQL and experience with database design and optimization.Expertise in ETL/ELT processes and tools.Familiarity with data warehousing concepts and best practices.Experience with data modeling and schema design.Proficiency in programming languages such as Python, Java, or Scala.Knowledge of data governance and security best practices in a cloud environment.Excellent problem-solving skills and the ability to work independently with minimal supervision.Strong communication and collaboration skills.Qualifications:AWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL ServerFamiliarity with big data technologies like Apache Spark or Hadoop.Experience with reporting and visualization tools like Tableau Knowledge of DevOps practices and tools for CI/CD such as Jira and Harness. Required details for submissionFull Name: Email id:Contact Number:Current Location:Relocation:Interview Availability:Notice Period:Visa Status:Any Interviews in Pipeline:Any offers in Pipeline:Linked in: Pay rate: Thanks & RegardsVenkat RamTalent Acquisition Managervenkat@neotechusa.comNeo Tech Solutions, Inc.Suite 204, 515 Plainfield Ave, Edison, NJ 08817\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "576_DEVOPS AZURE ENGINEER": "Sonu Uprati,\nValiantIQ Inc\nsuprati@valiantiq.com\nReply to: suprati@valiantiq.com\nContract: initial 6 monthsPosition: DEVOPS/AZURE ENGINEERClient: 66 DEGREESLocation: REMOTEExperience: 7+ years NOTE: REMOTENo visa restrictionsMust provide 1 photo ID upon submission of resume Please make sure that REQUIRED SKILLS are in the RESUME if they have experience Need minimum 7 years (max of 10 years) of experience specifically in Azure DevOps and Azure Cloud.-Contract will be till the end of this year, most likely will be extended.-Will be working on one pipeline for assessment analysis.-AWS - Must have- blue-green deployment experience. Thanks & Regards,Sonu UpratiTechnical Recruiter- ValiantIQ Inc.\"Searching Best Minds \u25a0 Searching Best Minds\"Email:Suprati@valiantIQ.comF. (302) 482-3672Disclaimer: If you are not interested in receiving our e-mails then please reply with a \"REMOVE\" in the subject line for automatic removal. And mention all the e-mail addresses to be removed with any e-mail addresses, which might be diverting the e-mails to you. We are sorry for the inconvenience.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "577_urgent need for AWS Developer withAnaplan  Cincinnati, OH": "Sunita Rani,\nScalable\nsunita.rani@scalable-systems.com\nReply to: sunita.rani@scalable-systems.com\nAWS Developer with AnaplanCincinnati, OH Experience with Anaplan data Integration for sales territory management2. Experience with Vertafore Integration for Agency Producer Management3. Experience with AWS and Kubernetes: The candidate should have hands-on experience working on Amazon Web Services and Kubernetes. Candidate should have worked on AWS services like AWS RDS Databases, AWS S3, AWS Athena, AWS Glue, AWS EFS, AWS Cloud Watch etc. Candidate should also have experience working on Docker, Kubernetes, Helm Charts.4. Experience with Informatica PowerCenter and Data Quality (IDQ): The candidate should have expertise in developing large integrated complex mappings with standard and complex transformations. Candidate should also have experience working on Informatica Data Quality (IDQ) projects and transformations.5. Experience with DevOps Tools: The candidate should have hands-on experience working on DevOps tools like Concourse, Jenkins, Artifactory, GitHub, Urban Code Deployment, Harness, New Relic, Splunk, BigPanda.6. Experience with Kafka Integration: The candidate should have experience with Kafka Integration, an event streaming tool, and a good knowledge of Kafka Brokers, Producers, Consumers, Offset.7. Experience with REST-API Development and Management: The candidate should have hands-on experience working on REST-API Development and Management including creating OAS, Proxy and KVM\u2019s, and working on API Developer Portal.8. Proficiency in Java and Java EE: The candidate should have good knowledge of Java\u2019s Object-Oriented Programming Concepts and experience in implementing and designing Enterprise applications with Java/Java EE design patterns, best practices, coding practices, and coding standards.9. Experience working in Insurance Agency management applicationsCompetencies:Digital : Kafka, Digital : Anaplan\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "578_Devops Engineer": "ASHOK,\n4SERVE\nashok@4-serveinc.com\nReply to: ashok@4-serveinc.com\nReq:Devops Engineer:Note: Need local to MI .(Client won't accept outside MI)Migrate .net applications and Other arcraft from team foundatuions server to Github .Create CI/CD workflow to deploy to re prem servers and other targets.Adopt Devops Methodology.Lead resolutions of Production Deployment Issues. Qualifications: storng exp on Git Hub and Git Hub ActionsStrong knowwledge on .net development.Scrpting Knowledge and Automation Highly Recommended .\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "579_Urgent Required AWS Data Engineer (Teraform and IAM) Location: Remote": "Sarvan Kumar,\nRivago infotech\nsarvan@rivagoinfotech.com\nReply to: sarvan@rivagoinfotech.com\nRole: AWS Data EngineerLocation: Remote Duration: Long term Project Mandatory Skills: Terraform, IAM, Python, AWS Must have:IAM, AWS, Glue , S3 Redshift , Kinesis , Python/Java , Scala RDS SQL ServerAWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Needs someone who can understand and work on AWS echo system - Glue , Kinensis, DynamoDB etc. ) and the access permissions and especially terraform infra as code.It is combination of skills AWS DE + some hands-on knowledge in AWS infra. Position Summary:We are seeking a highly skilled AWS Data Engineer with extensive experience in AWS technologies to join our team as a contractor.The ideal candidate will have a strong background in designing, building, and maintaining data pipelines, and must be capable of contributing immediately to our ongoing projects. Key Responsibilities:Utilize AWS services such as Kinesis, S3, Glue, Redshift, and RDS SQL Server for data processing and storage.Implement data ingestion processes to handle streaming and batch data.Ensure data quality and integrity through robust ETL processes.Collaborate with other data engineers and the Cloud engineering team to develop and deploy data pipelines in AWS.Optimize and tune data processing workflows for performance and cost efficiency.Monitor and troubleshoot data pipeline issues to ensure continuous data flow and reliability.Document data architecture, processes, and workflows. Qualifications:Bachelor's or master\u2019s degree in computer science, Engineering, or a related field.Minimum of 5 years of experience in data engineering, with a focus on AWS technologies.Proven experience with AWS services including Kinesis, S3, Glue, Redshift, and RDS SQL Server.Strong proficiency in SQL and experience with database design and optimization.Expertise in ETL/ELT processes and tools.Familiarity with data warehousing concepts and best practices.Experience with data modeling and schema design.Proficiency in programming languages such as Python, Java, or Scala.Knowledge of data governance and security best practices in a cloud environment.Excellent problem-solving skills and the ability to work independently with minimal supervision.Strong communication and collaboration skills.Qualifications:AWS Certified Data Analytics \u2013 Specialty or AWS Certified Solutions Architect.Experience with other AWS services such as Lambda, Cloudwatch, Kinesis, Firehose, Event bridge, Redshift, DynamoDB, IAM, RDS SQL ServerFamiliarity with big data technologies like Apache Spark or Hadoop.Experience with reporting and visualization tools like TableauKnowledge of DevOps practices and tools for CI/CD such as Jira and Harness.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "580_AWS Data Architectat  Torrance, CA (Day 1 Onsite)": "Vinkal Dhaka,\nSiri Info\nvinkal.dhaka@siriinfo.com\nReply to: vinkal.dhaka@siriinfo.com\nOnly H1B candidate required\u2026\u2026.Passport number is mandatory for i94. AWS Data Architect Location \u2013 Torrance, CA (Day 1 Onsite) Contract/C2C Job Description: AWS Data Architect would also develop and deliver AWS Cloud-based Big Data and Analytical Solutions.Responsibilities include evangelizing data on cloud solutions with customers, leading Business and IT stakeholders through designing robust, secure, and optimized AWS architectures, and the ability to be hands-on in delivering the target solution. AWS Architect will work with customers and data engineers in delivering big data solutions on cloud. - Minimum of 5 years of Consulting or client service delivery experience on Amazon AWS (AWS)- Minimum of 10 years of experience in big data, database and data warehouse architecture and delivery- Minimum of 5 years of professional experience in 2 of the following areas:- Solution/technical architecture in the cloud- Big Data/analytics/information analysis/database management in the cloud- IoT/event-driven/microservices in the cloud- Experience with private and public cloud architectures, pros/cons, and migration considerations.Good to have Blue Yonder experience- Extensive hands-on experience implementing data migration and data processing using AWS services: VPC/SG, EC2, S3, AutoScaling, CloudFormation, LakeFormation, DMS, Kinesis, Kafka, Nifi, CDC processing Redshift, Snowflake, RDS, Aurora, Neptune, DynamoDB, Cloudtrail, CloudWatch, Docker, Lambda, Spark,Glue, Sage Maker, AI/ML, API GW, etc.- Familiarity with the Technology stack available in the industry for data management, data ingestion, capture, processing and curation: Kafka, StreamSets, Attunity, GoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive, Impala, etc. Certifications- Certified AWS Solutions Architect - Associate\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "581_Software Developer with Python and devops II Chicago IL  II No H1 II Must be local with DL": "Santosh,\nStellentit\nsantosh@stellentit.com\nReply to: santosh@stellentit.com\nGreetingsI hope you are doing well!Please find the requirement below , If you find yourself comfortable with the requirement please revert with your updated resume and I will get back to you Job Title Software Developer with Python and devopsLocation Chicago, IL (Only local with DL)Duration 6+ monthsInterview Mode: Virtual Interview Location: Hybrid Chicago (local only) DL requiredLast 4 SSN and MM/DD required..Genuine 1 year old with good connections LinkedIn with profile picture required.Work authorization: GCEAD, H4, GC & US Qualifications:Knowledge of Linux/UNIX systems and shell scriptingExperience with Python, JavaScript, HTML/CSS, and DockerSkillful at developing, designing, deploying and supporting applications.Basic knowledge of relational and non-relational (NoSQL) databases such as MySQL, PostreSQL, MongoDB, and Cassandra.Experience with Git and in general with version controlIdentify and gain expertise in appropriate new technologies and/or software tools. Responsibilities:Works closely with customers, business analysts, and team members to understand business requirements that drive the analysis and design of quality technical solutions. These solutions must be aligned with business and IT strategies and comply with the organization's architectural standards. Involved in the full systems life cycle and is responsible for designing, coding, testing, implementing, maintaining and supporting application software that is delivered on time and within budget. Makes recommendations towards the development of new code or reuse of existing code. Responsibilities may also include participation in component and data architecture design, performance monitoring, product evaluation and buy versus build recommendations. Has experience in systems analysis, design and a solid understanding of development, quality assurance and integration methodologies. Thanks & RegardsSantosh PalIT Technical RecruiterEmail: Santosh@stellentit.comGtalk: Santosh@stellentit.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "582_HIRING-----(USC OR H4EAD ONLY OR H1B)-- Sr Cloud Enterprise Security Architect - Frisco  (3 days Onsite)": "Sid M,\nInfotechspectrum INC\nsid@infotechspectrum.com\nReply to: sid@infotechspectrum.com\nPosition: Sr Cloud Enterprise Security ArchitectLocation: Frisco TX ( Onsite 3 days ) Duration : 12 MonthsNO OPT EAD PleaseJob Description: Technical Skills/Experience:\u00b7 Experience working in DevOps/GitOps teams\u00b7 Experience developing Infrastructure and Operations code, Platforms, and Automations\u00b7 Experience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate \u00b7 Understanding of all the basic services provided by CSPs (AWS, Azure and GCP)Sid M,Infotech Spectrum Inc.2060 Walsh Ave, STE #120, Santa Clara, CA 95050Phone: 408-416-3878 EXT 105|| Fax: 408-716-2625E-mail: Sid@infotechspectrum.com | www.infotechspectrum.com /LinkedIn : https://www.linkedin.com/in/siddiqm/ Ranked and recognized in the INC 500/5000 List of FASTEST Growing Private Companies in America MBE- A Minority Business Enterprise Certified by NMSDC \u00b7 Knowledge and hands-on experience of interacting with CSP APIs\u00b7 Deep knowledge of IAM, Policies, Network and other security servicesAuthoring IAM policiesAuthoring Organization Policies Developing private network based applications (using private endpoints, Vnet integrations, IPSec)\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "583_Cloud Enterprise Security Architect || Frisco, TX (Onsite Mandatory)": "Rashmita,\nChabezTech LLC\nrashmita@chabeztech.com\nReply to: rashmita@chabeztech.com\nHi, Hope you are doing well !! We have below position,if you are interested please share your updated profile to my mail id rashmita@chabeztech.com Position: Cloud Enterprise Security Architect Location: Frisco, TX (Onsite Mandatory) Job/Responsibility Profile:\u00b7 Design and develop multi-tenant solutions for enabling cloud platform as service\u00b7 Deploy and Operate multi-cloud security solutions/platforms at Enterprise scale\u00b7 Develop end-to-end technical solutions in security space\u00b7 Develop self-service solutions to onboard customers and manage users on the platforms\u00b7 Assess the customers' security architecture, requirements and provide guidance\u00b7 Design and develop policies to improve security posture and prevent threat exposure\u00b7 Identify and adapt modern tools, principles and technologies to improve security across cloud landscape\u00b7 Support cloud customers through cloud-native architecture guidance, security architecture guidance, policy remediations, etc.\u00b7 Work with ITSM functions (Change management, Incident management, Problem management, Request management) as they apply to tools and platforms used by the teamTechnical Skills/Experience:\u00b7 Experience working in DevOps/GitOps teams\u00b7 Experience developing Infrastructure and Operations code, Platforms, and Automations\u00b7 Experience across full solution lifecycle - Design, Develop, Implement, Operationalize, & Operate\u00b7 Understanding of all the basic services provided by CSPs (AWS, Azure and GCP)\u00b7 Knowledge and hands-on experience of interacting with CSP APIs\u00b7 Deep knowledge of IAM, Policies, Network and other security servicesAuthoring IAM policiesAuthoring Organization PoliciesDeveloping private network based applications (using private endpoints, Vnet integrations, IPSec)\u00b7 Developing Git Pipelines for managing platforms and operations\u00b7 Experience in Java Springboot/Python/GoLang development\u00b7 Experience in developing SAML, OAuth based applications\u00b7 Experience working with IaC tools such as Terraform, CloudFormation, or ARM templates.\u00b7 Experience in K8s development\u00b7 General experience working within ITSM processes (Change, Incident, Problem, Request management) in an Enterprise context\u00b7 Certifications such as AWS Certified Security Specialty, Azure Security Engineer Associate, or GCP Professional Cloud Security Engineer are a plus RashmitaChabezTech LLC 4 Lemoyne Dr #102, Lemoyne, PA 17043, USA Mobile: 8168053982Email: rashmita@chabeztech.com \ue119Reply\ue0e7ForwardAttendee panel closed\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "584_AWS Cloud Network Engineer Hybrid in Atlanta, GA Locals only No H1B CPT OPT": "Elisha,\nSibitalent\netitus@sibitalent.com\nReply to: etitus@sibitalent.com\nJob Title: AWS Cloud Network EngineerLocation: Hybrid in Atlanta, GA (2 days onsite in a week)Duration: 6+ months ContractJob Description:the role is hybrid in Atlanta (2 days onsite a week) so we need some local and will not accept relocation candidates, 6 month contract with opportunity to extend. They need this experience in their current position On prem and AWS migration to AWSAzure to AWS migration Deployment of Palo Alto in AWSCheckpoint to Palo Alto migration Thanks and Regards \ud83d\udc95Elisha TitusTechnical Recruiter\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "585_Requirements for :: AWS Solution Architect :: Healthcare Domain :: Sacramento, CA(Onsite)": "Nirbhay singh,\nAppian infotech Inc\nnirbhay.s@appianinfotech.com\nReply to: nirbhay.s@appianinfotech.com\nHi,We Have Urgent requirements for AWS Solution Architect Job Title: AWS Solution Architect Location: Sacramento, CA-95814 (Onsite) Job Type: Long term Project Job Description: MANDATORY QUALIFICATIONS\u00b7Bachelor\u2019s degree in an IT-related or engineering field from an accredited college or university (additional qualifying experience may be substituted for the required education on a year-for-year basis).\u00b7Minimum of eight (8) years of FTE experience in systems development, analysis, programming, and testing.\u00b7Minimum of three (3) years of experience in the last five (5) years, architecting large enterprise IT systems.\u00b7Minimum of three (3) years of FTE experience in the last five (5) years, with Infrastructure as Code using Terraform, AWS Cloud Formation, or similar technology.\u00b7Minimum of three (3) years of FTE experience in the last five (5) years, migrating server-based applications to container-based or serverless applications.\u00b7One (1) year of FTE experience in the last three (3) years, with Agile/SCRUM system development methodologies.DESIRABLE QUALIFICATIONS\u00b7 FTE experience in the last five (5) years, working with the Medicare/Medicaid/Medi-Cal program.\u00b7 FTE experience in the last three (3) years, working with financial management and/or accounting information technology systems.--RegardsNirbhay SinghAppian Infotech IncContact No- 276 910 0146 Ext. 128Email- nirbhay.s@appianinfotech.comLinkedIn:- https://www.linkedin.com/in/n-k-singh-430076245/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "586_Salesforce Cloud Architect - Lansing, MI - Locals Only - 15+ years": "Sai,\no2f\nsai@o2finc.com\nReply to: sai@o2finc.com\nJob Description:As a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:\u2022 Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.\u2022 Work with the customer and end users to define technical requirements for new projects and any future enhancements.\u2022 Lead Technical workshops and design sessions with the Customer.\u2022 Propose/Articulate design/architecture options with pros/cons\u2022 Ensure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.\u2022 Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.\u2022 Review technical architecture deliverables throughout development to ensure quality and requirements traceability.\u2022 Has overall technical responsibility for the technical aspects of the project environments\u2022 Compile internal development guidelines/standards for the development team and ensure the team is following those standards.\u2022 Provide oversight on QA efforts to ensure adherence to quality assurance standards.\u2022 Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.\u2022 Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.\u2022 Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinement\u2022 Excellent written and verbal communication skills. Salesforce skillsets\u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform\u2022 Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. SkillsYears UsedLast Used5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting Experience using Salesforce Lightning Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL Experience with migrating from Workflow rules/Process builder to Flow builder. Demonstrates successful implementation of Batch Apex, Apex triggers and classes. Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts Backup and recovery, data seeding using Own formerly known as OwnBackup. Experience using Conga composer for generating documents Identification of risks and issues from a technical perspective.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "587_Requirements for :: AWS Technical Architect :: Medicare(Healthcare) Domain :: Sacramento, CA(Onsite)": "Nirbhay singh,\nAppian infotech Inc\nnirbhay.s@appianinfotech.com\nReply to: nirbhay.s@appianinfotech.com\nHi,We Have Urgent requirements for AWS Technical Architect Job Title: AWS Technical Architect Location: Sacramento, CA-95814 (Onsite) Job Type: Long term Project Job Description: MANDATORY QUALIFICATIONS:\u00b7Bachelor\u2019s degree in an IT-related or engineering field from an accredited college or university (additional qualifying experience may be substituted for the required education on a year-for-year basis).\u00b7Minimum of eight (8) years of FTE experience in systems development, analysis, programming, and testing.\u00b7Minimum of three (3) years of experience in the last five (5) years, architecting large enterprise IT systems.\u00b7Minimum of three (3) years of FTE experience in the last five (5) years, with Infrastructure as Code using Terraform, AWS Cloud Formation, or similar technology.\u00b7Minimum of three (3) years of FTE experience in the last five (5) years, migrating server-based applications to container-based or serverless applications.\u00b7One (1) year of FTE experience in the last three (3) years, with Agile/SCRUM system development methodologies.DESIRABLE QUALIFICATIONS:\u00b7 FTE experience in the last five (5) years, working with the Medicare/Medicaid/Medi-Cal program.\u00b7 FTE experience in the last three (3) years, working with financial management and/or accounting information technology systems.--RegardsNirbhay SinghAppian Infotech IncContact No- 276 910 0146 Ext. 128Email- nirbhay.s@appianinfotech.comLinkedIn:- https://www.linkedin.com/in/n-k-singh-430076245/\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "588_Job Title : Mainframe IMS DB with Cloud": "viraat,\nReliant vision\nviraat@reliantvision.com\nReply to: viraat@reliantvision.com\nJob Title : Mainframe IMS DB with CloudLocation : Wilmington DEJob Responsibilities:Required qualifications, capabilities, and skills. \u2022 Hands-on practical experience in system design, application development, testing, and operational stability\u2022 Proficient in coding in one or more languages.\u2022 Experience in developing, debugging, and maintaining code in a large corporate environment with one or more modern programming languages and database querying languages.\u2022 Overall knowledge of the Software Development Life Cycle.\u2022 Solid understanding of agile methodologies such as CI/CD, Application Resiliency, and Security\u2022 Hands on experience in IBM-Z/OS COBOL, CICS,IMS DB/DC, DB2, VSAM, JCL, CA7, Changeman/Endeavor Mainframe application development suits and IBM WebSphere MQ messaging solution.\u2022 Experience in creating API using zO/S connect & Kafka integration is a plus.\u2022 Demonstrated knowledge and work experience on AWS is a plus. Preferred qualifications, capabilities, and skills\u2022 Familiarity with modern front-end technologies\u2022 Knowledge on banking domain & credit card is a plus.\u2022 Exposure to cloud technologies\u2022 Exposure to JAVA\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "589_URGENT ROLE::Salesforce Cloud Architect need local to MI": "Saurabh Kesarvani,\nTEK Inspirations LLC\nsaurabh.kesarvani@tekinspirations.com\nReply to: saurabh.kesarvani@tekinspirations.com\nSalesforce Cloud Architect need local to MILocation: Lansing, MIDuration: 6+ monthsNote: need candidate LinkedIn and address proofParticipate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.Work with the customer and end users to define technical requirements for new projects and any future enhancements.Lead Technical workshops and design sessions with the Customer.Propose/Articulate design/architecture options with pros/consEnsure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.Review technical architecture deliverables throughout development to ensure quality and requirements traceability.Has overall technical responsibility for the technical aspects of the project environments Compile internal development guidelines/standards for the development team and ensure the team is following those standards.Provide oversight on QA efforts to ensure adherence to quality assurance standards. Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinementExcellent written and verbal communication skills. Salesforce skillsets5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. Regards,Saurabh KesarvaniSr.Technical Recruiter | IT Healthcare & InformaticsTEK Inspirations LLC 13573 Tabasco Cat Trail, Frisco, TX 75035Email: saurabh.kesarvani@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "590_Job opening Security Engineer in Bothell, WA  or Frisco, TX (Onsite ) rimary skills on Identity &amp; Access Management , Vault, Cloud &amp; Kubernetes": "Aayush Jain,\nResource logistics\naayush@resource-logistics.com\nReply to: aayush@resource-logistics.com\nHi Greetings It\u2019s Aayush Jain from Resource Logistics. Please review the job description below. If you are interested in this position, please forward your updated resume and let me know your work authorization for immediate consideration and preferred time to discuss this opportunity further. Job Role: Security Engineer Location : Bothell, WA /Frisco, TX (Onsite )Duration: Contract Job description:Responsible for analysis, design and implementation coordination for tool and service designs within the cloud identity domain. Required skillsDeep understanding of cloud computing principles, including virtualization, containerization, microservices, and serverless computing; Risk Management, RHCOS security, container security, Kubernetes security, IAM security, network security, auditing, encryption, secrets management and data protection, securing CI/CD IAM Policy \u2018as code\u2019 ; OPA \u2013 Open Policy Agent (Styra Enterprise version of OPA); Cedar \u2013 Aws opensource policy agentExperience implementing Zero trust architecturesExcellent problem-solving, analytical, and communication skills.Ability to work independently and collaboratively in a fast-paced, agile environment.Create Identity & Access as code leveraging tools such as ansible, terraform to provision in cloudAnalyze environments to identify both technical and operational challenges while making recommendations and developing solutions for improvementLead complex or high severity troubleshooting and incident/problem resolutions with other security or cloud teamsMaintain knowledge of current developments in identity and cybersecurity, pertaining to threats to IT environmentsBachelor\u2019s degree in IT, Cybersecurity or related field or equivalent experience5+ years of experience in Information security with 4+ years of experience in Identity and Access Management3+ years of experience of cloud IAM and security experience.Strong knowledge of hybrid cloud, AWS, GCP, Azure and EntraID/Azure AD, OpenShift, Openstack KeystoneHands on experience with HashiCorp Vault, Cyberark or similar (PAM, secrets, certificate management platform)Advances knowledge of Identity Security concepts, least-privilege, separation of duties, and Zero trust design principalsExperience implementing Kubernetes RBAC access controlsUnderstanding of federation technologies (WS-Fed, OAuth, OpenID connect, SAML \u2026) and of encryption technologies (encryption types and protocols/standards)RBAC based access for cluster namespacesVulnerability and threat managementProfessional certifications CIMP, CIAM, CISSPJob opening Security Engineer in Bothell, WA /Frisco, TX (Onsite )Hi Greetings It\u2019s Aayush Jain from Resource Logistics. Please review the job description below. If you are interested in this position, please forward your updated resume and let me know your work authorization for immediate consideration and preferred time to discuss this opportunity further. Job Role: Security Engineer Location : Bothell, WA /Frisco, TX (Onsite )Duration: Contract Job description:Responsible for analysis, design and implementation coordination for tool and service designs within the cloud identity domain. Required skillsDeep understanding of cloud computing principles, including virtualization, containerization, microservices, and serverless computing; Risk Management, RHCOS security, container security, Kubernetes security, IAM security, network security, auditing, encryption, secrets management and data protection, securing CI/CD IAM Policy \u2018as code\u2019 ; OPA \u2013 Open Policy Agent (Styra Enterprise version of OPA); Cedar \u2013 Aws opensource policy agentExperience implementing Zero trust architecturesExcellent problem-solving, analytical, and communication skills.Ability to work independently and collaboratively in a fast-paced, agile environment.Create Identity & Access as code leveraging tools such as ansible, terraform to provision in cloudAnalyze environments to identify both technical and operational challenges while making recommendations and developing solutions for improvementLead complex or high severity troubleshooting and incident/problem resolutions with other security or cloud teamsMaintain knowledge of current developments in identity and cybersecurity, pertaining to threats to IT environmentsBachelor\u2019s degree in IT, Cybersecurity or related field or equivalent experience5+ years of experience in Information security with 4+ years of experience in Identity and Access Management3+ years of experience of cloud IAM and security experience.Strong knowledge of hybrid cloud, AWS, GCP, Azure and EntraID/Azure AD, OpenShift, Openstack KeystoneHands on experience with HashiCorp Vault, Cyberark or similar (PAM, secrets, certificate management platform)Advances knowledge of Identity Security concepts, least-privilege, separation of duties, and Zero trust design principalsExperience implementing Kubernetes RBAC access controlsUnderstanding of federation technologies (WS-Fed, OAuth, OpenID connect, SAML \u2026) and of encryption technologies (encryption types and protocols/standards)RBAC based access for cluster namespacesVulnerability and threat managementProfessional certifications CIMP, CIAM, CISSP\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "591_Azure Data Engineer with certificate || Hybrid in NYC": "AZAD,\nRCI\nazad@rconsultinginc.com\nReply to: azad@rconsultinginc.com\nPOSITION: Azure Data EngineerLOCATION: Hybrid in NYC Duration : 6+ Month JOB DESCRIPTION1.Azure Platform Experience: The candidate should have hands-on experience with various Azure services including but not limited to Azure Data Factory, Databricks, Data Lake, and Power BI. They should be able to design, build, and maintain ETL pipelines, manage data lakes,and create insightful reports and visualizations. Experience with Azure SQL Database, Azure Synapse Analytics, Azure COSMOS DB , Azure ML, other Azure services is a plus. SQL skill is a must to have And .NET C# skills will be a great plus.2.Azure Data Engineer: Design, implement, and maintain data solutions using Azure cloud services. Develop ETL processes, optimize data pipelines, and ensure data quality and security. Collaborate with teams to deliver scalable data architecture supporting analytics and business intelligence.3.Security on Azure Platform: The candidate should have a strong understanding of security protocols within the Azure platform. This includes knowledge of identity and access management, network security, encryption methods, secure data communication, and securing serverless architectures.4.Data Governance: The candidate should have experience in implementing data governance strategies.This includes setting up data governance frameworks, defining data ownership,ensuring data quality, and implementing data privacy and compliance measures.Knowledge of data cataloging, data lineage, and metadata management is also important.5.Certifications: Azure certifications such as Azure Security Engineer Associate, or Azure Solutions Architect Expert are highly desirable. Azure Databricks Data Engineer Professional Certificate is required. Databricks Machine Learning Certificate Preferable. TECHNICAL SKILLSAbility to intrepid, modify and write scripts and SQL queriesApps/.Net DeveloperAzure Cloud Data EngineerAzure DatabricksAzure Data FactoryAzure logic AppPowerBi Report ServerPython NICE TO HAVE.NET C# skillsSSIS\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "592_Salesforce Cloud Architect || Lansing, MI - Hybrid || 1st video 2nd in person": "Aditi Sikarwar,\nTek Inspirations\naditi.sikarwar@tekinspirations.com\nReply to: aditi.sikarwar@tekinspirations.com\nSalesforce Cloud ArchitectPosition location: Lansing, MI - HybridInterview \u2013 2 round \u2013 1st video 2nd in personLooking for local candidate Non-local candidate who can be represented and can attend F2F interview will also work Top Skills & Years of Experience:- 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.Work with the customer and end users to define technical requirements for new projects and any future enhancements.Lead Technical workshops and design sessions with the Customer.Propose/Articulate design/architecture options with pros/consEnsure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.Review technical architecture deliverables throughout development to ensure quality and requirements traceability.Has overall technical responsibility for the technical aspects of the project environments Compile internal development guidelines/standards for the development team and ensure the team is following those standards.Provide oversight on QA efforts to ensure adherence to quality assurance standards. Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinementExcellent written and verbal communication skills.Salesforce skillsets5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permittingExperience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the SalesforceAssess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platformDesign and document technical architecture solutions that span multiple platforms and include integration and authentication across systemsCustom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQLExperience with migrating from Workflow rules/Process builder to Flow builder.Demonstrates successful implementation of Batch Apex, Apex triggers and classes.Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.comOwnership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc. CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology effortsBackup and recovery, data seeding using Own formerly known as OwnBackup.Experience using Conga composer for generating documentsIdentification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines. Thanks & RegAditi SikarwarIT Technical Recruiter|TEK Inspirations LLC13573 Tabasco, Cat Trail, Frisco, TX 75035Email:Aditi.sikarwar@tekinspirations.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "593_Urgent Job Opening :: Devops Release Engineer :: Bentonville, AR (Hybrid)": "Seema Pal,\nKPG99\nseema@kpgtech.com\nReply to: seema@kpgtech.com\nTitle : Devops Release EngineerLocation: Bentonville, AR (Hybrid)Duration : 6+ Month Need to be U.S. citizen Job Description : ML model development model. Devops will be setting up the environment, setting up the CICD environment for the platform. Devops engineers will be making sure the platform is up and running. Containerized in kubernetes. Deployment targets, moving into Walmart production state. Lots of automation, red tape processes. Is the object performing well? ML (How well is it performing, is the data changing?) Always looking for areas of improvement and making the environment easier for the devs. Infosec is going to be involved as well. GCP, Google, ML experience (Monitoring) Splunk, Grafana, prometheus. Python experience will be a plus. Code that assists w automation will be needed, and I need to understand the python code. Pyspark, Spark. No writing code, or translating it, that's the engineering devs issues. How detailed they are, what role did they play in previous implementations, any evidence of an Existing process and made it better. Situational tech questions of a broken system, how can we fix it? being able to understand all of the intricacies. Automation, CI'CD (setting up and configuring this process) Continual process improvement\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "594_Urgent required Salesforce Cloud Architect": "rashid,\nGSK Solutions Inc\nrashid@gsksolutions.com\nReply to: rashid@gsksolutions.com\nJob Title: Salesforce Cloud Architect (Hybrid Onsite - Locals Only)Location: Lansing, MIDuration: 1 year with extension possibleState of MichiganInterview Process: 1st round will be held via MS Teams. 2nd round interviews will be held in person. Candidates must be available for an in-person interview.Hybrid: Local candidates only. Candidates must be located within 2 hours of Lansing, MI. NO REMOTE ONLY OPTION. Will need to be onsite from day 1, two days a week. Resource will be working a hybrid schedule.Top Skills & Years of Experience:- 5+ years of working experience on Salesforce Sales Cloud, Service Cloud- Experience using Salesforce Lightning- (Full Job Description Attached)Job DescriptionAs a Salesforce Cloud Architect, you\u2019ll be responsible for providing technical oversight for all new and existing SaaS (Salesforce) implementations for the Dept. of Licensing and Regulatory Affairs, State of Michigan. Responsibilities for this position include but are not limited to:\u2022 Participate in vendor\u2019s solution design sessions for client projects providing design guidance in areas such as extensions/modifications, data conversion, environment provisioning and application integration to ensure the vendor solution is efficient, cost-effective, and easily maintainable for SOM.\u2022 Work with the customer and end users to define technical requirements for new projects and any future enhancements.\u2022 Lead Technical workshops and design sessions with the Customer.\u2022 Propose/Articulate design/architecture options with pros/cons\u2022 Ensure that the technical requirements tie back to the established customer requirements and performance goals and that the technical direction is consistent with the client's long-term strategy.\u2022 Fully understand the capabilities and limitations of the technical environments of the applications used by the enterprise.\u2022 Review technical architecture deliverables throughout development to ensure quality and requirements traceability.\u2022 Has overall technical responsibility for the technical aspects of the project environments\u2022 Compile internal development guidelines/standards for the development team and ensure the team is following those standards.\u2022 Provide oversight on QA efforts to ensure adherence to quality assurance standards.\u2022 Ensures the proposed solutions adhere to SOM security guidelines and standards throughout the application lifecycle.\u2022 Proven record of delivering business value by leveraging technology and an ability to communicate strategic technical concepts at an executive level and be a trusted voice at the decision-making table.\u2022 Technical leadership, to include coaching and mentoring, setting best practices including integration and application development, deployment, testing (unit and systems), and iterative refinement\u2022 Excellent written and verbal communication skills.Salesforce skillsets\u2022 5+ years of working experience on Salesforce Sales Cloud, Service Cloud and 2+ years with Public Sector Foundation for licensing/permitting\u2022 Experience in a minimum of two full Salesforce.com lifecycle implementations, In-depth understanding of the capabilities and constraints of the Salesforce\u2022 Assess Salesforce.com architecture and provide secure, high-performance technical solutions on the Force.com platform\u2022 Design and document technical architecture solutions that span multiple platforms and include integration and authentication across systems\u2022 Custom development experience using advanced structured programming (APEX, Force.com, Java), Salesforce Flow Builder, Visual Flow Pages, Lightning Web Components (LWC), SOQL\u2022 Experience with migrating from Workflow rules/Process builder to Flow builder.\u2022 Demonstrates successful implementation of Batch Apex, Apex triggers and classes.\u2022 Understands web services, REST services and other technologies that can be used to transport data in an enterprise environment and interact with Salesforce.com\u2022 Ownership of all End-to-End technical aspects of a Salesforce.com program: data modelling, data migrations, data quality, systems integrations, 3rd party applications, AppExchange products like Clariti Basic Gov, Conga etc.\u2022 CICD/Release Management - Define, communicate, and manage technical change management (e.g., release) processes for all Salesforce.com related technology efforts\u2022 Backup and recovery, data seeding using Own formerly known as OwnBackup.\u2022 Experience using Conga composer for generating documents\u2022 Identification of risks and issues from a technical perspective.The successful candidate will be a self-motivated individual, who can work under dynamic conditions and within deadlines.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "595_Azure Cloud Engineer ::": "aashu upadhayay,\nTEK inspirations\naashu.upadhayay@tekinspirations.com\nReply to: aashu.upadhayay@tekinspirations.com\nJob Description -Azure Cloud Engineer Maryland (Onsite)Candidate should not have less than 1 year projects on resume.Qualifications5 years\u2019 experience as Azure Cloud Administrator or Azure Cloud EngineerCertification- Azure Administrator Associate or higherUS local resident with Public Trust ClearanceKey Responsibilities include:Azure Administrator- provision, configure, manage, and monitor Azure Virtual Machines (Windows VMs), storage accounts, IaaS, PaaS, and networking components; perform regular OS and security patching, backups/restore, and implement disaster recovery strategies for Azure resources with maintenance, troubleshooting.Azure Security- be able to set up and manage Microsoft Defender for Cloud, Azure Key Vault, network security, data encryption, and related security compliance and remediation.DevOps- be able to set up and integrate related DevOps processes, managing CI/CD pipelines for Azure Applications.Azure Monitoring- be able to set up Azure Monitor, Log Analytics, Network Watcher, and Azure Application Insights with related automated alerts in emails based on pre-configured rules and thresholds and perform related monitoring and troubleshooting.Expert in Azure App Service, Azure SQL Database, Azure Application Gateway (WAF, load balancing), etc., including setup, configuration, and support.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "596_Urgent Opening || AWS Tech Lead || Raritan NJ || Onsite": "Abhishek Kumar,\nAppian Infotech\nk.abhi@appianinfotech.com\nReply to: k.abhi@appianinfotech.com\nHi, Greetings from Appian InfoTech, Hope you are doing great. I am Abhishek Kumar and I work as a Technical Recruiter at Appian InfoTech. We are currently looking for an AWS Tech Lead to work with one of our clients. I\u2019d love to tell you a little more about this position and learn a few things about you, as well. Position: AWS Tech LeadLocation: Raritan, NJ(Onsite)Duration: long term projectContract Type: w2/c2c JDDesigning scalable, cost-effective, and efficient Cloud solutions while taking care of compliance and best practices.Working closely with stakeholders and product teams to gather technical and non-technical requirements and translate them into well-architected solutions.Implement solutions using AWS offerings such as EC2,API,RDS, S3,EKS, Lambda, Redshift etc.Should have expertise in AWS Data Platform and AWS Infrastructure servicesConfigure and deploy solutions using automated solutions such as Terraform and AWS Cloud Formation.Ensure that the architecture and solutions are compliant with AWS best practices and mitigate any security vulnerabilities that might occur.Focus on optimizing performance, scalability, and cost-efficiency.Provide technical support on AWS-related issues, monitor solutions, and implement appropriate strategies to eliminate performance bottlenecks.Utilize tools such as IAM, resource tagging, cost management, etc. to enforce AWS governance policies and implement monitoring and alerting solutions to address issues proactively.Aware of best practices in the AWS Cloud and documenting the research done on new services and POC\u2019s.Cloud governance knowledge.Strong communication skills and the ability to articulate technical solutions clearly.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "597_React Developer with Azure || Remote": "Shikha Choudhary,\nePeople Technologies\nshikha@epeopletech.com\nReply to: shikha@epeopletech.com\nHi, Greetings!!We have an urgent opening with React Developer with Azure and I have sent you job description, please go through it and let me know your comfort with it and also send me your updated resume ASAPTitle: React Developer with Azure Location: RemoteDuration: ContractRate : $48/hr on C2CMust Have Skills: Bachelor\u2019s degree or equivalent with 5+ years of technical experience in front-end development5+ Years of experience in React technologies including typescript, graphql and MUIRelevant experience with associated web technologies like Angular and Google Design FrameworkInvolved in consuming and publishing SOAP and RESTFUL web servicesDomain expertise of cloud infrastructure solutions (Azure preferred)Should be able to work independently as well as in a team environment.Should have strong verbal and written communication skills.Preferred Qualifications for Consideration: Experience in CI/CD Pipeline using Azure DevOps Knowledge in Agile SDLC Experience in source control like Azure DevOps and GitHub Thanks and Regards,Shikha ChoudharyePeople Technologies Inc255 Baldwin Road, Suite 205,Parsippany, NJ 07054Email- shikha@epeopletech.comFax # 973-299-7602http://www.epeopletech.comLinkedin - linkedin.com/in/shikha-choudhary-50a060189\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "598_immediate opening for AWS IOT Architecture || Boston, MA (Hybrid)": "prashanth,\nconfluxsystems\nprashanth@confluxsystems.com\nReply to: prashanth@confluxsystems.com\nHi,Greeting from conflux systems.Please go through the JD and let me know if you are interested.Role: AWS IoT Architect |Location: Hybrid \u2013 Boston, MA | Roles & Responsibilities\u00b7 Assess AWS platform running IOT based payloads supporting 3.3 million users & identify technical debt in current state.\u00b7 Understand & document application architecture, technology landscape, current deployment cycle & activities performed.\u00b7 Understand & document current tooling set up & integration of various pipelines for in-scope application & publish a report.\u00b7 Review the code for quality, security, scale and stability for the in-scope applications.\u00b7 Evaluate continuous delivery, monitoring, configuration management, security within DevOps pipelines and create a back log to implement continues delivery pipes using DevOps principles.\u00b7 Review existing build & release process & also understand continuous testing set-up for functional & non-functional testing.\u00b7 Review test data and environment set-up process.\u00b7 Review current tools and infrastructure set-up for DevOps and recommend improvements.\u00b7 Identify current Service Level Indicator (SLI) / Service Level Objectives (SLO) & measure current availability & reliability.\u00b7 Identify the metric to monitor to ensure appropriate reliability.\u00b7 Define how to calculate the SLI for the metrices.\u00b7 Set a target as the SLO to compare against during use.\u00b7 Define steady state as a measurable output for normal behavior of the platform.\u00b7 Build hypothesis around steady state behavior to scale for supporting 2x growth to around 7.5 mn user bases in next 2 years\u00b7 Need to identify availability 9s for the platform and draw a road map to achieve Shark Ninja target (99.99%)\u00b7 Risk Analysis of the current platform to understand the break point.\u00b7 Introduce variables that reflect real world events for identification of risk points, breaking points due to increase load and define the point of failure.\u00b7 Build resilience to scale\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "599_Urgent Need || Security Engineer  Cloud Security &amp; Kurbenetes. || Frisco, TX || Bothell, WA. (Onsite).": "Ashish Awasthi,\nSamson software solutions\nashish.awasthi@samsonsoft.com\nReply to: ashish.awasthi@samsonsoft.com\nTitle : Security Engineer \u2013 Cloud Security & Kurbenetes.Location : Frisco, TX || Bothell, WA. (Onsite).Job Description : Responsible for planning, designing, testing and implementing computer and network security infrastructure design and implementation for project including Cloud-Native container as a service.Required skills\u00b7 Deep understanding of cloud computing principles, including virtualization, containerization, microservices, and serverless computing; Risk Management, RHCOS security, container security, Kubernetes security, IAM security, network security, auditing, encryption, secrets management and data protection, securing CI/CD\u00b7 Analyze environments to identify both technical and operational challenges while making recommendations and developing solutions for improvement\u00b7 Lead complex or high severity troubleshooting and incident/problem resolutions with other security or cloud teams\u00b7 Maintain knowledge of current developments in cloud, CaaS and cybersecurity, maintaining of threats to It environments\u00b7 Bachelor\u2019s degree in IT, Cybersecurity or related field or equivalent experience\u00b7 5+ years of experience in Information security with 4+ years of experience in cloud security\u00b7 3+ years of experience of cloud container security experience.\u00b7 Experience with cloud infrastructure as code tools such as Terraform, CloudFormation, and Azure Resource Manager.\u00b7 Observability: Tracing/Metrics/Logs and Dashboards for Platform and Application workloads (Promethius, Grafana, Vector Openshift logging)\u00b7 Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.(Github, Gitea, Gitguardian, )\u00b7 Experience with static container scanning Trivy, Snyk. sBOM (Bill of Material): Syft/Grype\u00b7 Experience with runtime container security, Falco, Red Hat ACS\u00b7 Experience with Red Hat OpenShift and Openstack cloud platforms, Advanced cluster security, Advanced cluster management\u00b7 Experience with Policy/Regulation compliance: OPA, Red Hat ACS, Kyverno\u00b7 Strong knowledge of hybrid cloud, AWS, GCP, Azure and Kurbenetes\u00b7 Service Mesh isolation\u00b7 Vulnerability and threat management\u00b7 Application pen testing\u00b7 Hands on experience with HashiCorp Vault, Cyberark or similar (PAM, secrets, certificate management platform)\u00b7 Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.\u00b7 Static Container Scanning: Trivy, Snyk. sBOM (Bill of Material): Syft/Grype\u00b7 Reporting/Observability: Grafana, Prometheus, Red Hat Advanced Cluster Security\u00b7 Professional certifications CISSP, CEH, CDP (Certified DevSecOps Professional) Thanks & Regards, Ashish Awasthi | US IT Recruiter Samson Software Solutions Inc. Email: ashish.awasthi@samsonsoft.com | Note: Due to the High Volume of calls, Email is the best way to reach me.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "600_Azure Infrastructure Engineer. Onsite TX locals": "Rakesh,\nRavin Software Inc\nrakesh.b@ravinsoft.com\nReply to: rakesh.b@ravinsoft.com\nRole: Azure Infrastructure Engineer.Location: Dallas, TX (Local only)Job Description: Azure Infrastructure L2 EngineerWe are seeking a highly experienced and skilled Azure Infrastructure L2 Senior Engineer to join our IT team. This role is crucial in ensuring the robust operation, maintenance, and optimization of our Azure cloud infrastructure. The ideal candidate will have extensive hands-on experience with Azure, strong technical skills, and the ability to provide advanced support and troubleshooting services. Proficiency in Terraform, ServiceNow, automation, and DevOps practices is essential.Roles and responsibilities:Advanced Infrastructure Monitoring: Proactively monitor Azure infrastructure to ensure optimal performance, availability, and security.Incident and Problem Management: Handle complex infrastructure-related incidents and problems, performing advanced troubleshooting and root cause analysis.ServiceNow Ticket Handling: Manage and resolve high-priority tickets using the ServiceNow platform, ensuring adherence to SLAs.Maintenance and Optimization: Perform advanced maintenance tasks, including system updates, patch management, performance tuning, and optimization.Automation and Scripting: Develop, implement, and maintain automation scripts and processes using tools such as PowerShell, Azure CLI, and Terraform.DevOps Practices: Collaborate with DevOps teams to integrate infrastructure management with CI/CD pipelines, promoting a seamless development and deployment process.Infrastructure as Code (IaC): Utilize Terraform to manage infrastructure as code, ensuring consistency, scalability, and reliability.Documentation and Knowledge Sharing: Maintain comprehensive documentation of configurations, processes, and procedures. Mentor and train junior engineers.Security and Compliance: Ensure the infrastructure complies with security policies, best practices, and regulatory requirements.Collaboration and Leadership: Work closely with other IT teams, providing guidance and leadership to ensure effective and efficient infrastructure management.Continuous Improvement: Identify and implement improvements to processes, tools, and technologies to enhance the overall performance and reliability of the Azure infrastructure.Required Mandatory experienceAzure Administration: Minimum of 5 years of hands-on experience managing and supporting Azure environments.Terraform Expertise: Extensive experience with Terraform for managing infrastructure as code, including creating, modifying, and managing infrastructure resources.ServiceNow Proficiency: Proficiency in using ServiceNow for advanced ticket handling, including creating, updating, resolving high-priority tickets, and managing workflows.Automation Skills: Strong experience in automation using PowerShell, Azure CLI, and other relevant tools.DevOps Practices: Proven experience in integrating infrastructure management with CI/CD pipelines and collaborating with DevOps teams.Troubleshooting and Problem Resolution: Demonstrated ability to diagnose and resolve complex technical issues efficiently. RakeshRavin Software Inc2600 K Ave, Suite#224,Plano, TX 75074Email Id : rakesh.b@ravinsoft.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "601_Urgent need for ADM- Agile DevOps": "swathi,\nYochana IT solutions\nswathin@yochana.com\nReply to: swathin@yochana.com\nHi,This is Swathi from Yochana IT Solutions. i tried reaching you unfortunately went to the VM; this call is regarding the job opportunity. if you are in the job market please reach me at swathin@yochana.com. Role: ADM- Agile/DevOps Location: Whippany, NJ (locals)Contract Job description:Organize and lead Daily standups, sprint planning, review and retrospective meetings\u00b7 Foster the culture of continuous improvement through regular feedback and retrospective\u00b7 Monitor teams progress towards their sprint goals and timelines.\u00b7 Ensure all the scrum artifacts are up to date and transparent to stake holders\u00b7 Manage the integrity of the product backlog, and sprint backlog \u00b7 Building and managing CI/CD (Continuous Integration/Continuous Delivery) workflows and complex system integration using DevOps scripts and frameworks/tools Jenkins, BitBucket, Nexus, CVS and AppviewX. \u00b7 Hands on experience in scripting languages (shell, python, groovy), Terraform modules/templates, CloudFormation(AWS) or other Public Cloud platforms, operating systems (Linux and windows), Configuration Management tools (Chef or Puppet), network fundamentals to build automation framework. \u00b7 Supporting releases through production, identifying and implementing improvements to our Continuous Integration and Deployment (CI/CD) processes which are critical to our business. \u00b7 Ensure that all activities and duties are carried out in full compliance with regulatory requirements, Enterprise Wide Risk Management Framework and internal Barclays Policies and Policy Standards.\u00b7 Expected to own problems and drive through solutions.\u00b7 Document designs and communicate them with the team\u00b7 Analyzing requirements to find the most appropriate technical solution Required skill set: \u2022 Continuous Integration/Continuous Deployment (CI/CD) - GitLab CI/CD, Jenkins\u2022 Agile methodology and Scrum management, Jira\u2022 Version Control - Git, GitHub, GitLab, Bitbucket\u2022 Languages - Groovy, Ruby, Java, .Net, PythonGood to have : \u2022 Platforms - Chaos Engineering, HashiCorp Vault(Secrets Manager), Load balancer & cert automation and Delphix\u2022 Infrastructure as Code (IaC) - Terraform, AWS CloudFormation, Chef\u2022 Containerization \u2013 OpenShift(BCP)Thanks & RegardsSwathi NerallaResource SpecialistEmail: swathin@yochana.com\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "602_Hiring || Application Developer - AWS Cloud Migration || Newark, NJ (Onsite)": "Raj Kumar,\nDelta System & Software, Inc\nraj.kumar@delta-ssi.net\nReply to: raj.kumar@delta-ssi.net\nHi, Hope you are doing Great!!!This is Raj from Delta System & Software, Inc, I am reaching out to you today because we have your resume in our database and wanted to check if you are currently available in the market actively looking for an opportunity. Please see the below mentioned requirement and I believe you would be a good match for the role. I would appreciate if you could respond with your updated resume along with the good time to speak so we can move forward with the process. Position Information Type: ContractPosition: Application Developer - AWS Cloud MigrationLocation: Newark, NJ (Onsite) Job Description:JavaMicroservicesAWSDataPower.NET Best Regards,Raj Kumar | Ext: 131 | D : 972-573-0396 | Email: raj.kumar@delta-ssi.net | Linked InDelta System & Software, Inc3721 S. Stonebridge Dr, Suite 301, McKinney, TX \u2013 75070\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "603_Need----GCP Data Engineer with Strong Kubernetes and Bazel---NY (Hybrid)": "Rama Kant Tiwari,\nSmart IT Frame LLC.\nramakant@smartitframe.com\nReply to: ramakant@smartitframe.com\nJD:Location \u2013 NYHYBRID ROLEData Engineer with Kubernetes, Bazel, and GCP Expertise.Must Have:Strong proficiency in Java, with a focus on data applications.Experience with Kubernetes for container orchestration and management.Proficient in using Bazel for build automation and testing.Hands-on experience with Google Cloud Platform (GCP) services and tools.Experience in identifying, analyzing, and mitigating technical risks in data infrastructure.Understanding of security best practices and compliance requirements in data management.Good to have:Experience with other cloud platforms (e.g., AWS, Azure).Knowledge of CI/CD pipelines and related tools (e.g., Jenkins, GitLab).Familiarity with microservices architecture and related technologies. Thanks, and RegardsRama Kant TiwariTeam LeadEmail: ramakant@smartitframe.comPhone: +1 201-201-1445Smart IT Frame LLC.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "604_Urgent Position !! VMWare Cloud Engineer on AWS Required in Los Angeles, CA or Las Vegas, NV (5 Days Onsite)": "Suman Gupta,\nKK Software Associates\nsuman.g@kksoftwareassociates.com\nReply to: suman.g@kksoftwareassociates.com\nHello Folks,Hope you are doing great.Kindly go through the job description if you are interested, please share your updated resume at Suman.g@kksoftwareassociates.com or you can call me at (614) 699-5977 Title: VMWare Cloud Engineer on AWSLocation: Los Angeles, CA / Las Vegas, NV (5 Days Onsite)Duration: 12 months Description of Activities:Installation and Deployment:Install custom hardware configurations into designated racks or server cabinets within the datacenter.Ensure proper alignment with power, cooling, and network infrastructure requirements.Hardware Configuration:Configure and reconfigure custom hardware setups according to specifications provided by R8D leams,Ensure hardware components are correctly assembled and/or integrated. Or installedMaintain and update hardware to have the latest BIOS/firmware/driver levelsTesting and Validation:Conduct rigorous testing of custom hardware configurations to ensure functionality, performance, and compatibility with existing systems.Conduct routine hardware diagnostics and repair replace faulty components.Work with engineering teams to perform tests that require physical interaction such as pulling out and re-installing drives.Troubleshooting and Debugging:Identity and resolve hardware issues through systematic troubleshooting.Debug hardware failures, connectivity issuesPartner with R&D teams to identify and resolve issuesDocumentation and ReportingMaintain detailed documentation of Custom hardware configurations, including diagrams, setup instructions, and test results.Generate comprehensive reports on testing outcomes, issues encountered\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "605_NEED -  AWS Data Engineer or Architect- Chicago, IL (onsite)": "Ayushi,\nCygnuspro\nayushi@cygnuspro.com\nReply to: ayushi@cygnuspro.com\nHi,This is Ayushi Kaushik from Cygnus Professionals. I have an urgent job requirement matching your profile. If You are Interested, Please reply with updated resume at ayushi@cygnuspro.com or directly reach me Role: AWS Data Engineer or ArchitectLocation: Chicago, IL (onsite)Duration: Long Term Contract JD Responsible for ensuring that the data management and governance frameworks are in place and adhered to* Responsible for Architecting Data engineering pipelines including data acquisition, Ingestion, management, processing, publishing by leveraging various AWS services* Responsible for developing and optimizing database models to store and retrieve company information* Responsible for determining strategic data requirements, creating high-level designs according to these requirements* Responsible for guiding and resolving issues with AWS service configuration, performance, security compliance, data quality issues and data retrieval.* Prepare Architecture and high-level design documents for the data engineering pipelines* Designing DevOps & DataOps pipelines and resolve issues during implementation and execution.* Responsible for researching data acquisition opportunities Thanks & RegardsAyushi KaushikCygnus Professionals3490 US Highway 1Princeton, NJ 08540 E: ayushi@cygnuspro.comLinkedIn: https://www.linkedin.com/in/ayushi-kaushik-032706217\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "606_Urgent Need || Security Engineer  Cloud Security &amp; Kurbenetes. || Frisco, TX || Bothell, WA. (Onsite).": "Ashish Awasthi,\nSamson software solutions\nashish.awasthi@samsonsoft.com\nReply to: ashish.awasthi@samsonsoft.com\nTitle : Security Engineer \u2013 Cloud Security & Kurbenetes.Location : Frisco, TX || Bothell, WA. (Onsite).Job Description : Responsible for planning, designing, testing and implementing computer and network security infrastructure design and implementation for project including Cloud-Native container as a service.Required skills\u00b7 Deep understanding of cloud computing principles, including virtualization, containerization, microservices, and serverless computing; Risk Management, RHCOS security, container security, Kubernetes security, IAM security, network security, auditing, encryption, secrets management and data protection, securing CI/CD\u00b7 Analyze environments to identify both technical and operational challenges while making recommendations and developing solutions for improvement\u00b7 Lead complex or high severity troubleshooting and incident/problem resolutions with other security or cloud teams\u00b7 Maintain knowledge of current developments in cloud, CaaS and cybersecurity, maintaining of threats to It environments\u00b7 Bachelor\u2019s degree in IT, Cybersecurity or related field or equivalent experience\u00b7 5+ years of experience in Information security with 4+ years of experience in cloud security\u00b7 3+ years of experience of cloud container security experience.\u00b7 Experience with cloud infrastructure as code tools such as Terraform, CloudFormation, and Azure Resource Manager.\u00b7 Observability: Tracing/Metrics/Logs and Dashboards for Platform and Application workloads (Promethius, Grafana, Vector Openshift logging)\u00b7 Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.(Github, Gitea, Gitguardian, )\u00b7 Experience with static container scanning Trivy, Snyk. sBOM (Bill of Material): Syft/Grype\u00b7 Experience with runtime container security, Falco, Red Hat ACS\u00b7 Experience with Red Hat OpenShift and Openstack cloud platforms, Advanced cluster security, Advanced cluster management\u00b7 Experience with Policy/Regulation compliance: OPA, Red Hat ACS, Kyverno\u00b7 Strong knowledge of hybrid cloud, AWS, GCP, Azure and Kurbenetes\u00b7 Service Mesh isolation\u00b7 Vulnerability and threat management\u00b7 Application pen testing\u00b7 Hands on experience with HashiCorp Vault, Cyberark or similar (PAM, secrets, certificate management platform)\u00b7 Experience working in DevSecOps, including knowledge and experience enforcing a secure software development lifecycle.\u00b7 Static Container Scanning: Trivy, Snyk. sBOM (Bill of Material): Syft/Grype\u00b7 Reporting/Observability: Grafana, Prometheus, Red Hat Advanced Cluster Security\u00b7 Professional certifications CISSP, CEH, CDP (Certified DevSecOps Professional) Thanks & Regards, Ashish Awasthi | US IT Recruiter Samson Software Solutions Inc. Email: ashish.awasthi@samsonsoft.com | Note: Due to the High Volume of calls, Email is the best way to reach me.\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists.",
    "607_Urgent need for React Front-End Developer with Azure experience": "Aashish,\nCreaservices\nasaini@creaservices.net\nReply to: asaini@creaservices.net\nPosition: React Front-End Developer with Azure experienceLocation: Remote Basic Qualifications for Consideration: Bachelor\u2019s degree or equivalent with 5+ years of technical experience in front-end development5+ Years of experience in React technologies including typescript, graphql and MUIRelevant experience with associated web technologies like Angular and Google Design FrameworkInvolved in consuming and publishing SOAP and RESTFUL web servicesDomain expertise of cloud infrastructure solutions (Azure preferred)Should be able to work independently as well as in a team environment.Should have strong verbal and written communication skills.Preferred Qualifications for Consideration: Experience in CI/CD Pipeline using Azure DevOps Knowledge in Agile SDLC Experience in source control like Azure DevOps and GitHub\nSign-Up for your account with PROHIRES POWERHOUSE Recruiting Portal to broadcast requirements & hotlists."
}